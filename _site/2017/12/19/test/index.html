<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>矩阵求导</title>
    <meta name="description" content="  1 摘要  2 标量对矩阵的求导  3 矩阵对矩阵的求导  4 反向传播算法的完整向量形式推导1 摘要矩阵求导的技术，在统计学、控制论、机器学习等领域有广泛的应用。鉴于我看过的一些资料或言之不详、或繁乱无绪，本文来做个科普，分作两部分，第一部分讲标量对矩阵的求导术，第二部分讲矩阵对矩阵的求导术。本文使用小写字...">

    <link rel="shortcut icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://at.alicdn.com/t/font_8v3czwksspqlg14i.css">
    <link rel="stylesheet" href="/css/main.css ">
    <link rel="canonical" href="https://caoxiaoqing.github.io/2017/12/19/test/">
    <link rel="alternate" type="application/rss+xml" title="CXQ" href="https://caoxiaoqing.github.io/feed.xml ">


    <script>
    // 百度统计代码
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?bf7bdaf2bdac297bf8b3a6797ec58ce0";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
    </script>



<script type="text/javascript" 
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


</head>


  <body>

    <header id="top">
    <div class="wrapper">
        <a href="/" class="brand">CXQ</a>
        <small>free your mind</small>
        <button id="headerMenu" class="menu"><i class="fa fa-bars"></i></button>
        <nav id="headerNav">
            <ul>
                <li>
                    
                    <a href="/">
                    
                        <i class="fa fa-home"></i>Home
                    </a>
                </li>

                
                    
                    <li>
                        
                        <a href="/archive/">
                        
                            <i class="fa fa-archive"></i>Archives
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/category/">
                        
                            <i class="fa fa-th-list"></i>Categories
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/tag/">
                        
                            <i class="fa fa-tags"></i>Tags
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/collection/">
                        
                            <i class="fa fa-bookmark"></i>Collections
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/about/">
                        
                            <i class="fa fa-heart"></i>About
                        </a>
                    </li>
                    
                
                    
                
                    
                
                    
                
            </ul>
        </nav>
    </div>
</header>


        <div class="page clearfix" post>
    <div class="left">
        <h1>矩阵求导</h1>
        <div class="label">

            <div class="label-card">
                <i class="fa fa-calendar"></i>2017-12-19
            </div>

            <div class="label-card">
                <i class="fa fa-user"></i>caoxiaoqing
                
            </div>

            <div class="label-card">
                
            </div>

            <div class="label-card">
            


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#算法" title="Category: 算法" rel="category">算法</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


            </div>

            <div class="label-card">
            
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <!--a href="/tag/#%E7%9F%A9%E9%98%B5" title="Tag: 矩阵" rel="tag">矩阵</a-->
        <a href="/tag/#矩阵" title="Tag: 矩阵" rel="tag">矩阵</a>&nbsp;
    
        <!--a href="/tag/#%E6%B1%82%E5%AF%BC" title="Tag: 求导" rel="tag">求导</a-->
        <a href="/tag/#求导" title="Tag: 求导" rel="tag">求导</a>
    
  

</span>

            </div>

        </div>
        <hr>
        <article itemscope itemtype="http://schema.org/BlogPosting">
        <ul id="markdown-toc">
  <li><a href="#section" id="markdown-toc-section">1 摘要</a></li>
  <li><a href="#section-1" id="markdown-toc-section-1">2 标量对矩阵的求导</a></li>
  <li><a href="#section-2" id="markdown-toc-section-2">3 矩阵对矩阵的求导</a></li>
  <li><a href="#section-3" id="markdown-toc-section-3">4 反向传播算法的完整向量形式推导</a></li>
</ul>

<h2 id="section">1 摘要</h2>

<p>矩阵求导的技术，在统计学、控制论、机器学习等领域有广泛的应用。鉴于我看过的一些资料或言之不详、或繁乱无绪，本文来做个科普，分作两部分，第一部分讲标量对矩阵的求导术，第二部分讲矩阵对矩阵的求导术。本文使用小写字母 $x$ 表示标量，粗体小写字母 $\vec{x}$ 表示向量，大写字母 $\vec{X}$ 表示矩阵。本文第一部分标量对矩阵的求导整理自：https://zhuanlan.zhihu.com/p/24709748，本文第二部分矩阵对矩阵的求导整理自：https://zhuanlan.zhihu.com/p/24863977</p>

<h2 id="section-1">2 标量对矩阵的求导</h2>

<p>首先来琢磨一下定义，标量 $f$ 对矩阵 $\vec{X}$ 的导数，定义为
<script type="math/tex">\frac{\partial f}{\partial \vec{X}} = \left[\frac{\partial f }{\partial \vec{X}_{ij}}\right].</script>
即 $f$ 对 $\vec{X}$ 逐元素求导排成与 $\vec{X}$ 尺寸相同的矩阵。然而，这个定义在计算中并不好用，实用上的原因是在对较复杂的函数难以逐元素求导；哲理上的原因是逐元素求导破坏了整体性。试想，为何要将 $f$ 看做矩阵 $\vec{X}$ 而不是各元素 $\vec{X}_{ij}$ 的函数呢？答案是用矩阵运算更整洁。所以在求导时不宜拆开矩阵，而是要找一个从整体出发的算法。为此，我们来回顾，一元微积分中的导数（标量对标量的导数）与微分有联系：
<script type="math/tex">\mathrm{d}f = f'(x)\mathrm{d}x.</script>
多元微积分中的梯度（标量对向量的导数）也与微分有联系：</p>

<script type="math/tex; mode=display">\begin{equation}
  \mathrm{d}f = \sum_{i} \frac{\partial f}{\partial x_i}\mathrm{d}x_i = \frac{\partial f}{\partial \vec{x}}^T \mathrm{d}\vec{x}.
\end{equation}</script>

<p>这里第一个等号是全微分公式，第二个等号表达了梯度 $\frac{\partial f}{\partial \vec{x}}$ 与微分的联系。受此启发，我们将矩阵导数与微分建立联系：
<script type="math/tex">\begin{equation}
  \mathrm{d}f = \sum_{i,j} \frac{\partial f}{\partial \vec{X}{ij}}\mathrm{d}\vec{X}_{ij} = \mathrm{tr}\left(\frac{\partial f}{\partial \vec{X}}^T \mathrm{d}\vec{X}\right).
\end{equation}</script>
这里 $\mathrm{tr}$ 代表迹(trace)是方阵对角线元素之和，满足性质：对尺寸相同的矩阵 $\vec{A}, \vec{B}$，有 $\mathrm{tr}(\vec{A}^T\vec{B}) = \sum_{i,j}\vec{A}<em>{ij}\vec{B}</em>{ij}$，即 $\mathrm{tr}(\vec{A}^T\vec{B})$ 是矩阵 $\vec{A}, \vec{B}$ 的内积，因此上式与原定义相容。</p>

<p>然后来建立运算法则。回想遇到较复杂的一元函数如 $f = \log(2+\sin x)e^{\sqrt{x}}$，我们是如何求导的呢？通常不是从定义开始求极限，而是先建立了初等函数求导和四则运算、复合等法则，再来运用这些法则。故而，我们来创立常用的矩阵微分的运算法则：</p>

<ul>
  <li>加减法：$\mathrm{d}(\vec{X} \pm \vec{Y}) = \mathrm{d}\vec{X} \pm \mathrm{d}\vec{Y}$；</li>
  <li>矩阵乘法：$\mathrm{d}(\vec{X}\vec{Y}) = \mathrm{d}\vec{X}\vec{Y} + \vec{X}\mathrm{d}\vec{Y}$；</li>
  <li>转置：$\mathrm{d}(\vec{X}^T) = (\mathrm{d}\vec{X})^T$；</li>
  <li>迹：$\mathrm{d}\mathrm{tr}(\vec{X}) = \mathrm{tr}(\mathrm{d}\vec{X})$；</li>
  <li>逆：$\mathrm{d}\vec{X}^{-1} = -\vec{X}^{-1}\mathrm{d}\vec{X}\vec{X}^{-1}$，此式可在 $\vec{X}\vec{X}^{-1}=\vec{I}$ 两侧求微分来证明；</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>行列式：$\mathrm{d}</td>
          <td>\vec{X}</td>
          <td>= \mathrm{tr}(\vec{X}^{#}\mathrm{d}\vec{X})$，其中 $\vec{X}^{#}$ 表示 $\vec{X}$ 的伴随矩阵，在 $\vec{X}$ 可逆时又可以写作 $\mathrm{d}</td>
          <td>\vec{X}</td>
          <td>=</td>
          <td>\vec{X}</td>
          <td>\mathrm{tr}(\vec{X}^{-1}\mathrm{d}\vec{X})$，此式可用Laplace展开来证明，详见张贤达《矩阵分析与应用》第279页；</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>逐元素乘法：$\mathrm{d}(\vec{X}\odot \vec{Y}) = \mathrm{d}\vec{X}\odot \vec{Y} + \vec{X}\odot \mathrm{d}\vec{Y}$，$\odot$ 表示尺寸相同的矩阵 $\vec{X},\vec{Y}$ 逐元素相乘；</li>
  <li>逐元素函数：$\mathrm{d}\sigma(\vec{X}) = \sigma’(\vec{X})\odot \mathrm{d}\vec{X}$，$\sigma(\vec{X}) = \left[\sigma(\vec{X}_{ij})\right]$ 是逐元素运算的标量函数。</li>
</ul>

<p>我们试图利用矩阵导数与微分的联系 $\mathrm{d}f = \mathrm{tr}\left(\frac{\partial f}{\partial \vec{X}}^T \mathrm{d}\vec{X}\right)$，在求出左侧的微分 $\mathrm{d}f$ 后，该如何写成右侧的形式并得到导数呢？这需要一些迹技巧(trace trick)：</p>

<ul>
  <li>标量套上迹：$a = \mathrm{tr}(a)$；</li>
  <li>转置：$\mathrm{tr}(\vec{A}^T) = \mathrm{tr}(\vec{A})$；</li>
  <li>线性：$\mathrm{tr}(\vec{A}\pm \vec{B}) = \mathrm{tr}(\vec{A})\pm \mathrm{tr}(\vec{B})$；</li>
  <li>矩阵乘法交换：$\mathrm{tr}(\vec{AB}) = \mathrm{tr}(\vec{BA})$，两侧都等于 $\sum_{i,j}\vec{A}<em>{ij}\vec{B}</em>{ji}$；</li>
  <li>矩阵乘法/逐元素乘法交换：$\mathrm{tr}(\vec{A}^T(\vec{B}\odot \vec{C})) = \mathrm{tr}((\vec{A}\odot \vec{B})^T\vec{C})$，两侧都等于 $\sum_{i,j}\vec{A}<em>{ij}\vec{B}</em>{ij}\vec{C}_{ij}$。</li>
</ul>

<p>观察一下可以断言，若标量函数 $f$ 是矩阵 $\vec{X}$ 经加减乘法、行列式、逆、逐元素函数等运算构成，则使用相应的运算法则对 $f$ 求微分，再使用迹技巧给 $\mathrm{d}f$ 套上迹并将其它项交换至 $\mathrm{d}\vec{X}$ 左侧，即能得到导数。</p>

<p>在建立法则的最后，来谈一谈复合：假设已求得 $\frac{\partial f}{\partial \vec{Y}}$，而 $\vec{Y}$ 是 $\vec{X}$ 的函数，如何求 $\frac{\partial f}{\partial \vec{X}}$ 呢？在微积分中有标量求导的链式法则 $\frac{\partial f}{\partial x} = \frac{\partial f}{\partial y} \frac{\partial y}{\partial x}$，但这里我们不能沿用链式法则，因为矩阵对矩阵的导数 $\frac{\partial \vec{Y}}{\partial \vec{X}}$ 截至目前仍是未定义的。于是我们继续追本溯源，链式法则是从何而来？源头仍然是微分。我们直接从微分入手建立复合法则：先写出 $\mathrm{d}f = \mathrm{tr}\left(\frac{\partial f}{\partial \vec{Y}}^T \mathrm{d}\vec{Y}\right)$，再将 $\mathrm{d}\mathbf{Y}$ 用 $\mathrm{d}\vec{X}$ 表示出来代入，并使用迹技巧将其他项交换至 $\mathrm{d}\vec{X}$ 左侧，即可得到 $\frac{\partial f}{\partial \vec{X}}$。</p>

<p>接下来演示一些算例。特别提醒要依据已经建立的运算法则来计算，不能随意套用微积分中标量导数的结论，比如认为 $\vec{A}\vec{X}$ 对 $\vec{X}$ 的导数为 $\vec{A}$，这是没有根据、意义不明的。</p>

<p><strong>例1</strong>：$f = \vec{a}^T \vec{X}\vec{b}$，求 $\frac{\partial f}{\partial \vec{X}}$。</p>

<p>解：先使用矩阵乘法法则求微分：$\mathrm{d}f = \vec{a}^T \mathrm{d}\vec{X}\vec{b}$，再套上迹并做交换：$\mathrm{d}f = \mathrm{tr}(\vec{a}^T\mathrm{d}\vec{X}\vec{b}) = \mathrm{tr}(\vec{b}\vec{a}^T\mathrm{d}\vec{X})$，对照导数与微分的联系，得到 $\frac{\partial f}{\partial \vec{X}} = \vec{a}\vec{b}^T$。</p>

<p>注意：这里不能用 $\frac{\partial f}{\partial \vec{X}} =\vec{a}^T \frac{\partial \vec{X}}{\partial \vec{X}}\vec{b}=?$，导数与乘常数矩阵的交换是不合法则的运算（而微分是合法的）。有些资料在计算矩阵导数时，会略过求微分这一步，这是逻辑上解释不通的。</p>

<p><strong>例2【线性回归】</strong>：$l = |\vec{X}\vec{w}- \vec{y}|^2$，求 $\frac{\partial l}{\partial \vec{w}}$。</p>

<p>解：严格来说这是标量对向量的导数，不过可以把向量看做矩阵的特例。将向量范数写成 $l = (\vec{X}\vec{w}- \vec{y})^T(\vec{X}\vec{w}- \vec{y})$，求微分，使用矩阵乘法、转置等法则：$\mathrm{d}l = (\vec{X}\mathrm{d}\vec{w})^T(\vec{X}\vec{w}-\vec{y})+(\vec{X}\vec{w}-\vec{y})^T(\vec{X}\mathrm{d}\vec{w}) = 2(\vec{X}\vec{w}-\vec{y})^T\vec{X}\mathrm{d}\vec{w}$。 对照导数与微分的联系，得到 $\frac{\partial l}{\partial \vec{w}}= 2\vec{X}^T(\vec{X}\vec{w}-\vec{y})$。</p>

<p><strong>例3【多元 logistic 回归】</strong>：$l = -\vec{y}^T\log \mathrm{softmax}(\vec{Wx})$，求 $\frac{\partial l}{\partial \vec{W}}$。其中 $\vec{y}$ 是除一个元素为 1 外其它元素为 0 的向量；$\mathrm{softmax}(\vec{a}) = \frac{\exp(\vec{a})}{\vec{1}^T\exp(\vec{a})}$，其中 $\exp(\vec{a})$ 表示逐元素求指数，$\vec{1}$ 代表全 1 向量。</p>

<p>解：首先将 softmax 函数代入并写成 $l = -\vec{y}^T \left(\log (\exp(\vec{Wx}))-\vec{1}\log(\vec{1}^T\exp(\vec{Wx}))\right) = -\vec{y}^T\vec{Wx} + \log(\vec{1}^T\exp(\vec{Wx}))$，这里要注意逐元素 $\log$ 满足等式 $\log(\vec{u}/c) = \log(\vec{u}) - \vec{1}\log(c)$，以及 $\vec{y}$ 满足 $\vec{y}^T \vec{1} = 1$。求微分，使用矩阵乘法、逐元素函数等法则：$\mathrm{d}l = -\vec{y}^T\mathrm{d}\vec{Wx}+\frac{\vec{1}^T\left(\exp(\vec{Wx})\odot(\mathrm{d}\vec{Wx})\right)}{\vec{1}^T\exp(\vec{Wx})}$。
再套上迹并做交换，注意可化简 $\vec{1}^T\left(\exp(\vec{Wx})\odot(\mathrm{d}\vec{Wx})\right) = \exp(\vec{Wx})^T\mathrm{d}\vec{Wx}$，这是根据等式 $\vec{1}^T (\vec{u}\odot \vec{v}) = \vec{u}^T \vec{v}$，故 $\mathrm{d}l = \mathrm{tr}\left(-\vec{y}^T\mathrm{d}\vec{Wx}+\frac{\exp(\vec{Wx})^T\mathrm{d}\vec{Wx}}{\vec{1}^T\exp(\vec{Wx})}\right) =\mathrm{tr}(\vec{x}(\mathrm{softmax}(\vec{Wx})-\vec{y})^T\mathrm{d}\vec{W})$。对照导数与微分的联系，得到 $\frac{\partial l}{\partial \vec{W}}= (\mathrm{softmax}(\vec{Wx})-\vec{y})\vec{x}^T$。</p>

<p><strong>另解</strong>：定义 $\vec{a} = \vec{Wx}$，则 $l = -\vec{y}^T\log\mathrm{softmax}(\vec{a})$，先如上求出 $\frac{\partial l}{\partial \vec{a}} = \mathrm{softmax}(\vec{a})-\vec{y}$，再利用复合法则：$\mathrm{d}l = \mathrm{tr}\left(\frac{\partial l}{\partial \vec{a}}^T\mathrm{d}\vec{a}\right) = \mathrm{tr}\left(\frac{\partial l}{\partial \vec{a}}^T\mathrm{d}\vec{Wx}\right) = \mathrm{tr}\left(\vec{x}\frac{\partial l}{\partial \vec{a}}^T\mathrm{d}\vec{W}\right)$，得到 $\frac{\partial l}{\partial \vec{W}}= \frac{\partial l}{\partial\vec{a}}\vec{x}^T$。</p>

<table>
  <tbody>
    <tr>
      <td><strong>例4【方差的最大似然估计】</strong>：样本 $\vec{x}_1,\dots, \vec{x}_n\sim N(\vec{\mu}, \Sigma)$，其中 $\Sigma$ 是对称正定矩阵，求方差 $\Sigma$ 的最大似然估计。写成数学式是：$l = \log</td>
      <td>\Sigma</td>
      <td>+\frac{1}{n}\sum_{i=1}^n(\vec{x}_i-\vec{\bar{x}})^T\Sigma^{-1}(\vec{x}_i-\vec{\bar{x}})$，求 $\frac{\partial l }{\partial \Sigma}$ 的零点。</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>解：首先求微分，使用矩阵乘法、行列式、逆等运算法则，第一项是 $\mathrm{d}\log</td>
      <td>\Sigma</td>
      <td>=</td>
      <td>\Sigma</td>
      <td>^{-1}\mathrm{d}</td>
      <td>\Sigma</td>
      <td>= \mathrm{tr}(\Sigma^{-1}\mathrm{d}\Sigma)$，第二项是 $\frac{1}{n}\sum_{i=1}^n(\vec{x}<em>i-\vec{\bar{x}})^T\mathrm{d}\Sigma^{-1}(\vec{x}_i-\vec{\bar{x}}) = -\frac{1}{n}\sum</em>{i=1}^n(\vec{x}<em>i-\vec{\bar{x}})^T\Sigma^{-1}\mathrm{d}\Sigma\Sigma^{-1}(\vec{x}_i-\vec{\bar{x}})$。再给第二项套上迹做交换：$\mathrm{d}l = \mathrm{tr}\left(\left(\Sigma^{-1}-\Sigma^{-1}\vec{S}\Sigma^{-1}\right)\mathrm{d}\Sigma\right)$，其中 $\vec{S} = \frac{1}{n}\sum</em>{i=1}^n(\vec{x}_i-\vec{\bar{x}})(\vec{x}_i-\vec{\bar{x}})^T$  定义为样本方差。对照导数与微分的联系，有 $\frac{\partial l }{\partial \Sigma}=(\Sigma^{-1}-\Sigma^{-1}\vec{S}\Sigma^{-1})^T$，其零点即 $\Sigma$ 的最大似然估计为 $\Sigma = \vec{S}$。</td>
    </tr>
  </tbody>
</table>

<p>最后一例留给经典的神经网络。神经网络的求导术是学术史上的重要成果，还有个专门的名字叫做 BP 算法，我相信如今很多人在初次推导 BP 算法时也会颇费一番脑筋，事实上使用矩阵求导术来推导并不复杂。为简化起见，我们推导二层神经网络的 BP 算法。</p>

<p><strong>例5【二层神经网络】</strong>：$l = -\vec{y}^T\log\mathrm{softmax}(\vec{W}_2\sigma(\vec{W}_1\vec{x}))$，求 $\frac{\partial l}{\partial \vec{W}_1}$ 和 $\frac{\partial l}{\partial \vec{W}_2}$。其中 $\vec{y}$ 是除一个元素为 $1$ 外其它元素为 $0$ 的向量，$\mathrm{softmax}(\vec{a}) = \frac{\exp(\vec{a})}{\vec{1}^T\exp(\vec{a})}$ 同例 3，$\sigma(\cdot)$ 是逐元素 sigmoid 函数 $\sigma(a) = \frac{1}{1+\exp(-a)}$。</p>

<p>解：定义 $\vec{a}_1=\vec{W}_1\vec{x}$，$\vec{h}_1 = \sigma(\vec{a}_1)$，$\vec{a}_2 = \vec{W}_2 \vec{h}_1$，则 $l =-\vec{y}^T\log\mathrm{softmax}(\vec{a}_2)$。 在例 3 中已求出 $\frac{\partial l}{\partial \vec{a}_2} = \mathrm{softmax}(\vec{a}_2)-\vec{y}$。使用复合法则，注意此处 $\vec{h}_1, \vec{W}_2$ 都是变量：$\mathrm{d}l = \mathrm{tr}\left(\frac{\partial l}{\partial \vec{a}_2}^T\mathrm{d}\vec{a}_2\right) = \mathrm{tr}\left(\frac{\partial l}{\partial \vec{a}_2}^T\mathrm{d}\vec{W}_2 \vec{h}_1\right) + \mathrm{tr}\left(\frac{\partial l}{\partial \vec{a}_2}^T\vec{W}_2 \mathrm{d}\vec{h}_1\right)$，使用矩阵乘法交换的迹技巧从第一项得到 $\frac{\partial l}{\partial \vec{W}_2}= \frac{\partial l}{\partial\vec{a}_2}\vec{h}_1^T$，从第二项得到 $\frac{\partial l}{\partial \vec{h}_1}= \vec{W}_2^T\frac{\partial l}{\partial\vec{a}_2}$。接下来求 $\frac{\partial l}{\partial \vec{a}_1}$，继续使用复合法则，并利用矩阵乘法和逐元素乘法交换的迹技巧：$\mathrm{tr}\left(\frac{\partial l}{\partial\vec{h}_1}^T\mathrm{d}\vec{h}_1\right) = \mathrm{tr}\left(\frac{\partial l}{\partial\vec{h}_1}^T(\sigma’(\vec{a}_1)\odot \mathrm{d}\vec{a}_1)\right) = \mathrm{tr}\left(\left(\frac{\partial l}{\partial\vec{h}_1}\odot \sigma’(\vec{a}_1)\right)^T\mathrm{d}\vec{a}_1\right)$，得到 $\frac{\partial l}{\partial \vec{a}_1}= \frac{\partial l}{\partial\vec{h}_1}\odot\sigma’(\vec{a}_1)$。 为求 $\frac{\partial l}{\partial \vec{W}_1}$，再用一次复合法则：$\mathrm{tr}\left(\frac{\partial l}{\partial\vec{a}_1}^T\mathrm{d}\vec{a}_1\right) = \mathrm{tr}\left(\frac{\partial l}{\partial\vec{a}_1}^T\mathrm{d}\vec{W}_1\vec{x}\right) = \mathrm{tr}\left(\vec{x}\frac{\partial l}{\partial\vec{a}_1}^T\mathrm{d}\vec{W}_1\right)$，得到 $\frac{\partial l}{\partial \vec{W}_1}= \frac{\partial l}{\partial\vec{a}_1}\vec{x}^T$。</p>

<h2 id="section-2">3 矩阵对矩阵的求导</h2>

<p>矩阵对矩阵的求导采用了向量化的思路，常应用于二阶方法求解优化问题。</p>

<p>首先来琢磨一下定义。矩阵对矩阵的导数，需要什么样的定义？第一，矩阵 $\vec{F}(p\times q)$ 对矩阵 $\vec{X}(m\times n)$ 的导数应包含所有 $mnpq$ 个偏导数 $\frac{\partial \vec{F}<em>{kl}}{\partial \vec{X}</em>{ij}}$，从而不损失信息；第二，导数与微分有简明的联系，因为在计算导数和应用中需要这个联系；第三，导数有简明的从整体出发的算法。我们先定义向量 $\vec{f}(p\times 1)$ 对向量 $\vec{x}(m\times1)$ 的导数
<script type="math/tex">% <![CDATA[
\begin{equation}
    \frac{\partial \vec{f}}{\partial \vec{x}} =
    \begin{bmatrix}
    \frac{\partial f_1}{\partial x_1} & \frac{\partial f_2}{\partial x_1} & \cdots & \frac{\partial f_p}{\partial x_1}\\ \frac{\partial f_1}{\partial x_2} & \frac{\partial f_2}{\partial x_2} & \cdots & \frac{\partial f_p}{\partial x_2}\\ \vdots & \vdots & \ddots & \vdots\\ \frac{\partial f_1}{\partial x_m} & \frac{\partial f_2}{\partial x_m} & \cdots & \frac{\partial f_p}{\partial x_m}\\ \end{bmatrix}(m\times p)，
\end{equation} %]]></script>
有 $\mathrm{d}\vec{f} = \frac{\partial \vec{f} }{\partial \vec{x} }^T \mathrm{d}\vec{x}$；再定义矩阵的（按列优先）向量化
<script type="math/tex">\begin{equation}
    \mathrm{vec}(\vec{X}) = [\vec{X}_{11}, \ldots, \vec{X}_{m1}, \vec{X}_{12}, \ldots, \vec{X}_{m2}, \ldots, \vec{X}_{1n}, \ldots, \vec{X}_{mn}]^T(mn\times1),
\end{equation}</script>
并定义矩阵 $\vec{F}$ 对矩阵 $\vec{X}$ 的导数：
<script type="math/tex">\begin{equation}
    \frac{\partial \vec{F}}{\partial \vec{X}} = \frac{\partial \mathrm{vec}(\vec{F})}{\partial \mathrm{vec}(\vec{X})}(mn\times pq).
\end{equation}</script>
导数与微分有联系 $\mathrm{vec}(\mathrm{d}\vec{F}) = \frac{\partial \vec{F}}{\partial \vec{X}}^T \mathrm{vec}(\mathrm{d}\vec{X})$。几点说明如下：</p>

<ul>
  <li>按此定义，标量 $f$ 对矩阵 $\vec{X}(m\times n)$ 的导数 $\frac{\partial f}{\partial \vec{X}}$ 是 $mn\times1$ 向量，与上篇的定义不兼容，不过二者容易相互转换。为避免混淆，我们用 $\nabla_{\vec{X}} f$ 表示上篇定义的 $m\times n$ 矩阵，则有 $\frac{\partial f}{\partial \vec{X}}=\mathrm{vec}(\nabla_{\vec{X}} f)$。虽然本篇的技术可以用于标量对矩阵求导这种特殊情况，但使用上篇中的技术更方便。读者可以通过上篇中的算例试验两种方法的等价转换；</li>
  <li>标量对矩阵的二阶导数，又称 Hessian 矩阵，定义为 $\nabla^2_{\vec{X}} f = \frac{\partial^2 f}{\partial \vec{X}^2} = \frac{\partial \nabla_{\vec{X}} f}{\partial \vec{X}}(mn\times mn)$，是对称矩阵。对向量 $\frac{\partial f}{\partial \vec{X}}$ 或矩阵 $\nabla_{\vec{X}} f$ 求导都可以得到 Hessian 矩阵，但从矩阵 $\nabla_{\vec{X}} f$ 出发更方便；</li>
  <li>$\frac{\partial \vec{F}}{\partial \vec{X}} = \frac{\partial\mathrm{vec} (\vec{F})}{\partial \vec{X}} = \frac{\partial \vec{F}}{\partial \mathrm{vec}(\vec{X})} = \frac{\partial\mathrm{vec}(\vec{F})}{\partial \mathrm{vec}(\vec{X})}$，求导时矩阵被向量化，弊端是这在一定程度破坏了矩阵的结构，会导致结果变得形式复杂；好处是多元微积分中关于梯度、Hessian 矩阵的结论可以沿用过来，只需将矩阵向量化。例如优化问题中，牛顿法的更新 $\Delta \vec{X}$，满足 $\mathrm{vec}(\Delta \vec{X}) = -(\nabla^2_{\vec{X}} f)^{-1}\mathrm{vec}(\nabla_{\vec{X}} f)$；</li>
  <li>在资料中，矩阵对矩阵的导数还有其它定义，比如 $\frac{\partial \vec{F}}{\partial \vec{X}} = \left<a href="mp\times nq">\frac{\partial \vec{F}_{kl}}{\partial \vec{X}}\right</a>$，它能兼容上篇中的标量对矩阵导数的定义，但微分与导数的联系（$\mathrm{d}\vec{F}$ 等于 $\frac{\partial \vec{F}}{\partial \vec{X}}$ 中每个 $m\times n$ 子块分别与 $\mathrm{d}\vec{X}$ 做内积）不够简明，不便于计算和应用。</li>
</ul>

<p>然后来建立运算法则。仍然要利用导数与微分的联系 $\mathrm{vec}(\mathrm{d}\vec{F}) = \frac{\partial \vec{F}}{\partial \vec{X}}^T \mathrm{vec}(\mathrm{d}\vec{X})$，求微分的方法与上篇相同，而从微分得到导数需要一些向量化的技巧：</p>

<ul>
  <li>线性：$\mathrm{vec}(\vec{A}+\vec{B}) = \mathrm{vec}(\vec{A}) + \mathrm{vec}(\vec{B})$；</li>
  <li>矩阵乘法：$\mathrm{vec}(\vec{AXB}) = (\vec{B}^T \otimes \vec{A}) \mathrm{vec}(\vec{X})$，其中 $\otimes$ 表示 Kronecker 积，$\vec{A}(m\times n)$ 与 $\vec{B}(p\times q)$ 的 Kronecker 积是 $\vec{A}\otimes \vec{B} = <a href="mp\times nq">\vec{A}_{ij}\vec{B}</a>$。此式证明见张贤达《矩阵分析与应用》第 107-108 页；</li>
  <li>转置：$\mathrm{vec}(\vec{A}^T) = \vec{K}<em>{mn}\mathrm{vec}(\vec{A})$，$\vec{A}$ 是 $m\times n$ 矩阵，其中 $\vec{K}</em>{mn}(mn\times mn)$ 是交换矩阵(commutation matrix)；</li>
  <li>逐元素乘法：$\mathrm{vec}(\vec{A}\odot \vec{X}) = \mathrm{diag}(\vec{A})\mathrm{vec}(\vec{X})$，其中 $\mathrm{diag}(\vec{A})(mn\times mn)$ 是用 $A$ 的元素（按列优先）排成的对角阵。</li>
</ul>

<p>观察一下可以断言，若矩阵函数 $\vec{F}$ 是矩阵 $\vec{X}$ 经加减乘法、行列式、逆、逐元素函数等运算构成，则使用相应的运算法则对 $\vec{F}$ 求微分，再做向量化并使用技巧将其它项交换至 $\mathrm{vec}(\mathrm{d}\vec{X})$  左侧，即能得到导数。</p>

<p>再谈一谈复合：假设已求得 $\frac{\partial \vec{F}}{\partial \vec{Y}}$，而 $\vec{Y}$ 是 $\vec{X}$ 的函数，如何求 $\frac{\partial \vec{F}}{\partial \vec{X}}$ 呢？从导数与微分的联系入手，$\mathrm{vec}(\mathrm{d}\vec{F}) = \frac{\partial \vec{F}}{\partial Y}^T\mathrm{vec}(\mathrm{d}\vec{Y}) = \frac{\partial \vec{F}}{\partial \vec{Y}}^T\frac{\partial \vec{Y}}{\partial \vec{X}}^T\mathrm{vec}(\mathrm{d}\vec{X})$，可以推出链式法则 $\frac{\partial \vec{F}}{\partial \vec{X}} = \frac{\partial \vec{Y}}{\partial \vec{X}}\frac{\partial \vec{F}}{\partial \vec{Y}}$。</p>

<p>和标量对矩阵的导数相比，矩阵对矩阵的导数形式更加复杂，从不同角度出发常会得到形式不同的结果。有一些 Kronecker 积和交换矩阵相关的恒等式，可用来做等价变形：</p>

<ul>
  <li>$(\vec{A}\otimes \vec{B})^T = \vec{A}^T \otimes \vec{B}^T$；</li>
  <li>$\mathrm{vec}(\vec{ab}^T) = \vec{b}\otimes\vec{a}$；</li>
  <li>$(\vec{A}\otimes \vec{B})(\vec{C}\otimes \vec{D}) = (\vec{AC})\otimes (\vec{BD})$。可以对 $\vec{F} = \vec{D}^T\vec{B}^T\vec{XAC}$ 求导来证明，一方面，直接求导得到 $\frac{\partial \vec{F}}{\partial \vec{X}} = (\vec{AC}) \otimes (\vec{BD})$；另一方面，引入 $\vec{Y} = \vec{B}^T\vec{XA}$，有 $\frac{\partial \vec{F}}{\partial \vec{Y}} = \vec{C} \otimes \vec{D}$，$\frac{\partial \vec{Y}}{\partial \vec{X}} = \vec{A} \otimes \vec{B}$，用链式法则得到 $\frac{\partial \vec{F}}{\partial \vec{X}} = (\vec{A}\otimes \vec{B})(\vec{C} \otimes \vec{D})$；</li>
  <li>$\vec{K}<em>{mn} = \vec{K}</em>{nm}^T, \vec{K}<em>{mn}\vec{K}</em>{nm} = \vec{I}$；</li>
  <li>$\vec{K}<em>{pm}(\vec{A}\otimes \vec{B}) \vec{K}</em>{nq} = \vec{B}\otimes \vec{A}$，$\vec{A}$ 是 $m\times n$ 矩阵，$\vec{B}$ 是 $p\times q$ 矩阵。可以对 $\vec{AXB}^T$ 做向量化来证明，一方面，$\mathrm{vec}(\vec{AXB}^T) = (\vec{B}\otimes \vec{A})\mathrm{vec}(\vec{X})$；另一方面，$\mathrm{vec}(\vec{AXB}^T) = \vec{K}<em>{pm}\mathrm{vec}(\vec{BX}^T\vec{A}^T) = \vec{K}</em>{pm}(\vec{A}\otimes \vec{B})\mathrm{vec}(\vec{X}^T) = \vec{K}<em>{pm}(\vec{A}\otimes \vec{B}) \vec{K}</em>{nq}\mathrm{vec}(\vec{X})$。</li>
</ul>

<p>接下来演示一些算例。</p>

<p><strong>例1</strong>：$\vec{F} = \vec{AX}$，$\vec{X}$ 是 $m\times n$ 矩阵，求 $\frac{\partial \vec{F}}{\partial \vec{X}}$。</p>

<p>解：先求微分：$\mathrm{d}\vec{F}=\vec{A}\mathrm{d}\vec{X}$，再做向量化，使用矩阵乘法的技巧，注意在 $\mathrm{d}\vec{X}$ 右侧添加单位阵：$\mathrm{vec}(\mathrm{d}\vec{F}) = \mathrm{vec}(\vec{A}\mathrm{d}\vec{X}) = (\vec{I}_n\otimes \vec{A})\mathrm{vec}(\mathrm{d}\vec{X})$，对照导数与微分的联系得到 $\frac{\partial \vec{F}}{\partial \vec{X}} = \vec{I}_n\otimes \vec{A}^T$。</p>

<p>特例：如果 $\vec{X}$ 退化为向量，$\vec{f} = \vec{A} \vec{x}$，则根据向量的导数与微分的关系 $\mathrm{d}\vec{f} = \frac{\partial \vec{f}}{\partial \vec{x}}^T \mathrm{d}\vec{x}$，得到  $\frac{\partial \vec{f}}{\partial \vec{x}} = \vec{A}^T$。</p>

<table>
  <tbody>
    <tr>
      <td><strong>例2</strong>：$\vec{f} = \log</td>
      <td>\vec{X}</td>
      <td>$，$\vec{X}$ 是 $n\times n$ 矩阵，求 $\nabla_{\vec{X}} \vec{f}$ 和 $\nabla^2_{\vec{X}} \vec{f}$。</td>
    </tr>
  </tbody>
</table>

<p>解：使用第一部分中的技术可求得 $\nabla_{\vec{X}} \vec{f} = \vec{X}^{-1T}$。为求 $\nabla^2_{\vec{X}} \vec{f}$，先求微分：$\mathrm{d}\nabla_{\vec{X}} \vec{f} = -(\vec{X}^{-1}\mathrm{d}\vec{XX}^{-1})^T$，再做向量化，使用转置和矩阵乘法的技巧
 $\mathrm{vec}(\mathrm{d}\nabla_{\vec{X}} \vec{f})= -\vec{K}<em>{nn}\mathrm{vec}(\vec{X}^{-1}\mathrm{d}\vec{XX}^{-1}) = -\vec{K}</em>{nn}(\vec{X}^{-1T}\otimes \vec{X}^{-1})\mathrm{vec}(\mathrm{d}\vec{X})$，对照导数与微分的联系，得到 $\nabla^2_{\vec{X}} \vec{f} = -\vec{K}<em>{nn}(\vec{X}^{-1T}\otimes \vec{X}^{-1})$，注意它是对称矩阵。在 $\vec{X}$ 是对称矩阵时，可简化为 $\nabla^2</em>{\vec{X}} \vec{f} = -\vec{X}^{-1}\otimes \vec{X}^{-1}$。</p>

<p><strong>例3</strong>：$\vec{F} = \vec{A}\exp(\vec{XB})$，$\vec{A}$ 是 $l\times m$，$\vec{X}$ 是 $m\times n$，$\vec{B}$ 是 $n\times p$ 矩阵，$\exp()$ 为逐元素函数，求 $\frac{\partial \vec{F}}{\partial \vec{X}}$。</p>

<p>解：先求微分：$\mathrm{d}\vec{F} = \vec{A}(\exp(\vec{XB})\odot (\mathrm{d}\vec{XB}))$，再做向量化，使用矩阵乘法的技巧：$\mathrm{vec}(\mathrm{d}\vec{F}) = (\vec{I}_p\otimes \vec{A})\mathrm{vec}(\exp(\vec{XB})\odot (\mathrm{d}\vec{XB}))$，再用逐元素乘法的技巧：$\mathrm{vec}(\mathrm{d}\vec{F}) = (\vec{I}_p \otimes \vec{A}) \mathrm{diag}(\exp(\vec{XB}))\mathrm{vec}(\mathrm{d}\vec{XB})$，再用矩阵乘法的技巧：$\mathrm{vec}(\mathrm{d}\vec{F}) = (\vec{I}_p\otimes \vec{A})\mathrm{diag}(\exp(\vec{XB}))(\vec{B}^T\otimes I_m)\mathrm{vec}(\mathrm{d}\vec{X})$，对照导数与微分的联系得到 $\frac{\partial \vec{F}}{\partial \vec{X}} = (\vec{B}\otimes \vec{I}_m)\mathrm{diag}(\exp(\vec{XB}))(\vec{I}_p\otimes \vec{A}^T)$。</p>

<p><strong>例4【一元 logistic 回归】</strong>：$l = -y \vec{x}^T \vec{w} + \log(1 + \exp(\vec{x}^T\vec{w}))$，求 $\nabla_{\vec{w}} l$ 和 $\nabla_{\vec{w}}^2 l$。其中 $y$ 是取值 0 或 1 的标量，$\vec{x}$ 和 $\vec{w}$ 是向量。</p>

<p>解：使用上篇中的技术可求得 $\nabla_{\vec{w}} l = \vec{x}(\sigma(\vec{x}^T\vec{w}) - y)$，其中 $\sigma(a) = \frac{\exp(a)}{1+\exp(a)}$ 为 sigmoid 函数。为求 $\nabla_{\vec{w}}^2 l$，先求微分： $\mathrm{d}\nabla_{\vec{w}} l = \vec{x} \sigma’(\vec{x}^T\vec{w})\vec{x}^T \mathrm{d}\vec{w}$，其中 $\sigma’(a) = \frac{\exp(a)}{(1+\exp(a))^2}$ 为 sigmoid 函数的导数，对照导数与微分的联系，得到 $\nabla_{\vec{w}}^2 l = \vec{x}\sigma’(\vec{x}^T\vec{w})\vec{x}^T$。</p>

<p><strong>推广</strong>：样本( $\vec{x}<em>1, y_1), \dots, (\vec{x}_n,y_n)$，$l = \sum</em>{i=1}^N \left(-y_i \vec{x}<em>i^T\vec{w} + \log(1+\exp(\vec{x_i}^T\vec{w}))\right)$，求 $\nabla</em>{\vec{w}} l$ 和 $\nabla_{\vec{w}}^2 l$。 有两种方法，方法一：先对每个样本求导，然后相加；方法二：定义矩阵 $\vec{X} = \begin{bmatrix}\vec{x}<em>1^T \ \vdots \ \vec{x}_n^T \end{bmatrix}$，向量 $\vec{y} = \begin{bmatrix}y_1 \ \vdots \ y_n\end{bmatrix}$，将 $l$ 写成矩阵形式 $l = -\vec{y}^T \vec{X}\vec{w} + \vec{1}^T\log(\vec{1} + \exp(\vec{X}\vec{w}))$，进而可以求得
 $\nabla</em>{\vec{w}} l = \vec{X}^T(\sigma(\vec{X}\vec{w}) - \vec{y})$，$\nabla_{\vec{w}}^2 l = \vec{X}^T\text{diag}(\sigma’(\vec{X}\vec{w}))\vec{X}$。</p>

<p><strong>例5【多元 logistic 回归】</strong>：$l = -\vec{y}^T\log \mathrm{softmax}(\vec{Wx})=<script type="math/tex">-\vec{y}^T\vec{Wx}+</script>\log(\vec{1}^T\exp(\vec{Wx}))$，求 $\nabla_{\vec{W}} l$ 和 $\nabla_{\vec{W}}^2 l$。</p>

<p><strong>解</strong>：上篇例 3 中已求得 $\nabla_{\vec{W}} l = (\mathrm{softmax}(\vec{Wx})-\vec{y})\vec{x}^T$。为求 $\nabla_{\vec{W}}^2 l$，先求微分：定义 $\vec{a} = \vec{Wx}，\mathrm{d}\mathrm{softmax}(\vec{a}) = \frac{\exp(\vec{a})\odot \mathrm{d}\vec{a}}{\vec{1}^T\exp(\vec{a})} - \frac{\exp(\vec{a}) (\vec{1}^T(\exp(\vec{a})\odot \mathrm{d}\vec{a}))}{(\vec{1}^T\exp(\vec{a}))^2}$，这里需要化简去掉逐元素乘法，第一项中 $\exp(\vec{a})\odot \mathrm{d}\vec{a} = \mathrm{diag}(\exp(\vec{a})) \mathrm{d}\vec{a} $，第二项中 $\vec{1}^T(\exp(\vec{a})\odot \mathrm{d}\vec{a}) = \exp(\vec{a})^T\mathrm{d}\vec{a}$，故有 $\mathrm{d}\mathrm{softmax}(\vec{a}) = \mathrm{softmax}’(\vec{a})\mathrm{d}\vec{a}$，其中
 $\mathrm{softmax}’(\vec{a}) = \frac{\mathrm{diag}(\exp(\vec{a}))}{\vec{1}^T\exp(\vec{a})} - \frac{\exp(\vec{a})\exp(\vec{a})^T}{(\vec{1}^T\exp(\vec{a}))^2} $，代入有 $\mathrm{d}\nabla_{\vec{W}} l = \mathrm{softmax}’(\vec{a})\mathrm{d}\vec{a}\vec{x}^T = \mathrm{softmax}’(\vec{Wx})\mathrm{d}\vec{Wx}\vec{x}^T$，做向量化并使用矩阵乘法的技巧，得到 $\nabla_{\vec{W}}^2 l = (\vec{x}\vec{x}^T) \otimes \mathrm{softmax}’(\vec{Wx})$。</p>

<p>最后做个总结。我们发展了从整体出发的矩阵求导的技术，导数与微分的联系是计算的枢纽，标量对矩阵的导数与微分的联系是 $\mathrm{d}f = \mathrm{tr}(\nabla_{\vec{X}}^T f \mathrm{d}\vec{X})$，先对 $f$ 求微分，再使用迹技巧可求得导数，特别地，标量对向量的导数与微分的联系是 $\mathrm{d}f = \nabla_{\vec{x}}f^T \mathrm{d}\vec{x}$；矩阵对矩阵的导数与微分的联系是 $\mathrm{vec}(\mathrm{d}\vec{F}) = \frac{\partial \vec{F}}{\partial \vec{X}}^T \mathrm{vec}(\mathrm{d}\vec{X})$，先对 $\vec{F}$  求微分，再使用向量化的技巧可求得导数，特别地，向量对向量的导数与微分的联系是 $\mathrm{d}\vec{f} = \frac{\partial \vec{f}}{\partial \vec{x}}^T\mathrm{d}\vec{x}$。</p>

<h2 id="section-3">4 反向传播算法的完整向量形式推导</h2>

<p>首先定义一些常用的变量：</p>

<ul>
  <li>$n^l$：第 $l$ 层的神经元个数；</li>
  <li>$f_l(\cdot)$：第 $l$ 层的激活函数；</li>
  <li>$\vec{W}^l\in \mathbb{R}^{n^l\times n^{l-1}}$：第 $l-1$ 层到第 $l$ 层的权重矩阵；</li>
  <li>$\vec{b}^l\in \mathbb{R}^{n^l}$：第 $l-1$ 层到第 $l$ 层的偏置；</li>
  <li>$\vec{z}^l\in \mathbb{R}^{n^l}$：第 $l$ 层的输入；</li>
  <li>$\vec{a}^l\in \mathbb{R}^{n^l}$：第 $l$ 层的输出。</li>
</ul>

<p>在每一层的正向传播过程计算过程如下所示：
$$
\begin{align}</p>

<div class="highlighter-rouge"><pre class="highlight"><code>\vec{z}^l &amp;= \vec{W}^l\vec{a}^{(l-1)} + \vec{b}^l \\
\vec{a}^l &amp;= f_l(\vec{z}^l) \end{align} $$ 为进行梯度计算，我们需要先写出目标函数： $$ \begin{equation} J(\vec{W},\vec{b})=\sum_{i=1}^{N}J(\vec{W},\vec{b};\vec{x}^i,y^i)+\frac{1}{2}\lambda||\vec{W}||_F^2 \end{equation} $$ 其中 $||\vec{W}||_F^2=\sum_{l=1}^L\sum_{j=1}^{n^{l+1}}\sum_{i=1}^{n^l}W_{ij}^l$。那么对 $\vec{W}$ 和 $\vec{b}$ 的更新公式理论上可以写成： $$ \begin{align}
\vec{W}^l &amp;= \vec{W}^l-\alpha\frac{\partial J(\vec{W},\vec{b})}{\partial \vec{W}^l} \\
          &amp;= \vec{W}^l-\alpha\bigg( \sum_{i=1}^N \frac{\partial J(\vec{W},\vec{b};\vec{x}^i,y^i)}{\partial \vec{W}^l} + \lambda\vec{W}^l \bigg) \label{eq1:dw}\\
\vec{b}^l &amp;= \vec{b}^l-\alpha\frac{\partial J(\vec{W},\vec{b})}{\partial \vec{b}^l} \\
          &amp;= \vec{b}^l-\alpha \sum_{i=1}^N \frac{\partial J(\vec{W},\vec{b};\vec{x}^i,y^i)}{\partial \vec{b}^l} \label{eq1:db} \end{align} $$ 我们考虑每一个单独样本的梯度，先假设 $\frac{\partial J(\vec{W},\vec{b};\vec{x},y)}{\partial \vec{z}^l}=\vec{\delta}^l$，根据第一部分矩阵导数与微分的联系，有 $\mathrm{d}J=\mathrm{tr}\bigg(\vec{\delta}^{lT}\mathrm{d}\vec{z}^l\bigg)$，而 $\vec{z}^l = \vec{W}^l\vec{a}^{(l-1)} + \vec{b}^l$，所以 $\mathrm{d}\vec{z}^l = \mathrm{d}\vec{W}^l\vec{a}^{(l-1)}$，因此， $$ \begin{align}
\mathrm{d}J &amp;= \mathrm{tr}\bigg( \vec{\delta}^{lT}\mathrm{d}\vec{W}^l\vec{a}^{(l-1)} \bigg)\\
            &amp;= \mathrm{tr}\bigg( \vec{a}^{(l-1)}\vec{\delta}^{lT}\mathrm{d}\vec{W}^l \bigg) \end{align} $$ 所以最后有 $$ \begin{equation}
\frac{\partial J(\vec{W},\vec{b};\vec{x},y)}{\partial \vec{W}^l}=\vec{\delta}^l\vec{a}^{(l-1)T}. \label{eq2:dw} \end{equation} $$ 同理有 $$ \begin{equation}
\frac{\partial J(\vec{W},\vec{b};\vec{x},y)}{\partial \vec{b}^l}=\vec{\delta}^l. \label{eq2:db} \end{equation} $$ 下面求 $\vec{\delta}^l$，假设已知 $\frac{\partial J(\vec{W},\vec{b};\vec{x},y)}{\partial \vec{z}^{(l+1)}}=\vec{\delta}^{(l+1)}$，则有 $\mathrm{d}J=\mathrm{tr}\bigg(\vec{\delta}^{(l+1)T}\mathrm{d}\vec{z}^{(l+1)}\bigg)$，而 $\vec{z}^{(l+1)} = \vec{W}^{(l+1)}\vec{a}^l + \vec{b}^{(l+1)}$，所以 $\mathrm{d}\vec{z}^{(l+1)} = \vec{W}^{(l+1)} \mathrm{d}\vec{a}^l$，又因为 $\vec{a}^l=f_l(\vec{z}^l)$ 是逐元素的，所以 $\mathrm{d}\vec{a}^l=\mathrm{diag}(f'_l(\vec{z}^l))\mathrm{d}\vec{z}^l$，代入后有 $$ \begin{equation}
\mathrm{d}J=\mathrm{tr}\bigg( \vec{\delta}^{(l+1)T}\vec{W}^{(l+1)}\mathrm{diag}(f'_l(\vec{z}^l))\mathrm{d}\vec{z}^l \bigg). \end{equation} $$ 所以有 $$ \begin{equation}
\vec{\delta}^l=\frac{\partial J(\vec{W},\vec{b};\vec{x},y)}{\partial \vec{z}^l}=\mathrm{diag}(f'_l(\vec{z}^l))\vec{W}^{(l+1)T}\vec{\delta}^{(l+1)}. \label{eq2:delta} \end{equation} $$ 其中当 $f(\vec{x})=\mathrm{sigmoid}(\vec{x})$ 时，有 $f'(\vec{x})=f(\vec{x})(1-f(\vec{x}))$，当 $f(\vec{x})=\mathrm{tanh}(\vec{x})$ 时，有 $f'(\vec{x})=1-f(\vec{x})^2$。
</code></pre>
</div>

<p>最后考虑最后一层的 $\vec{\delta}^{(l+1)}$。当使用交叉熵损失函数时（以下 $\vec{a}$ 和 $\vec{z}$ 都略去上标 $(l+1)$）
<script type="math/tex">\begin{equation}
    J(\vec{W},\vec{b};\vec{x},y)=-\vec{y}^T\ln\vec{a}-(\vec{1}-\vec{y})^T\ln(1-\vec{a}).
\end{equation}</script>
这时
<script type="math/tex">% <![CDATA[
\begin{align}
    \mathrm{d}J &= -\vec{y}^T\bigg(\frac{1}{\vec{a}} \odot \mathrm{d}\vec{a}\bigg)-(\vec{1}-\vec{y})^T\bigg(\frac{-1}{\vec{1}-\vec{a}}\odot \mathrm{d}\vec{a}\bigg) \\
                &= -\bigg(\vec{y} \odot \frac{1}{\vec{a}} \bigg)^T \mathrm{d}\vec{a} + \bigg( (\vec{1}-\vec{y})\odot\frac{1}{\vec{1}-\vec{a}}\bigg)^T \mathrm{d}\vec{a} \\
                &= \bigg(-\vec{y} \odot \frac{1}{\vec{a}} + (\vec{1}-\vec{y})\odot\frac{1}{\vec{1}-\vec{a}}\bigg)^T \mathrm{d}\vec{a}
\end{align} %]]></script>
又因为 $\mathrm{d}\vec{a}=\mathrm{diag}(\mathrm{sigmoid}’(\vec{z}))\mathrm{d}\vec{z}=\mathrm{diag}(\vec{a(1-a)})\mathrm{d}\vec{z}​$，代入上式有
<script type="math/tex">% <![CDATA[
\begin{align}
    \mathrm{d}J &= \bigg(-\vec{y} \odot \frac{1}{\vec{a}} + (\vec{1-y})\odot\frac{1}{\vec{1-a}}\bigg)^T \mathrm{diag}(\vec{a(1-a)})\mathrm{d}\vec{z} \\
                &= \bigg(-\mathrm{diag}(\vec{a(1-a)}) \vec{y} \odot \frac{1}{\vec{a}} + \mathrm{diag}(\vec{a(1-a)})(\vec{1-y})\odot\frac{1}{\vec{1-a}}\bigg)^T \mathrm{d}\vec{z} \\
                &= (-\vec{y}+\vec{y}\odot\vec{a} + \vec{a} - \vec{a}\odot\vec{y})^T \mathrm{d}\vec{z} \\
                &= (-\vec{y}+\vec{a})^T \mathrm{d}\vec{z} 
\end{align} %]]></script>
所以有
<script type="math/tex">\begin{equation}
    \vec{\delta}^{(l+1)}=\frac{\partial J(\vec{W},\vec{b};\vec{x},y)}{\partial \vec{z}^{(l+1)}}=-\vec{y}+\vec{a}^{(l+1)}. \label{eq:delta}
\end{equation}</script>
最后，将式 \ref{eq:delta} 和式 \ref{eq2:db} 代入式 \ref{eq1:db} 则得到偏置项 $\vec{b}$ 的更新公式，将式 \ref{eq:delta}，式 \ref{eq2:delta} 和式 \ref{eq2:dw} 代入式 \ref{eq1:dw} 则得到 $\vec{W}$ 的更新公式。</p>

        </article>
        <hr>

        
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
        
            
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
        
        

        <div class="post-recent">
    <div class="pre">
        
        <p><strong>上一篇</strong> <a href="/2016/03/20/GitHub-Pages%E4%B8%8A%E8%BE%93%E5%87%BA%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/">GitHub Pages上输出数学公式</a></p>
        
    </div>
    <div class="nex">

        
        <p><strong>下一篇</strong> <a href="/2018/05/08/%E6%84%8F%E8%AF%86%E8%AF%9E%E7%94%9F%E4%BA%8E%E4%B8%8A%E5%B8%9D%E6%B2%89%E9%BB%98%E6%97%B6/">意识诞生于上帝沉默时</a></p>
        
    </div>
</div>


        <h2 id="comments">Comments</h2>
        


<div id="disqus_thread"></div>
<script>
    /**
     * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
     * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
     */

    var disqus_config = function() {
        this.page.url = 'https://caoxiaoqing.github.io/2017/12/19/test/'; // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'https://caoxiaoqing.github.io/2017/12/19/test/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };

    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document,
            s = d.createElement('script');

        s.src = '//caoxiaoqing.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>




    </div>
    <button class="anchor"><i class="fa fa-anchor"></i></button>
    <div class="right">
        <div class="wrap">

            <!-- Content -->
            <div class="side content">
                <div>
                    Content
                </div>
                <ul id="content-side" class="content-ul">
                    
                    <li><a href="#comments">Comments</a></li>
                </ul>
            </div>
            <!-- 其他div框放到这里 -->
            <!-- <div class="side">bbbb</div> -->
        </div>
    </div>
</div>
<script>
/**
 * target _blank
 */
(function() {
    var aTags = document.querySelectorAll('article a:not([id])')
    for (var i = 0; i < aTags.length; i++) {
        aTags[i].setAttribute('target', '_blank')
    }
}());
</script>
<script src="/js/pageContent.js " charset="utf-8"></script>


    <footer class="site-footer">


    <div class="wrapper">

        <p class="description">
             曹孝卿的博客 
        </p>
        <p class="contact">
            Contact me at: 
            <a href="https://github.com/caoxiaoqing" title="GitHub"><i class="fa fa-github" aria-hidden="true"></i></a>  
            <a href="mailto:CaoXiaoqing2008@gmail.com" title="email"><i class="fa fa-envelope-o" aria-hidden="true"></i></a>        
        </p>
        <p>
            本站总访问量<span id="busuanzi_value_site_pv"></span>次，本站访客数<span id="busuanzi_value_site_uv"></span>人次，本文总阅读量<span id="busuanzi_value_page_pv"></span>次
        </p>
        <p class="power">
            <span>
                Site powered by <a href="https://jekyllrb.com/">Jekyll</a> & <a href="https://pages.github.com/">Github Pages</a>.
            </span>
        </p>
    </div>
</footer>
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <div class="back-to-top">
    <a href="#top" data-scroll>
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
    </a>
</div>

    <script src=" /js/main.js " charset="utf-8"></script>
    <script src=" /js/smooth-scroll.min.js " charset="utf-8"></script>
    <script type="text/javascript">
      smoothScroll.init({
        speed: 500, // Integer. How fast to complete the scroll in milliseconds
        easing: 'easeInOutCubic', // Easing pattern to use
        offset: 20, // Integer. How far to offset the scrolling anchor location in pixels
      });
    </script>
    <!-- <script src=" /js/scroll.min.js " charset="utf-8"></script> -->
  </body>

</html>
