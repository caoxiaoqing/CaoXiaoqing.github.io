<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>矩阵求导</title>
    <meta name="description" content="矩阵求导的技术，在统计学、控制论、机器学习等领域有广泛的应用。本文来做个科普，分作两部分，第一部分讲标量对矩阵的求导术，第二部分讲矩阵对矩阵的求导术。本文使用小写字母  表示标量，粗体小写字母  表示向量，大写字母  表示矩阵。本文第一部分标量对矩阵的求导整理自：这里，第二部分矩阵对矩阵的求导整理自：那里 ，第三...">

    <link rel="shortcut icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://at.alicdn.com/t/font_8v3czwksspqlg14i.css">
    <link rel="stylesheet" href="/css/main.css ">
    <link rel="canonical" href="https://caoxiaoqing.github.io/2017/12/19/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/">
    <link rel="alternate" type="application/rss+xml" title="CXQ" href="https://caoxiaoqing.github.io/feed.xml ">


    <script>
    // 百度统计代码
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?bf7bdaf2bdac297bf8b3a6797ec58ce0";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
    </script>



<script type="text/javascript" 
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


</head>


  <body>

    <header id="top">
    <div class="wrapper">
        <a href="/" class="brand">CXQ</a>
        <small>free your mind</small>
        <button id="headerMenu" class="menu"><i class="fa fa-bars"></i></button>
        <nav id="headerNav">
            <ul>
                <li>
                    
                    <a href="/">
                    
                        <i class="fa fa-home"></i>Home
                    </a>
                </li>

                
                    
                    <li>
                        
                        <a href="/archive/">
                        
                            <i class="fa fa-archive"></i>Archives
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/category/">
                        
                            <i class="fa fa-th-list"></i>Categories
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/tag/">
                        
                            <i class="fa fa-tags"></i>Tags
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/collection/">
                        
                            <i class="fa fa-bookmark"></i>Collections
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/about/">
                        
                            <i class="fa fa-heart"></i>About
                        </a>
                    </li>
                    
                
                    
                
                    
                
                    
                
            </ul>
        </nav>
    </div>
</header>


        <div class="page clearfix" post>
    <div class="left">
        <h1>矩阵求导</h1>
        <div class="label">

            <div class="label-card">
                <i class="fa fa-calendar"></i>2017-12-19
            </div>

            <div class="label-card">
                <i class="fa fa-user"></i>caoxiaoqing
                
            </div>

            <div class="label-card">
                
            </div>

            <div class="label-card">
            


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#算法" title="Category: 算法" rel="category">算法</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


            </div>

            <div class="label-card">
            
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <!--a href="/tag/#%E7%9F%A9%E9%98%B5" title="Tag: 矩阵" rel="tag">矩阵</a-->
        <a href="/tag/#矩阵" title="Tag: 矩阵" rel="tag">矩阵</a>&nbsp;
    
        <!--a href="/tag/#%E6%B1%82%E5%AF%BC" title="Tag: 求导" rel="tag">求导</a-->
        <a href="/tag/#求导" title="Tag: 求导" rel="tag">求导</a>
    
  

</span>

            </div>

        </div>
        <hr>
        <article itemscope itemtype="http://schema.org/BlogPosting">
        <ul id="markdown-toc">
  <li><a href="#section" id="markdown-toc-section">1 标量对矩阵的求导</a></li>
  <li><a href="#section-1" id="markdown-toc-section-1">2 矩阵对矩阵的求导</a></li>
  <li><a href="#section-2" id="markdown-toc-section-2">3 反向传播算法的完整向量形式推导</a></li>
</ul>

<p>矩阵求导的技术，在统计学、控制论、机器学习等领域有广泛的应用。本文来做个科普，分作两部分，第一部分讲标量对矩阵的求导术，第二部分讲矩阵对矩阵的求导术。本文使用小写字母 <script type="math/tex">x</script> 表示标量，粗体小写字母 <script type="math/tex">\vec{x}</script> 表示向量，大写字母 <script type="math/tex">\vec{X}</script> 表示矩阵。本文第一部分标量对矩阵的求导整理自：<a href="https://zhuanlan.zhihu.com/p/24709748">这里</a>，第二部分矩阵对矩阵的求导整理自：<a href="https://zhuanlan.zhihu.com/p/24863977">那里</a> ，第三部分则是根据以上两部分提供的方法，完成了 BP 算法的完整向量形式推导。</p>

<h2 id="section">1 标量对矩阵的求导</h2>

<p>首先来琢磨一下定义，标量 <script type="math/tex">f</script> 对矩阵 <script type="math/tex">\vec{X}</script> 的导数，定义为</p>

<script type="math/tex; mode=display">\frac{\partial f}{\partial \vec{X}} = \left[\frac{\partial f }{\partial \vec{X}_{ij}}\right].</script>

<p>即 <script type="math/tex">f</script> 对 <script type="math/tex">\vec{X}</script> 逐元素求导排成与 <script type="math/tex">\vec{X}</script> 尺寸相同的矩阵。然而，这个定义在计算中并不好用，实用上的原因是在对较复杂的函数难以逐元素求导；哲理上的原因是逐元素求导破坏了整体性。试想，为何要将 <script type="math/tex">f</script> 看做矩阵 <script type="math/tex">\vec{X}</script> 而不是各元素 <script type="math/tex">\vec{X}_{ij}</script> 的函数呢？答案是用矩阵运算更整洁。所以在求导时不宜拆开矩阵，而是要找一个从整体出发的算法。为此，我们来回顾，一元微积分中的导数（标量对标量的导数）与微分有联系：</p>

<script type="math/tex; mode=display">\mathrm{d}f = f'(x)\mathrm{d}x.</script>

<p>多元微积分中的梯度（标量对向量的导数）也与微分有联系：</p>

<script type="math/tex; mode=display">\begin{equation}
  \mathrm{d}f = \sum_{i} \frac{\partial f}{\partial x_i}\mathrm{d}x_i = \frac{\partial f}{\partial \vec{x}}^T \mathrm{d}\vec{x}.
\end{equation}</script>

<p>这里第一个等号是全微分公式，第二个等号表达了梯度 <script type="math/tex">\frac{\partial f}{\partial \vec{x}}</script> 与微分的联系。受此启发，我们将矩阵导数与微分建立联系：</p>

<script type="math/tex; mode=display">\begin{equation}
  \mathrm{d}f = \sum_{i,j} \frac{\partial f}{\partial \vec{X}{ij}}\mathrm{d}\vec{X}_{ij} = \mathrm{tr}\left(\frac{\partial f}{\partial \vec{X}}^T \mathrm{d}\vec{X}\right).
\end{equation}</script>

<p>这里 <script type="math/tex">\mathrm{tr}</script> 代表迹(trace)是方阵对角线元素之和，满足性质：对尺寸相同的矩阵 <script type="math/tex">\vec{A}, \vec{B}</script>，有 <script type="math/tex">\mathrm{tr}(\vec{A}^T\vec{B}) = \sum_{i,j}\vec{A}_{ij}\vec{B}_{ij}</script>，即 <script type="math/tex">\mathrm{tr}(\vec{A}^T\vec{B})</script> 是矩阵 <script type="math/tex">\vec{A}, \vec{B}</script> 的内积，因此上式与原定义相容。</p>

<p>然后来建立运算法则。回想遇到较复杂的一元函数如 <script type="math/tex">f = \log(2+\sin x)e^{\sqrt{x}}</script>，我们是如何求导的呢？通常不是从定义开始求极限，而是先建立了初等函数求导和四则运算、复合等法则，再来运用这些法则。故而，我们来创立常用的矩阵微分的运算法则：</p>

<ul>
  <li>加减法：<script type="math/tex">\mathrm{d}(\vec{X} \pm \vec{Y}) = \mathrm{d}\vec{X} \pm \mathrm{d}\vec{Y}</script>；</li>
  <li>矩阵乘法：<script type="math/tex">\mathrm{d}(\vec{X}\vec{Y}) = \mathrm{d}\vec{X}\vec{Y} + \vec{X}\mathrm{d}\vec{Y}</script>；</li>
  <li>转置：<script type="math/tex">\mathrm{d}(\vec{X}^T) = (\mathrm{d}\vec{X})^T</script>；</li>
  <li>迹：<script type="math/tex">\mathrm{d}\mathrm{tr}(\vec{X}) = \mathrm{tr}(\mathrm{d}\vec{X})</script>；</li>
  <li>逆：<script type="math/tex">\mathrm{d}\vec{X}^{-1} = -\vec{X}^{-1}\mathrm{d}\vec{X}\vec{X}^{-1}</script>，此式可在 <script type="math/tex">\vec{X}\vec{X}^{-1}=\vec{I}</script> 两侧求微分来证明；</li>
  <li>行列式：<script type="math/tex">\mathrm{d}\lvert\vec{X}\rvert = \mathrm{tr}(\vec{X}^{\#}\mathrm{d}\vec{X})</script>，其中 <script type="math/tex">\vec{X}^{\#}</script> 表示 <script type="math/tex">\vec{X}</script> 的伴随矩阵，在 <script type="math/tex">\vec{X}</script> 可逆时又可以写作 <script type="math/tex">\mathrm{d}\lvert\vec{X}\rvert= \lvert\vec{X}\rvert\mathrm{tr}(\vec{X}^{-1}\mathrm{d}\vec{X})</script>，此式可用Laplace展开来证明，详见张贤达《矩阵分析与应用》第279页；</li>
  <li>逐元素乘法：<script type="math/tex">\mathrm{d}(\vec{X}\odot \vec{Y}) = \mathrm{d}\vec{X}\odot \vec{Y} + \vec{X}\odot \mathrm{d}\vec{Y}</script>，<script type="math/tex">\odot</script> 表示尺寸相同的矩阵 <script type="math/tex">\vec{X},\vec{Y}</script> 逐元素相乘；</li>
  <li>逐元素函数：<script type="math/tex">\mathrm{d}\sigma(\vec{X}) = \sigma'(\vec{X})\odot \mathrm{d}\vec{X}</script>，<script type="math/tex">\sigma(\vec{X}) = \left[\sigma(\vec{X}_{ij})\right]</script> 是逐元素运算的标量函数。</li>
</ul>

<p>我们试图利用矩阵导数与微分的联系 <script type="math/tex">\mathrm{d}f = \mathrm{tr}\left(\frac{\partial f}{\partial \vec{X}}^T \mathrm{d}\vec{X}\right)</script>，在求出左侧的微分 <script type="math/tex">\mathrm{d}f</script> 后，该如何写成右侧的形式并得到导数呢？这需要一些迹技巧(trace trick)：</p>

<ul>
  <li>标量套上迹：<script type="math/tex">a = \mathrm{tr}(a)</script>；</li>
  <li>转置：<script type="math/tex">\mathrm{tr}(\vec{A}^T) = \mathrm{tr}(\vec{A})</script>；</li>
  <li>线性：<script type="math/tex">\mathrm{tr}(\vec{A}\pm \vec{B}) = \mathrm{tr}(\vec{A})\pm \mathrm{tr}(\vec{B})</script>；</li>
  <li>矩阵乘法交换：<script type="math/tex">\mathrm{tr}(\vec{AB}) = \mathrm{tr}(\vec{BA})</script>，两侧都等于 <script type="math/tex">\sum_{i,j}\vec{A}_{ij}\vec{B}_{ji}</script>；</li>
  <li>矩阵乘法/逐元素乘法交换：<script type="math/tex">\mathrm{tr}(\vec{A}^T(\vec{B}\odot \vec{C})) = \mathrm{tr}((\vec{A}\odot \vec{B})^T\vec{C})</script>，两侧都等于 <script type="math/tex">\sum_{i,j}\vec{A}_{ij}\vec{B}_{ij}\vec{C}_{ij}</script>。</li>
</ul>

<p>观察一下可以断言，若标量函数 <script type="math/tex">f</script> 是矩阵 <script type="math/tex">\vec{X}</script> 经加减乘法、行列式、逆、逐元素函数等运算构成，则使用相应的运算法则对 <script type="math/tex">f</script> 求微分，再使用迹技巧给 <script type="math/tex">\mathrm{d}f</script> 套上迹并将其它项交换至 <script type="math/tex">\mathrm{d}\vec{X}</script> 左侧，即能得到导数。</p>

<p>在建立法则的最后，来谈一谈复合：假设已求得 <script type="math/tex">\frac{\partial f}{\partial \vec{Y}}</script>，而 <script type="math/tex">\vec{Y}</script> 是 <script type="math/tex">\vec{X}</script> 的函数，如何求 <script type="math/tex">\frac{\partial f}{\partial \vec{X}}</script> 呢？在微积分中有标量求导的链式法则 <script type="math/tex">\frac{\partial f}{\partial x} = \frac{\partial f}{\partial y} \frac{\partial y}{\partial x}</script>，但这里我们不能沿用链式法则，因为矩阵对矩阵的导数 <script type="math/tex">\frac{\partial \vec{Y}}{\partial \vec{X}}</script> 截至目前仍是未定义的。于是我们继续追本溯源，链式法则是从何而来？源头仍然是微分。我们直接从微分入手建立复合法则：先写出 <script type="math/tex">\mathrm{d}f = \mathrm{tr}\left(\frac{\partial f}{\partial \vec{Y}}^T \mathrm{d}\vec{Y}\right)</script>，再将 <script type="math/tex">\mathrm{d}\mathbf{Y}</script> 用 <script type="math/tex">\mathrm{d}\vec{X}</script> 表示出来代入，并使用迹技巧将其他项交换至 <script type="math/tex">\mathrm{d}\vec{X}</script> 左侧，即可得到 <script type="math/tex">\frac{\partial f}{\partial \vec{X}}</script>。</p>

<p>接下来演示一些算例。特别提醒要依据已经建立的运算法则来计算，不能随意套用微积分中标量导数的结论，比如认为 <script type="math/tex">\vec{A}\vec{X}</script> 对 <script type="math/tex">\vec{X}</script> 的导数为 <script type="math/tex">\vec{A}</script>，这是没有根据、意义不明的。</p>

<p><strong>例1</strong>：<script type="math/tex">f = \vec{a}^T \vec{X}\vec{b}</script>，求 <script type="math/tex">\frac{\partial f}{\partial \vec{X}}</script>。</p>

<p>解：先使用矩阵乘法法则求微分：<script type="math/tex">\mathrm{d}f = \vec{a}^T \mathrm{d}\vec{X}\vec{b}</script>，再套上迹并做交换：<script type="math/tex">\mathrm{d}f = \mathrm{tr}(\vec{a}^T\mathrm{d}\vec{X}\vec{b}) = \mathrm{tr}(\vec{b}\vec{a}^T\mathrm{d}\vec{X})</script>，对照导数与微分的联系，得到 <script type="math/tex">\frac{\partial f}{\partial \vec{X}} = \vec{a}\vec{b}^T</script>。</p>

<p>注意：这里不能用 <script type="math/tex">\frac{\partial f}{\partial \vec{X}} =\vec{a}^T \frac{\partial \vec{X}}{\partial \vec{X}}\vec{b}=?</script>，导数与乘常数矩阵的交换是不合法则的运算（而微分是合法的）。有些资料在计算矩阵导数时，会略过求微分这一步，这是逻辑上解释不通的。</p>

<p><strong>例2【线性回归】</strong>：<script type="math/tex">l = \|\vec{X}\vec{w}- \vec{y}\|^2</script>，求 <script type="math/tex">\frac{\partial l}{\partial \vec{w}}</script>。</p>

<p>解：严格来说这是标量对向量的导数，不过可以把向量看做矩阵的特例。将向量范数写成 <script type="math/tex">l = (\vec{X}\vec{w}- \vec{y})^T(\vec{X}\vec{w}- \vec{y})</script>，求微分，使用矩阵乘法、转置等法则：<script type="math/tex">\mathrm{d}l = (\vec{X}\mathrm{d}\vec{w})^T(\vec{X}\vec{w}-\vec{y})+(\vec{X}\vec{w}-\vec{y})^T(\vec{X}\mathrm{d}\vec{w}) = 2(\vec{X}\vec{w}-\vec{y})^T\vec{X}\mathrm{d}\vec{w}</script>。 对照导数与微分的联系，得到 <script type="math/tex">\frac{\partial l}{\partial \vec{w}}= 2\vec{X}^T(\vec{X}\vec{w}-\vec{y})</script>。</p>

<p><strong>例3【多元 logistic 回归】</strong>：<script type="math/tex">l = -\vec{y}^T\log \mathrm{softmax}(\vec{Wx})</script>$，求 <script type="math/tex">\frac{\partial l}{\partial \vec{W}}</script>。其中 <script type="math/tex">\vec{y}</script> 是除一个元素为 1 外其它元素为 0 的向量；<script type="math/tex">\mathrm{softmax}(\vec{a}) = \frac{\exp(\vec{a})}{\vec{1}^T\exp(\vec{a})}</script>，其中 <script type="math/tex">\exp(\vec{a})</script> 表示逐元素求指数，<script type="math/tex">\vec{1}</script> 代表全 1 向量。</p>

<p>解：首先将 softmax 函数代入并写成 <script type="math/tex">l = -\vec{y}^T \left(\log (\exp(\vec{Wx}))-\vec{1}\log(\vec{1}^T\exp(\vec{Wx}))\right) = -\vec{y}^T\vec{Wx} + \log(\vec{1}^T\exp(\vec{Wx}))</script>，这里要注意逐元素 <script type="math/tex">\log</script> 满足等式 <script type="math/tex">\log(\vec{u}/c) = \log(\vec{u}) - \vec{1}\log(c)</script>，以及 <script type="math/tex">\vec{y}</script> 满足 <script type="math/tex">\vec{y}^T \vec{1} = 1</script>。求微分，使用矩阵乘法、逐元素函数等法则：<script type="math/tex">\mathrm{d}l = -\vec{y}^T\mathrm{d}\vec{Wx}+\frac{\vec{1}^T\left(\exp(\vec{Wx})\odot(\mathrm{d}\vec{Wx})\right)}{\vec{1}^T\exp(\vec{Wx})}</script>。
再套上迹并做交换，注意可化简 <script type="math/tex">\vec{1}^T\left(\exp(\vec{Wx})\odot(\mathrm{d}\vec{Wx})\right) = \exp(\vec{Wx})^T\mathrm{d}\vec{Wx}</script>，这是根据等式 <script type="math/tex">\vec{1}^T (\vec{u}\odot \vec{v}) = \vec{u}^T \vec{v}</script>，故 <script type="math/tex">\mathrm{d}l = \mathrm{tr}\left(-\vec{y}^T\mathrm{d}\vec{Wx}+\frac{\exp(\vec{Wx})^T\mathrm{d}\vec{Wx}}{\vec{1}^T\exp(\vec{Wx})}\right) =\mathrm{tr}(\vec{x}(\mathrm{softmax}(\vec{Wx})-\vec{y})^T\mathrm{d}\vec{W})</script>。对照导数与微分的联系，得到 <script type="math/tex">\frac{\partial l}{\partial \vec{W}}= (\mathrm{softmax}(\vec{Wx})-\vec{y})\vec{x}^T</script>。</p>

<p><strong>另解</strong>：定义 <script type="math/tex">\vec{a} = \vec{Wx}</script>，则 <script type="math/tex">l = -\vec{y}^T\log\mathrm{softmax}(\vec{a})</script>，先如上求出 <script type="math/tex">\frac{\partial l}{\partial \vec{a}} = \mathrm{softmax}(\vec{a})-\vec{y}</script>，再利用复合法则：<script type="math/tex">\mathrm{d}l = \mathrm{tr}\left(\frac{\partial l}{\partial \vec{a}}^T\mathrm{d}\vec{a}\right) = \mathrm{tr}\left(\frac{\partial l}{\partial \vec{a}}^T\mathrm{d}\vec{Wx}\right) = \mathrm{tr}\left(\vec{x}\frac{\partial l}{\partial \vec{a}}^T\mathrm{d}\vec{W}\right)</script>，得到 <script type="math/tex">\frac{\partial l}{\partial \vec{W}}= \frac{\partial l}{\partial\vec{a}}\vec{x}^T</script>。</p>

<p><strong>例4【方差的最大似然估计】</strong>：样本 <script type="math/tex">\vec{x}_1,\dots, \vec{x}_n\sim N(\vec{\mu}, \Sigma)</script>，其中 <script type="math/tex">\Sigma</script> 是对称正定矩阵，求方差 <script type="math/tex">\Sigma</script> 的最大似然估计。写成数学式是：<script type="math/tex">l = \log\lvert\Sigma\rvert+\frac{1}{n}\sum_{i=1}^n(\vec{x}_i-\vec{\bar{x}})^T\Sigma^{-1}(\vec{x}_i-\vec{\bar{x}})</script>，求 <script type="math/tex">\frac{\partial l }{\partial \Sigma}</script> 的零点。</p>

<p>解：首先求微分，使用矩阵乘法、行列式、逆等运算法则，第一项是 <script type="math/tex">\mathrm{d}\log\lvert\Sigma\rvert = \lvert\Sigma\rvert^{-1}\mathrm{d}\lvert\Sigma\rvert = \mathrm{tr}(\Sigma^{-1}\mathrm{d}\Sigma)</script>，第二项是 <script type="math/tex">\frac{1}{n}\sum_{i=1}^n(\vec{x}_i-\vec{\bar{x}})^T\mathrm{d}\Sigma^{-1}(\vec{x}_i-\vec{\bar{x}}) = -\frac{1}{n}\sum_{i=1}^n(\vec{x}_i-\vec{\bar{x}})^T\Sigma^{-1}\mathrm{d}\Sigma\Sigma^{-1}(\vec{x}_i-\vec{\bar{x}})</script>。再给第二项套上迹做交换：<script type="math/tex">\mathrm{d}l = \mathrm{tr}\left(\left(\Sigma^{-1}-\Sigma^{-1}\vec{S}\Sigma^{-1}\right)\mathrm{d}\Sigma\right)</script>，其中 <script type="math/tex">\vec{S} = \frac{1}{n}\sum_{i=1}^n(\vec{x}_i-\vec{\bar{x}})(\vec{x}_i-\vec{\bar{x}})^T</script>，定义为样本方差。对照导数与微分的联系，有 <script type="math/tex">\frac{\partial l }{\partial \Sigma}=(\Sigma^{-1}-\Sigma^{-1}\vec{S}\Sigma^{-1})^T</script>，其零点即 <script type="math/tex">\Sigma</script> 的最大似然估计为 <script type="math/tex">\Sigma = \vec{S}</script>。</p>

<p>最后一例留给经典的神经网络。神经网络的求导术是学术史上的重要成果，还有个专门的名字叫做 BP 算法，我相信如今很多人在初次推导 BP 算法时也会颇费一番脑筋，事实上使用矩阵求导术来推导并不复杂。为简化起见，我们推导二层神经网络的 BP 算法。</p>

<p><strong>例5【二层神经网络】</strong>：<script type="math/tex">l = -\vec{y}^T\log\mathrm{softmax}(\vec{W}_2\sigma(\vec{W}_1\vec{x}))</script>，求 <script type="math/tex">\frac{\partial l}{\partial \vec{W}_1}</script> 和 <script type="math/tex">\frac{\partial l}{\partial \vec{W}_2}</script>。其中 <script type="math/tex">\vec{y}</script> 是除一个元素为 <script type="math/tex">1</script> 外其它元素为 <script type="math/tex">0</script> 的向量，<script type="math/tex">\mathrm{softmax}(\vec{a}) = \frac{\exp(\vec{a})}{\vec{1}^T\exp(\vec{a})}</script> 同例 3，<script type="math/tex">\sigma(\cdot)</script> 是逐元素 sigmoid 函数 <script type="math/tex">\sigma(a) = \frac{1}{1+\exp(-a)}</script>。</p>

<p>解：定义 <script type="math/tex">\vec{a}_1=\vec{W}_1\vec{x}</script>，<script type="math/tex">\vec{h}_1 = \sigma(\vec{a}_1)</script>，<script type="math/tex">\vec{a}_2 = \vec{W}_2 \vec{h}_1</script>，则 <script type="math/tex">l =-\vec{y}^T\log\mathrm{softmax}(\vec{a}_2)</script>。 在例 3 中已求出 <script type="math/tex">\frac{\partial l}{\partial \vec{a}_2} = \mathrm{softmax}(\vec{a}_2)-\vec{y}</script>。使用复合法则，注意此处 <script type="math/tex">\vec{h}_1, \vec{W}_2</script> 都是变量：<script type="math/tex">\mathrm{d}l = \mathrm{tr}\left(\frac{\partial l}{\partial \vec{a}_2}^T\mathrm{d}\vec{a}_2\right) = \mathrm{tr}\left(\frac{\partial l}{\partial \vec{a}_2}^T\mathrm{d}\vec{W}_2 \vec{h}_1\right) + \mathrm{tr}\left(\frac{\partial l}{\partial \vec{a}_2}^T\vec{W}_2 \mathrm{d}\vec{h}_1\right)</script>，使用矩阵乘法交换的迹技巧从第一项得到 <script type="math/tex">\frac{\partial l}{\partial \vec{W}_2}= \frac{\partial l}{\partial\vec{a}_2}\vec{h}_1^T</script>，从第二项得到 <script type="math/tex">\frac{\partial l}{\partial \vec{h}_1}= \vec{W}_2^T\frac{\partial l}{\partial\vec{a}_2}</script>。接下来求 <script type="math/tex">\frac{\partial l}{\partial \vec{a}_1}</script>，继续使用复合法则，并利用矩阵乘法和逐元素乘法交换的迹技巧：<script type="math/tex">\mathrm{tr}\left(\frac{\partial l}{\partial\vec{h}_1}^T\mathrm{d}\vec{h}_1\right) = \mathrm{tr}\left(\frac{\partial l}{\partial\vec{h}_1}^T(\sigma'(\vec{a}_1)\odot \mathrm{d}\vec{a}_1)\right) = \mathrm{tr}\left(\left(\frac{\partial l}{\partial\vec{h}_1}\odot \sigma'(\vec{a}_1)\right)^T\mathrm{d}\vec{a}_1\right)</script>，得到 <script type="math/tex">\frac{\partial l}{\partial \vec{a}_1}= \frac{\partial l}{\partial\vec{h}_1}\odot\sigma'(\vec{a}_1)</script>。 为求 <script type="math/tex">\frac{\partial l}{\partial \vec{W}_1}</script>，再用一次复合法则：<script type="math/tex">\mathrm{tr}\left(\frac{\partial l}{\partial\vec{a}_1}^T\mathrm{d}\vec{a}_1\right) = \mathrm{tr}\left(\frac{\partial l}{\partial\vec{a}_1}^T\mathrm{d}\vec{W}_1\vec{x}\right) = \mathrm{tr}\left(\vec{x}\frac{\partial l}{\partial\vec{a}_1}^T\mathrm{d}\vec{W}_1\right)</script>，得到 <script type="math/tex">\frac{\partial l}{\partial \vec{W}_1}= \frac{\partial l}{\partial\vec{a}_1}\vec{x}^T</script>。</p>

<h2 id="section-1">2 矩阵对矩阵的求导</h2>

<p>矩阵对矩阵的求导采用了向量化的思路，常应用于二阶方法求解优化问题。</p>

<p>首先来琢磨一下定义。矩阵对矩阵的导数，需要什么样的定义？第一，矩阵 <script type="math/tex">\vec{F}(p\times q)</script> 对矩阵 <script type="math/tex">\vec{X}(m\times n)</script> 的导数应包含所有 <script type="math/tex">mnpq</script> 个偏导数 <script type="math/tex">\frac{\partial \vec{F}_{kl}}{\partial \vec{X}_{ij}}</script>，从而不损失信息；第二，导数与微分有简明的联系，因为在计算导数和应用中需要这个联系；第三，导数有简明的从整体出发的算法。我们先定义向量 <script type="math/tex">\vec{f}(p\times 1)</script> 对向量 <script type="math/tex">\vec{x}(m\times1)</script> 的导数</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
    \frac{\partial \vec{f}}{\partial \vec{x}} =
    \begin{bmatrix}
    \frac{\partial f_1}{\partial x_1} & \frac{\partial f_2}{\partial x_1} & \cdots & \frac{\partial f_p}{\partial x_1}\\ \frac{\partial f_1}{\partial x_2} & \frac{\partial f_2}{\partial x_2} & \cdots & \frac{\partial f_p}{\partial x_2}\\ \vdots & \vdots & \ddots & \vdots\\ \frac{\partial f_1}{\partial x_m} & \frac{\partial f_2}{\partial x_m} & \cdots & \frac{\partial f_p}{\partial x_m}\\ \end{bmatrix}(m\times p)，
\end{equation} %]]></script>

<p>有 <script type="math/tex">\mathrm{d}\vec{f} = \frac{\partial \vec{f} }{\partial \vec{x} }^T \mathrm{d}\vec{x}</script>；再定义矩阵的（按列优先）向量化</p>

<script type="math/tex; mode=display">\begin{equation}
    \mathrm{vec}(\vec{X}) = [\vec{X}_{11}, \ldots, \vec{X}_{m1}, \vec{X}_{12}, \ldots, \vec{X}_{m2}, \ldots, \vec{X}_{1n}, \ldots, \vec{X}_{mn}]^T(mn\times1),
\end{equation}</script>

<p>并定义矩阵 <script type="math/tex">\vec{F}</script> 对矩阵 <script type="math/tex">\vec{X}</script> 的导数：</p>

<script type="math/tex; mode=display">\begin{equation}
    \frac{\partial \vec{F}}{\partial \vec{X}} = \frac{\partial \mathrm{vec}(\vec{F})}{\partial \mathrm{vec}(\vec{X})}(mn\times pq).
\end{equation}</script>

<p>导数与微分有联系 <script type="math/tex">\mathrm{vec}(\mathrm{d}\vec{F}) = \frac{\partial \vec{F}}{\partial \vec{X}}^T \mathrm{vec}(\mathrm{d}\vec{X})</script>。几点说明如下：</p>

<ul>
  <li>按此定义，标量 <script type="math/tex">f</script> 对矩阵 <script type="math/tex">\vec{X}(m\times n)</script> 的导数 <script type="math/tex">\frac{\partial f}{\partial \vec{X}}</script> 是 <script type="math/tex">mn\times1</script> 向量，与上篇的定义不兼容，不过二者容易相互转换。为避免混淆，我们用 <script type="math/tex">\nabla_{\vec{X}} f</script> 表示上篇定义的 <script type="math/tex">m\times n</script> 矩阵，则有 <script type="math/tex">\frac{\partial f}{\partial \vec{X}}=\mathrm{vec}(\nabla_{\vec{X}} f)</script>。虽然本篇的技术可以用于标量对矩阵求导这种特殊情况，但使用上篇中的技术更方便。读者可以通过上篇中的算例试验两种方法的等价转换；</li>
  <li>标量对矩阵的二阶导数，又称 Hessian 矩阵，定义为 <script type="math/tex">\nabla^2_{\vec{X}} f = \frac{\partial^2 f}{\partial \vec{X}^2} = \frac{\partial \nabla_{\vec{X}} f}{\partial \vec{X}}(mn\times mn)</script>，是对称矩阵。对向量 <script type="math/tex">\frac{\partial f}{\partial \vec{X}}</script> 或矩阵 <script type="math/tex">\nabla_{\vec{X}} f</script> 求导都可以得到 Hessian 矩阵，但从矩阵 <script type="math/tex">\nabla_{\vec{X}} f</script> 出发更方便；</li>
  <li><script type="math/tex">\frac{\partial \vec{F}}{\partial \vec{X}} = \frac{\partial\mathrm{vec} (\vec{F})}{\partial \vec{X}} = \frac{\partial \vec{F}}{\partial \mathrm{vec}(\vec{X})} = \frac{\partial\mathrm{vec}(\vec{F})}{\partial \mathrm{vec}(\vec{X})}</script>，求导时矩阵被向量化，弊端是这在一定程度破坏了矩阵的结构，会导致结果变得形式复杂；好处是多元微积分中关于梯度、Hessian 矩阵的结论可以沿用过来，只需将矩阵向量化。例如优化问题中，牛顿法的更新 <script type="math/tex">\Delta \vec{X}</script>，满足 <script type="math/tex">\mathrm{vec}(\Delta \vec{X}) = -(\nabla^2_{\vec{X}} f)^{-1}\mathrm{vec}(\nabla_{\vec{X}} f)</script>；</li>
  <li>在资料中，矩阵对矩阵的导数还有其它定义，比如 <script type="math/tex">\frac{\partial \vec{F}}{\partial \vec{X}} = \left[\frac{\partial \vec{F}_{kl}}{\partial \vec{X}}\right](mp\times nq)</script>，它能兼容上篇中的标量对矩阵导数的定义，但微分与导数的联系（<script type="math/tex">\mathrm{d}\vec{F}</script> 等于 <script type="math/tex">\frac{\partial \vec{F}}{\partial \vec{X}}</script> 中每个 <script type="math/tex">m\times n</script> 子块分别与 <script type="math/tex">\mathrm{d}\vec{X}</script> 做内积）不够简明，不便于计算和应用。</li>
</ul>

<p>然后来建立运算法则。仍然要利用导数与微分的联系 <script type="math/tex">\mathrm{vec}(\mathrm{d}\vec{F}) = \frac{\partial \vec{F}}{\partial \vec{X}}^T \mathrm{vec}(\mathrm{d}\vec{X})</script>，求微分的方法与上篇相同，而从微分得到导数需要一些向量化的技巧：</p>

<ul>
  <li>线性：<script type="math/tex">\mathrm{vec}(\vec{A}+\vec{B}) = \mathrm{vec}(\vec{A}) + \mathrm{vec}(\vec{B})</script>；</li>
  <li>矩阵乘法：<script type="math/tex">\mathrm{vec}(\vec{AXB}) = (\vec{B}^T \otimes \vec{A}) \mathrm{vec}(\vec{X})</script>，其中 <script type="math/tex">\otimes</script> 表示 Kronecker 积，<script type="math/tex">\vec{A}(m\times n)</script> 与 <script type="math/tex">\vec{B}(p\times q)</script> 的 Kronecker 积是 <script type="math/tex">\vec{A}\otimes \vec{B} = [\vec{A}_{ij}\vec{B}](mp\times nq)</script>。此式证明见张贤达《矩阵分析与应用》第 107-108 页；</li>
  <li>转置：<script type="math/tex">\mathrm{vec}(\vec{A}^T) = \vec{K}_{mn}\mathrm{vec}(\vec{A})</script>，<script type="math/tex">\vec{A}</script> 是 <script type="math/tex">m\times n</script> 矩阵，其中 <script type="math/tex">\vec{K}_{mn}(mn\times mn)</script> 是交换矩阵(commutation matrix)；</li>
  <li>逐元素乘法：<script type="math/tex">\mathrm{vec}(\vec{A}\odot \vec{X}) = \mathrm{diag}(\vec{A})\mathrm{vec}(\vec{X})</script>，其中 <script type="math/tex">\mathrm{diag}(\vec{A})(mn\times mn)</script> 是用 <script type="math/tex">A</script> 的元素（按列优先）排成的对角阵。</li>
</ul>

<p>观察一下可以断言，若矩阵函数 <script type="math/tex">\vec{F}</script> 是矩阵 <script type="math/tex">\vec{X}</script> 经加减乘法、行列式、逆、逐元素函数等运算构成，则使用相应的运算法则对 <script type="math/tex">\vec{F}</script> 求微分，再做向量化并使用技巧将其它项交换至 <script type="math/tex">\mathrm{vec}(\mathrm{d}\vec{X})</script>  左侧，即能得到导数。</p>

<p>再谈一谈复合：假设已求得 <script type="math/tex">\frac{\partial \vec{F}}{\partial \vec{Y}}</script>，而 <script type="math/tex">\vec{Y}</script> 是 <script type="math/tex">\vec{X}</script> 的函数，如何求 <script type="math/tex">\frac{\partial \vec{F}}{\partial \vec{X}}</script> 呢？从导数与微分的联系入手，<script type="math/tex">\mathrm{vec}(\mathrm{d}\vec{F}) = \frac{\partial \vec{F}}{\partial Y}^T\mathrm{vec}(\mathrm{d}\vec{Y}) = \frac{\partial \vec{F}}{\partial \vec{Y}}^T\frac{\partial \vec{Y}}{\partial \vec{X}}^T\mathrm{vec}(\mathrm{d}\vec{X})</script>，可以推出链式法则 <script type="math/tex">\frac{\partial \vec{F}}{\partial \vec{X}} = \frac{\partial \vec{Y}}{\partial \vec{X}}\frac{\partial \vec{F}}{\partial \vec{Y}}</script>。</p>

<p>和标量对矩阵的导数相比，矩阵对矩阵的导数形式更加复杂，从不同角度出发常会得到形式不同的结果。有一些 Kronecker 积和交换矩阵相关的恒等式，可用来做等价变形：</p>

<ul>
  <li><script type="math/tex">(\vec{A}\otimes \vec{B})^T = \vec{A}^T \otimes \vec{B}^T</script>；</li>
  <li><script type="math/tex">\mathrm{vec}(\vec{ab}^T) = \vec{b}\otimes\vec{a}</script>；</li>
  <li><script type="math/tex">(\vec{A}\otimes \vec{B})(\vec{C}\otimes \vec{D}) = (\vec{AC})\otimes (\vec{BD})</script>。可以对 <script type="math/tex">\vec{F} = \vec{D}^T\vec{B}^T\vec{XAC}</script> 求导来证明，一方面，直接求导得到 <script type="math/tex">\frac{\partial \vec{F}}{\partial \vec{X}} = (\vec{AC}) \otimes (\vec{BD})</script>；另一方面，引入 <script type="math/tex">\vec{Y} = \vec{B}^T\vec{XA}</script>，有 <script type="math/tex">\frac{\partial \vec{F}}{\partial \vec{Y}} = \vec{C} \otimes \vec{D}</script>，<script type="math/tex">\frac{\partial \vec{Y}}{\partial \vec{X}} = \vec{A} \otimes \vec{B}</script>，用链式法则得到 <script type="math/tex">\frac{\partial \vec{F}}{\partial \vec{X}} = (\vec{A}\otimes \vec{B})(\vec{C} \otimes \vec{D})</script>；</li>
  <li><script type="math/tex">\vec{K}_{mn} = \vec{K}_{nm}^T, \vec{K}_{mn}\vec{K}_{nm} = \vec{I}</script>；</li>
  <li><script type="math/tex">\vec{K}_{pm}(\vec{A}\otimes \vec{B}) \vec{K}_{nq} = \vec{B}\otimes \vec{A}</script>，<script type="math/tex">\vec{A}</script> 是 <script type="math/tex">m\times n</script> 矩阵，<script type="math/tex">\vec{B}</script> 是 <script type="math/tex">p\times q</script> 矩阵。可以对 <script type="math/tex">\vec{AXB}^T</script> 做向量化来证明，一方面，<script type="math/tex">\mathrm{vec}(\vec{AXB}^T) = (\vec{B}\otimes \vec{A})\mathrm{vec}(\vec{X})</script>；另一方面，<script type="math/tex">\mathrm{vec}(\vec{AXB}^T) = \vec{K}_{pm}\mathrm{vec}(\vec{BX}^T\vec{A}^T) = \vec{K}_{pm}(\vec{A}\otimes \vec{B})\mathrm{vec}(\vec{X}^T) = \vec{K}_{pm}(\vec{A}\otimes \vec{B}) \vec{K}_{nq}\mathrm{vec}(\vec{X})</script>。</li>
</ul>

<p>接下来演示一些算例。</p>

<p><strong>例1</strong>：<script type="math/tex">\vec{F} = \vec{AX}</script>，<script type="math/tex">\vec{X}</script> 是 <script type="math/tex">m\times n</script> 矩阵，求 <script type="math/tex">\frac{\partial \vec{F}}{\partial \vec{X}}</script>。</p>

<p>解：先求微分：<script type="math/tex">\mathrm{d}\vec{F}=\vec{A}\mathrm{d}\vec{X}</script>，再做向量化，使用矩阵乘法的技巧，注意在 <script type="math/tex">\mathrm{d}\vec{X}</script> 右侧添加单位阵：<script type="math/tex">\mathrm{vec}(\mathrm{d}\vec{F}) = \mathrm{vec}(\vec{A}\mathrm{d}\vec{X}) = (\vec{I}_n\otimes \vec{A})\mathrm{vec}(\mathrm{d}\vec{X})</script>，对照导数与微分的联系得到 <script type="math/tex">\frac{\partial \vec{F}}{\partial \vec{X}} = \vec{I}_n\otimes \vec{A}^T</script>。</p>

<p>特例：如果 <script type="math/tex">\vec{X}</script> 退化为向量，<script type="math/tex">\vec{f} = \vec{A} \vec{x}</script>，则根据向量的导数与微分的关系 <script type="math/tex">\mathrm{d}\vec{f} = \frac{\partial \vec{f}}{\partial \vec{x}}^T \mathrm{d}\vec{x}</script>，得到  <script type="math/tex">\frac{\partial \vec{f}}{\partial \vec{x}} = \vec{A}^T</script>。</p>

<p><strong>例2</strong>：<script type="math/tex">\vec{f} = \log\lvert\vec{X}\rvert</script>，<script type="math/tex">\vec{X}</script> 是 <script type="math/tex">n\times n</script> 矩阵，求 <script type="math/tex">\nabla_{\vec{X}} \vec{f}</script> 和 <script type="math/tex">\nabla^2_{\vec{X}} \vec{f}</script>。</p>

<p>解：使用第一部分中的技术可求得 <script type="math/tex">\nabla_{\vec{X}} \vec{f} = \vec{X}^{-1T}</script>。为求 <script type="math/tex">\nabla^2_{\vec{X}} \vec{f}</script>，先求微分：<script type="math/tex">\mathrm{d}\nabla_{\vec{X}} \vec{f} = -(\vec{X}^{-1}\mathrm{d}\vec{XX}^{-1})^T</script>，再做向量化，使用转置和矩阵乘法的技巧
 <script type="math/tex">\mathrm{vec}(\mathrm{d}\nabla_{\vec{X}} \vec{f})= -\vec{K}_{nn}\mathrm{vec}(\vec{X}^{-1}\mathrm{d}\vec{XX}^{-1}) = -\vec{K}_{nn}(\vec{X}^{-1T}\otimes \vec{X}^{-1})\mathrm{vec}(\mathrm{d}\vec{X})</script>，对照导数与微分的联系，得到 <script type="math/tex">\nabla^2_{\vec{X}} \vec{f} = -\vec{K}_{nn}(\vec{X}^{-1T}\otimes \vec{X}^{-1})</script>，注意它是对称矩阵。在 <script type="math/tex">\vec{X}</script> 是对称矩阵时，可简化为 <script type="math/tex">\nabla^2_{\vec{X}} \vec{f} = -\vec{X}^{-1}\otimes \vec{X}^{-1}</script>。</p>

<p><strong>例3</strong>：<script type="math/tex">\vec{F} = \vec{A}\exp(\vec{XB})</script>，<script type="math/tex">\vec{A}</script> 是 <script type="math/tex">l\times m</script>，<script type="math/tex">\vec{X}</script> 是 <script type="math/tex">m\times n</script>，<script type="math/tex">\vec{B}</script> 是 <script type="math/tex">n\times p</script> 矩阵，<script type="math/tex">\exp()</script> 为逐元素函数，求 <script type="math/tex">\frac{\partial \vec{F}}{\partial \vec{X}}</script>。</p>

<p>解：先求微分：<script type="math/tex">\mathrm{d}\vec{F} = \vec{A}(\exp(\vec{XB})\odot (\mathrm{d}\vec{XB}))</script>，再做向量化，使用矩阵乘法的技巧：<script type="math/tex">\mathrm{vec}(\mathrm{d}\vec{F}) = (\vec{I}_p\otimes \vec{A})\mathrm{vec}(\exp(\vec{XB})\odot (\mathrm{d}\vec{XB}))</script>，再用逐元素乘法的技巧：<script type="math/tex">\mathrm{vec}(\mathrm{d}\vec{F}) = (\vec{I}_p \otimes \vec{A}) \mathrm{diag}(\exp(\vec{XB}))\mathrm{vec}(\mathrm{d}\vec{XB})</script>，再用矩阵乘法的技巧：<script type="math/tex">\mathrm{vec}(\mathrm{d}\vec{F}) = (\vec{I}_p\otimes \vec{A})\mathrm{diag}(\exp(\vec{XB}))(\vec{B}^T\otimes I_m)\mathrm{vec}(\mathrm{d}\vec{X})</script>，对照导数与微分的联系得到 <script type="math/tex">\frac{\partial \vec{F}}{\partial \vec{X}} = (\vec{B}\otimes \vec{I}_m)\mathrm{diag}(\exp(\vec{XB}))(\vec{I}_p\otimes \vec{A}^T)</script>。</p>

<p><strong>例4【一元 logistic 回归】</strong>：<script type="math/tex">l = -y \vec{x}^T \vec{w} + \log(1 + \exp(\vec{x}^T\vec{w}))</script>，求 <script type="math/tex">\nabla_{\vec{w}} l</script> 和 <script type="math/tex">\nabla_{\vec{w}}^2 l</script>。其中 <script type="math/tex">y</script> 是取值 0 或 1 的标量，<script type="math/tex">\vec{x}</script> 和 <script type="math/tex">\vec{w}</script> 是向量。</p>

<p>解：使用上篇中的技术可求得 <script type="math/tex">\nabla_{\vec{w}} l = \vec{x}(\sigma(\vec{x}^T\vec{w}) - y)</script>，其中 <script type="math/tex">\sigma(a) = \frac{\exp(a)}{1+\exp(a)}</script> 为 sigmoid 函数。为求 <script type="math/tex">\nabla_{\vec{w}}^2 l</script>，先求微分： <script type="math/tex">\mathrm{d}\nabla_{\vec{w}} l = \vec{x} \sigma'(\vec{x}^T\vec{w})\vec{x}^T \mathrm{d}\vec{w}</script>，其中 <script type="math/tex">\sigma'(a) = \frac{\exp(a)}{(1+\exp(a))^2}</script> 为 sigmoid 函数的导数，对照导数与微分的联系，得到 <script type="math/tex">\nabla_{\vec{w}}^2 l = \vec{x}\sigma'(\vec{x}^T\vec{w})\vec{x}^T</script>。</p>

<p><strong>推广</strong>：样本( <script type="math/tex">\vec{x}_1, y_1), \dots, (\vec{x}_n,y_n)</script>，<script type="math/tex">l = \sum_{i=1}^N \left(-y_i \vec{x}_i^T\vec{w} + \log(1+\exp(\vec{x_i}^T\vec{w}))\right)</script>，求 <script type="math/tex">\nabla_{\vec{w}} l</script> 和 <script type="math/tex">\nabla_{\vec{w}}^2 l</script>。 有两种方法，方法一：先对每个样本求导，然后相加；方法二：定义矩阵 <script type="math/tex">\vec{X} = \begin{bmatrix}\vec{x}_1^T \\ \vdots \\ \vec{x}_n^T \end{bmatrix}</script>，向量 <script type="math/tex">\vec{y} = \begin{bmatrix}y_1 \\ \vdots \\ y_n\end{bmatrix}</script>，将 <script type="math/tex">l</script> 写成矩阵形式 <script type="math/tex">$l = -\vec{y}^T \vec{X}\vec{w} + \vec{1}^T\log(\vec{1} + \exp(\vec{X}\vec{w}))</script>，进而可以求得
 <script type="math/tex">\nabla_{\vec{w}} l = \vec{X}^T(\sigma(\vec{X}\vec{w}) - \vec{y})</script>，<script type="math/tex">\nabla_{\vec{w}}^2 l = \vec{X}^T\text{diag}(\sigma'(\vec{X}\vec{w}))\vec{X}</script>。</p>

<p><strong>例5【多元 logistic 回归】</strong>：<script type="math/tex">l = -\vec{y}^T\log \mathrm{softmax}(\vec{Wx})=</script><script type="math/tex">-\vec{y}^T\vec{Wx}+</script>\log(\vec{1}^T\exp(\vec{Wx}))<script type="math/tex">，求</script>\nabla_{\vec{W}} l<script type="math/tex">和</script>\nabla_{\vec{W}}^2 l$$。</p>

<p><strong>解</strong>：上篇例 3 中已求得 <script type="math/tex">\nabla_{\vec{W}} l = (\mathrm{softmax}(\vec{Wx})-\vec{y})\vec{x}^T</script>。为求 <script type="math/tex">\nabla_{\vec{W}}^2 l</script>，先求微分：定义 <script type="math/tex">\vec{a} = \vec{Wx}，\mathrm{d}\mathrm{softmax}(\vec{a}) = \frac{\exp(\vec{a})\odot \mathrm{d}\vec{a}}{\vec{1}^T\exp(\vec{a})} - \frac{\exp(\vec{a}) (\vec{1}^T(\exp(\vec{a})\odot \mathrm{d}\vec{a}))}{(\vec{1}^T\exp(\vec{a}))^2}</script>，这里需要化简去掉逐元素乘法，第一项中 <script type="math/tex">\exp(\vec{a})\odot \mathrm{d}\vec{a} = \mathrm{diag}(\exp(\vec{a})) \mathrm{d}\vec{a}</script>，第二项中 <script type="math/tex">\vec{1}^T(\exp(\vec{a})\odot \mathrm{d}\vec{a}) = \exp(\vec{a})^T\mathrm{d}\vec{a}</script>，故有 <script type="math/tex">\mathrm{d}\mathrm{softmax}(\vec{a}) = \mathrm{softmax}'(\vec{a})\mathrm{d}\vec{a}</script>，其中
 <script type="math/tex">\mathrm{softmax}'(\vec{a}) = \frac{\mathrm{diag}(\exp(\vec{a}))}{\vec{1}^T\exp(\vec{a})} - \frac{\exp(\vec{a})\exp(\vec{a})^T}{(\vec{1}^T\exp(\vec{a}))^2}</script>，代入有 <script type="math/tex">\mathrm{d}\nabla_{\vec{W}} l = \mathrm{softmax}'(\vec{a})\mathrm{d}\vec{a}\vec{x}^T = \mathrm{softmax}'(\vec{Wx})\mathrm{d}\vec{Wx}\vec{x}^T</script>，做向量化并使用矩阵乘法的技巧，得到 <script type="math/tex">\nabla_{\vec{W}}^2 l = (\vec{x}\vec{x}^T) \otimes \mathrm{softmax}'(\vec{Wx})</script>。</p>

<p>最后做个总结。我们发展了从整体出发的矩阵求导的技术，导数与微分的联系是计算的枢纽，标量对矩阵的导数与微分的联系是 <script type="math/tex">\mathrm{d}f = \mathrm{tr}(\nabla_{\vec{X}}^T f \mathrm{d}\vec{X})</script>，先对 <script type="math/tex">f</script> 求微分，再使用迹技巧可求得导数，特别地，标量对向量的导数与微分的联系是 <script type="math/tex">\mathrm{d}f = \nabla_{\vec{x}}f^T \mathrm{d}\vec{x}</script>；矩阵对矩阵的导数与微分的联系是 <script type="math/tex">\mathrm{vec}(\mathrm{d}\vec{F}) = \frac{\partial \vec{F}}{\partial \vec{X}}^T \mathrm{vec}(\mathrm{d}\vec{X})</script>，先对 <script type="math/tex">\vec{F}</script>  求微分，再使用向量化的技巧可求得导数，特别地，向量对向量的导数与微分的联系是 <script type="math/tex">\mathrm{d}\vec{f} = \frac{\partial \vec{f}}{\partial \vec{x}}^T\mathrm{d}\vec{x}</script>。</p>

<h2 id="section-2">3 反向传播算法的完整向量形式推导</h2>

<p>首先定义一些常用的变量：</p>

<ul>
  <li><script type="math/tex">n^l</script>：第 <script type="math/tex">l</script> 层的神经元个数；</li>
  <li><script type="math/tex">f_l(\cdot)</script>：第 <script type="math/tex">l</script> 层的激活函数；</li>
  <li><script type="math/tex">\vec{W}^l\in \mathbb{R}^{n^l\times n^{l-1}}</script>：第 <script type="math/tex">l-1</script> 层到第 <script type="math/tex">l</script> 层的权重矩阵；</li>
  <li><script type="math/tex">\vec{b}^l\in \mathbb{R}^{n^l}</script>：第 <script type="math/tex">l-1</script> 层到第 <script type="math/tex">l</script> 层的偏置；</li>
  <li><script type="math/tex">\vec{z}^l\in \mathbb{R}^{n^l}</script>：第 <script type="math/tex">l</script> 层的输入；</li>
  <li><script type="math/tex">\vec{a}^l\in \mathbb{R}^{n^l}</script>：第 <script type="math/tex">l</script> 层的输出。</li>
</ul>

<p>在每一层的正向传播过程计算过程如下所示：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}

    \vec{z}^l &= \vec{W}^l\vec{a}^{(l-1)} + \vec{b}^l \\
    \vec{a}^l &= f_l(\vec{z}^l)
\end{align} %]]></script>

<p>为进行梯度计算，我们需要先写出目标函数：</p>

<script type="math/tex; mode=display">\begin{equation}
J(\vec{W},\vec{b})=\sum_{i=1}^{N}J(\vec{W},\vec{b};\vec{x}^i,y^i)+\frac{1}{2}\lambda||\vec{W}||_F^2
\end{equation}</script>

<p>其中</p>

<script type="math/tex; mode=display">||\vec{W}||_F^2=\sum_{l=1}^L\sum_{j=1}^{n^{l+1}}\sum_{i=1}^{n^l}W_{ij}^l</script>

<p>那么对 <script type="math/tex">\vec{W}</script> 和 <script type="math/tex">\vec{b}</script> 的更新公式理论上可以写成：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
    \vec{W}^l &= \vec{W}^l-\alpha\frac{\partial J(\vec{W},\vec{b})}{\partial \vec{W}^l} \\
              &= \vec{W}^l-\alpha\bigg( \sum_{i=1}^N \frac{\partial J(\vec{W},\vec{b};\vec{x}^i,y^i)}{\partial \vec{W}^l} + \lambda\vec{W}^l \bigg) \label{eq1:dw}\\
    \vec{b}^l &= \vec{b}^l-\alpha\frac{\partial J(\vec{W},\vec{b})}{\partial \vec{b}^l} \\
              &= \vec{b}^l-\alpha \sum_{i=1}^N \frac{\partial J(\vec{W},\vec{b};\vec{x}^i,y^i)}{\partial \vec{b}^l} \label{eq1:db}
\end{align} %]]></script>

<p>我们考虑每一个单独样本的梯度，先假设 <script type="math/tex">\frac{\partial J(\vec{W},\vec{b};\vec{x},y)}{\partial \vec{z}^l}=\vec{\delta}^l</script>，根据第一部分矩阵导数与微分的联系，有 <script type="math/tex">\mathrm{d}J=\mathrm{tr}\bigg(\vec{\delta}^{lT}\mathrm{d}\vec{z}^l\bigg)</script>，而 <script type="math/tex">\vec{z}^l = \vec{W}^l\vec{a}^{(l-1)} + \vec{b}^l</script>，所以 <script type="math/tex">\mathrm{d}\vec{z}^l = \mathrm{d}\vec{W}^l\vec{a}^{(l-1)}</script>，因此，</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
    \mathrm{d}J &= \mathrm{tr}\bigg( \vec{\delta}^{lT}\mathrm{d}\vec{W}^l\vec{a}^{(l-1)} \bigg)\\
                &= \mathrm{tr}\bigg( \vec{a}^{(l-1)}\vec{\delta}^{lT}\mathrm{d}\vec{W}^l \bigg)
\end{align} %]]></script>

<p>所以最后有</p>

<script type="math/tex; mode=display">\begin{equation}
    \frac{\partial J(\vec{W},\vec{b};\vec{x},y)}{\partial \vec{W}^l}=\vec{\delta}^l\vec{a}^{(l-1)T}. \label{eq2:dw}
\end{equation}</script>

<p>同理有</p>

<script type="math/tex; mode=display">\begin{equation}
    \frac{\partial J(\vec{W},\vec{b};\vec{x},y)}{\partial \vec{b}^l}=\vec{\delta}^l. \label{eq2:db}
\end{equation}</script>

<p>下面求 <script type="math/tex">\vec{\delta}^l</script>，假设已知 <script type="math/tex">\frac{\partial J(\vec{W},\vec{b};\vec{x},y)}{\partial \vec{z}^{(l+1)}}=\vec{\delta}^{(l+1)}</script>，则有 <script type="math/tex">\mathrm{d}J=\mathrm{tr}\bigg(\vec{\delta}^{(l+1)T}\mathrm{d}\vec{z}^{(l+1)}\bigg)</script>，而 <script type="math/tex">\vec{z}^{(l+1)} = \vec{W}^{(l+1)}\vec{a}^l + \vec{b}^{(l+1)}</script>，所以 <script type="math/tex">\mathrm{d}\vec{z}^{(l+1)} = \vec{W}^{(l+1)} \mathrm{d}\vec{a}^l</script>，又因为 <script type="math/tex">\vec{a}^l=f_l(\vec{z}^l)</script> 是逐元素的，所以 <script type="math/tex">\mathrm{d}\vec{a}^l=\mathrm{diag}(f'_l(\vec{z}^l))\mathrm{d}\vec{z}^l</script>，代入后有</p>

<script type="math/tex; mode=display">\begin{equation}
    \mathrm{d}J=\mathrm{tr}\bigg( \vec{\delta}^{(l+1)T}\vec{W}^{(l+1)}\mathrm{diag}(f'_l(\vec{z}^l))\mathrm{d}\vec{z}^l \bigg).
\end{equation}</script>

<p>所以有</p>

<script type="math/tex; mode=display">\begin{equation}
    \vec{\delta}^l=\frac{\partial J(\vec{W},\vec{b};\vec{x},y)}{\partial \vec{z}^l}=\mathrm{diag}(f'_l(\vec{z}^l))\vec{W}^{(l+1)T}\vec{\delta}^{(l+1)}. \label{eq2:delta}
\end{equation}</script>

<p>其中当 <script type="math/tex">f(\vec{x})=\mathrm{sigmoid}(\vec{x})</script> 时，有 <script type="math/tex">f'(\vec{x})=f(\vec{x})(1-f(\vec{x}))</script>，当 <script type="math/tex">f(\vec{x})=\mathrm{tanh}(\vec{x})</script> 时，有 <script type="math/tex">f'(\vec{x})=1-f(\vec{x})^2</script>。</p>

<p>最后考虑最后一层的 <script type="math/tex">\vec{\delta}^{(l+1)}</script>。当使用交叉熵损失函数时（以下 <script type="math/tex">\vec{a}</script> 和 <script type="math/tex">\vec{z}</script> 都略去上标 <script type="math/tex">(l+1)</script>）</p>

<script type="math/tex; mode=display">\begin{equation}
    J(\vec{W},\vec{b};\vec{x},y)=-\vec{y}^T\ln\vec{a}-(\vec{1}-\vec{y})^T\ln(1-\vec{a}).
\end{equation}</script>

<p>这时</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
    \mathrm{d}J &= -\vec{y}^T\bigg(\frac{1}{\vec{a}} \odot \mathrm{d}\vec{a}\bigg)-(\vec{1}-\vec{y})^T\bigg(\frac{-1}{\vec{1}-\vec{a}}\odot \mathrm{d}\vec{a}\bigg) \\
                &= -\bigg(\vec{y} \odot \frac{1}{\vec{a}} \bigg)^T \mathrm{d}\vec{a} + \bigg( (\vec{1}-\vec{y})\odot\frac{1}{\vec{1}-\vec{a}}\bigg)^T \mathrm{d}\vec{a} \\
                &= \bigg(-\vec{y} \odot \frac{1}{\vec{a}} + (\vec{1}-\vec{y})\odot\frac{1}{\vec{1}-\vec{a}}\bigg)^T \mathrm{d}\vec{a}
\end{align} %]]></script>

<p>又因为 <script type="math/tex">\mathrm{d}\vec{a}=\mathrm{diag}(\mathrm{sigmoid}'(\vec{z}))\mathrm{d}\vec{z}=\mathrm{diag}(\vec{a(1-a)})\mathrm{d}\vec{z}</script>，代入上式有</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
    \mathrm{d}J &= \bigg(-\vec{y} \odot \frac{1}{\vec{a}} + (\vec{1-y})\odot\frac{1}{\vec{1-a}}\bigg)^T \mathrm{diag}(\vec{a(1-a)})\mathrm{d}\vec{z} \\
                &= \bigg(-\mathrm{diag}(\vec{a(1-a)}) \vec{y} \odot \frac{1}{\vec{a}} + \mathrm{diag}(\vec{a(1-a)})(\vec{1-y})\odot\frac{1}{\vec{1-a}}\bigg)^T \mathrm{d}\vec{z} \\
                &= (-\vec{y}+\vec{y}\odot\vec{a} + \vec{a} - \vec{a}\odot\vec{y})^T \mathrm{d}\vec{z} \\
                &= (-\vec{y}+\vec{a})^T \mathrm{d}\vec{z} 
\end{align} %]]></script>

<p>所以有</p>

<script type="math/tex; mode=display">\begin{equation}
    \vec{\delta}^{(l+1)}=\frac{\partial J(\vec{W},\vec{b};\vec{x},y)}{\partial \vec{z}^{(l+1)}}=-\vec{y}+\vec{a}^{(l+1)}. \label{eq:delta}
\end{equation}</script>

<p>最后，将式 \ref{eq:delta} 和式 \ref{eq2:db} 代入式 \ref{eq1:db} 则得到偏置项 <script type="math/tex">\vec{b}</script> 的更新公式，将式 \ref{eq:delta}，式 \ref{eq2:delta} 和式 \ref{eq2:dw} 代入式 \ref{eq1:dw} 则得到 <script type="math/tex">\vec{W}</script> 的更新公式。</p>

        </article>
        <hr>

        
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
        
        

        <div class="post-recent">
    <div class="pre">
        
        <p><strong>上一篇</strong> <a href="/2016/03/20/GitHub-Pages%E4%B8%8A%E8%BE%93%E5%87%BA%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/">GitHub Pages上输出数学公式</a></p>
        
    </div>
    <div class="nex">

        
        <p><strong>下一篇</strong> <a href="/2018/05/08/%E6%84%8F%E8%AF%86%E8%AF%9E%E7%94%9F%E4%BA%8E%E4%B8%8A%E5%B8%9D%E6%B2%89%E9%BB%98%E6%97%B6/">意识诞生于上帝沉默时</a></p>
        
    </div>
</div>


        <h2 id="comments">Comments</h2>
        <div id="container"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: document.title, // 可选。默认为 location.href, '<%= page.title %>'
  owner: 'caoxiaoqing',
  repo: 'CaoXiaoqing.github.io',
  oauth: {
    client_id: 'f6c98a7a3fd7c7cf3b6b',
    client_secret: '3ffd1d22c828ecafb8ae282bffa8fe1b3af0ada8',
  },
})
gitment.render('container')
</script>







    </div>
    <button class="anchor"><i class="fa fa-anchor"></i></button>
    <div class="right">
        <div class="wrap">

            <!-- Content -->
            <div class="side content">
                <div>
                    Content
                </div>
                <ul id="content-side" class="content-ul">
                    
                    <li><a href="#comments">Comments</a></li>
                </ul>
            </div>
            <!-- 其他div框放到这里 -->
            <!-- <div class="side">bbbb</div> -->
        </div>
    </div>
</div>
<script>
/**
 * target _blank
 */
(function() {
    var aTags = document.querySelectorAll('article a:not([id])')
    for (var i = 0; i < aTags.length; i++) {
        aTags[i].setAttribute('target', '_blank')
    }
}());
</script>
<script src="/js/pageContent.js " charset="utf-8"></script>


    <footer class="site-footer">


    <div class="wrapper">

        <p class="description">
             曹孝卿的博客 
        </p>
        <p class="contact">
            Contact me at: 
            <a href="https://github.com/caoxiaoqing" title="GitHub"><i class="fa fa-github" aria-hidden="true"></i></a>  
            <a href="mailto:CaoXiaoqing2008@gmail.com" title="email"><i class="fa fa-envelope-o" aria-hidden="true"></i></a>        
        </p>
        <p>
            本站总访问量<span id="busuanzi_value_site_pv"></span>次，本站访客数<span id="busuanzi_value_site_uv"></span>人次，本文总阅读量<span id="busuanzi_value_page_pv"></span>次
        </p>
        <p class="power">
            <span>
                Site powered by <a href="https://jekyllrb.com/">Jekyll</a> & <a href="https://pages.github.com/">Github Pages</a>.
            </span>
            <span>
                Theme designed by <a href="https://github.com/Gaohaoyang">HyG</a>.
            </span>
        </p>
    </div>
</footer>
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <div class="back-to-top">
    <a href="#top" data-scroll>
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
    </a>
</div>

    <script src=" /js/main.js " charset="utf-8"></script>
    <script src=" /js/smooth-scroll.min.js " charset="utf-8"></script>
    <script type="text/javascript">
      smoothScroll.init({
        speed: 500, // Integer. How fast to complete the scroll in milliseconds
        easing: 'easeInOutCubic', // Easing pattern to use
        offset: 20, // Integer. How far to offset the scrolling anchor location in pixels
      });
    </script>
    <!-- <script src=" /js/scroll.min.js " charset="utf-8"></script> -->
  </body>

</html>
