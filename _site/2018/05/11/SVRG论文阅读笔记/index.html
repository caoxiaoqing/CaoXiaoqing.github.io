<!DOCTYPE html>
<html>

  <head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>SVRG算法的阅读理解和实践</title>
	<meta name="description" content="SVRG算法的阅读理解和实践">
	
	<link rel="canonical" href="/2018/05/11/SVRG%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">
	<link rel="alternate" type="application/rss+xml" title="CaoXiaoqing's Blog" href="/feed.xml" />
	
	<!-- <link rel="stylesheet" href="/css/main.css"> -->

	<link rel="stylesheet" type="text/css" href="http://apps.bdimg.com/libs/bootstrap/3.3.0/css/bootstrap.min.css">
	<!-- <link rel="stylesheet" type="text/css" href="http://apps.bdimg.com/libs/fontawesome/4.2.0/css/font-awesome.min.css"> -->
	<!-- <link rel="stylesheet" type="text/css" href="/static/css/bootstrap.min.css"> -->
	

	<link rel="stylesheet" type="text/css" href="/static/css/index.css">
	
	<!-- <script type="text/javascript" src="/static/js/jquery-1.11.1.min.js"></script>
	<script type="text/javascript" src="/static/js/bootstrap.min.js"></script> -->

	<script type="text/javascript" src="http://apps.bdimg.com/libs/jquery/2.1.1/jquery.min.js"></script>
	<script type="text/javascript" src="http://apps.bdimg.com/libs/bootstrap/3.3.0/js/bootstrap.min.js"></script>

	<script type="text/javascript" src="/static/js/index.js"></script>
	
	<link rel="stylesheet" type="text/css" href="http://apps.bdimg.com/libs/highlight.js/8.4/styles/monokai_sublime.min.css">
	<!-- <link rel="stylesheet" type="text/css" href="http://apps.bdimg.com/libs/highlight.js/8.4/styles/railscasts.min.css"> -->
	<!-- <link rel="stylesheet" type="text/css" href="http://apps.bdimg.com/libs/highlight.js/8.4/styles/monokai.min.css"> -->
	<!-- <script type="text/javascript" src="http://apps.bdimg.com/libs/highlight.js/8.4/languages/dos.min.js"></script> -->
	<script type="text/javascript" src="http://apps.bdimg.com/libs/highlight.js/8.4/highlight.min.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
	
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

	<!--script>
	var _hmt = _hmt || [];
	(function() {
	  var hm = document.createElement("script");
	  hm.src = "//hm.baidu.com/hm.js?b636473d6ffa17615f94e5db1459ea81";
	  var s = document.getElementsByTagName("script")[0]; 
	  s.parentNode.insertBefore(hm, s);
	})();
	</script-->

</head>

 <!--  <body data-spy="scroll" data-target="#myAffix"> -->
  <body>

    <header>

<!-- navbar -->
  <nav class="navbar navbar-inverse">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">CaoXiaoqing's Blog</a>
      <p class="navbar-text">Free Your Mind</p>
    </div>
    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav navbar-right">

        
          <li>
        
          <a href="/"><span class="glyphicon glyphicon-th-large"></span> Home</a></li>

        
          
        
          
        
      </ul>
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

</header>


    <div id="main" class="container main">
      <div class="row">
  <div id="myArticle" class="col-sm-9">
    <div class="post-area post">
      <header>
        <h1>SVRG算法的阅读理解和实践</h1>
        <p>May 11, 2018 • CaoXiaoqing</p>
      </header>
      <hr>
      <article>
        <p>最佳拜读了下大名鼎鼎的 SVRG 算法 <a href="#Reference">[5]</a>，读完后把前前后后涉及到的方法都看了一遍，这里做个简单的综述和阅读理解，并描述了如何将方差缩减方法改造成一种在线算法。</p>

<ul id="markdown-toc">
  <li><a href="#section" id="markdown-toc-section">1. 背景介绍</a></li>
  <li><a href="#noise-reduction-methods" id="markdown-toc-noise-reduction-methods">2. 方差缩减方法 Noise Reduction Methods</a></li>
  <li><a href="#sag--10reference" id="markdown-toc-sag--10reference">3. SAG 算法 <a href="#Reference">[10]</a></a></li>
  <li><a href="#svrg--5reference" id="markdown-toc-svrg--5reference">4. SVRG 算法 <a href="#Reference">[5]</a></a></li>
  <li><a href="#saga--6reference" id="markdown-toc-saga--6reference">5. SAGA 算法 <a href="#Reference">[6]</a></a></li>
  <li><a href="#scsg--8reference" id="markdown-toc-scsg--8reference">6. SCSG 算法 <a href="#Reference">[8]</a></a>    <ul>
      <li><a href="#section-1" id="markdown-toc-section-1">7. 其它方法</a></li>
    </ul>
  </li>
  <li><a href="#section-2" id="markdown-toc-section-2">8. 应用于在线学习</a></li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ul>

<h2 id="section">1. 背景介绍</h2>

<p>考虑优化问题：</p>

<script type="math/tex; mode=display">min \, R_n(w)=\frac{1}{n}\sum_{i=1}^{n}f_i(w)</script>

<p>当我们采用 Gradient Descent (GD) 方法时，<script type="math/tex">w</script> 的更新公式是：</p>

<script type="math/tex; mode=display">w_{k+1}=w_{k}-\eta_k\nabla R_n(w_{k})=w_{k}-\frac{\eta_k}{n}\sum_{i=1}^{n}\nabla f_{i}(w_{k})</script>

<p>梯度下降方法可以追溯到 Cauchy 1847 年的论文 <a href="#Reference">[1]</a>。梯度下降对于样本数目比较多的时候有一个很大的劣势，那就是每次需要求解所有样本的梯度，导致计算量大增，所以实际生产环境中，往往采用随机梯度下降算法（Stochastic Gradient Descent），一般简写做 SGD，它于 1951 和 1952 年在文献 <a href="#Reference">[2,3]</a> 中被提出。SGD 每次迭代的时候均匀随机得选择一个样本或者 mini-batch 做更新。当我们采用 SGD 方法来进行计算的时候，<script type="math/tex">w</script> 的更新公式是</p>

<script type="math/tex; mode=display">w_{k+1}=w_{k}-\eta_k\nabla f_{i_k}(w_{k})</script>

<p>相对于梯度下降，SGD 的好处非常明显，就是可以减少每次更新的计算代价，不过也正是因为每次都是随机的使用一个样本或一个 mini batch 来估计梯度，因此<strong>对梯度估计的方差就大了</strong>。这给 SGD 带来的问题是收敛速度不如梯度下降，从收敛速度分析上看，梯度下降则可以在目标函数强凸的情况下做到 <script type="math/tex">% <![CDATA[
O(\rho^T) (\rho<1) %]]></script> 的线性收敛（linear convergence），在目标函数为凸函数的情况下可以做到次线性收敛 <script type="math/tex">O(1/T)</script>（收敛速度是衡量优化算法计算复杂度的基本工具，可以参考 <a href="https://en.wikipedia.org/wiki/Rate_of_convergence" title="Rate_of_convergence">wiki</a> 或者 <a href="https://zhuanlan.zhihu.com/p/27644403">这里</a>）。而 SGD 能够在目标函数强凸并且递减步长的情况下只做到 <script type="math/tex">O(1/T)</script> 的次线性收敛（sublinear convergence）。也就是说，如果想快速得到一个可以勉强接受的解，SGD 比梯度下降更加合适，但是如果想得到一个精确度高的解，应当选择梯度下降，因为为了达到同样的精度，SGD 需要的总迭代次数要大于梯度下降。</p>

<p>至于为什么对梯度估计的方差过大会降低收敛速度，以及 SGD 为什么一定要步长递减，具体原因可参考文章 <a href="#Reference">[4]</a> 的 Theorem 4.6 和 Theorem 4.7 以及 Theorem 4.8，Theorem 4.9 和 Theorem 4.10，简单来说就是如果步长不是递减的，SGD 会收敛到最优解的一个领域中，对梯度估计的噪声让它最终无法进一步收敛。既然影响 SGD 收敛速度的主要原因之一是在所计算的梯度的方差，那就想办法降低这个方差，然后自然就可以提升算法的收敛速度了，这一类方法就被称为方差缩减方法（Noise Reduction Methods）。</p>

<h2 id="noise-reduction-methods">2. 方差缩减方法 Noise Reduction Methods</h2>

<p>在所谓的方差缩减方法中，又可以分为 3 小类，第一类动态采样方法（dynamic sampling methods）是通过在计算梯度时逐步增加样本量来减少梯度估计的方差；第二类迭代平均方法（iterate averaging methods）则是通过对得到的 <script type="math/tex">w</script> 进行历史平均来减少其方差；第三类梯度聚合方法（gradient aggregation methods）则是通过存储历史梯度，在每次估计梯度时用历史梯度来做修正，前两类方法太过暴力而不优雅，因此我们作为优雅的人主要讨论第三类方法。</p>

<p>我们先来直观的感受下什么叫方差缩减，比如你要通过 monte carlo 采样的方法来估计随机变量 <script type="math/tex">X</script> 的期望 <script type="math/tex">\mathbb{E}X</script>，同时假设你已经能够比较容易地估计与随机变量 <script type="math/tex">X</script> 强相关的随机变量 <script type="math/tex">Y</script> 的期望 <script type="math/tex">\mathbb{E}Y</script>，一个利用方差缩减思想来近似估计 <script type="math/tex">\mathbb{E}X</script> 的估计量 (estimator) 是：</p>

<script type="math/tex; mode=display">\theta_{\alpha}=\alpha(X-Y)+\mathbb{E}Y</script>

<p>我们可以看看这个估计量的期望和方差：</p>

<script type="math/tex; mode=display">\mathbb{E}\theta_{\alpha}=\alpha\mathbb{E}X+(1-\alpha)\mathbb{E}Y \\
Var(\theta_{\alpha})=\alpha^2[Var(X)+Var(Y)-2Cov(X,Y)]</script>

<p>可以发现，当 <script type="math/tex">\alpha=1</script> 时，<script type="math/tex">\theta_{\alpha}</script> 是 <script type="math/tex">\mathbb{E}X</script> 的无偏估计，且当 <script type="math/tex">Cov(X,Y)</script> 足够大的时候，<script type="math/tex">\theta_{\alpha}</script> 的方差也比直接对 <script type="math/tex">X</script> 做估计要来的小，这就是方差缩减的基本思想。后面要提到的各种方差缩减类算法比如大名鼎鼎的 SVRG 等都是这个<strong>套路，套路，套路</strong>。</p>

<h2 id="sag--10reference">3. SAG 算法 <a href="#Reference">[10]</a></h2>

<p>SAG 算法想啊，既然 GD 用上所有数据的梯度就能做到线性收敛，SGD 一次只能用一个样本或者一小批样本来计算梯度，因此对梯度估计的方差影响了最终的收敛速度，那我们能不能在 SGD 上也能用上所有样本的梯度呢？于是就有了 SAG 算法的更新方式：</p>

<script type="math/tex; mode=display">% <![CDATA[
w_{k+1}=w_k-\frac{\eta_k}{n}\sum_{i=1}^{n}y_i^{k} \\
y_i^{k}=\left\{
\begin{aligned}
&\nabla f_i(w_k), \, &\text{if}\,i_k=i\\
&\nabla f_i(w_{k-1}) \, &\text{otherwise}
\end{aligned}
\right. %]]></script>

<p>可以把上面的梯度计算方式写成一个式子</p>

<script type="math/tex; mode=display">\tilde g_k \, \leftarrow \, \frac{\nabla f_{i_k}(w_k) - \nabla f_{i_k}(w_{k-1})}{n} + \nabla R_n(w_{k-1})</script>

<p>这样就容易看出这个方法就是上面 <script type="math/tex">\alpha=1/n</script> 时的一个应用，虽然这种估计不是无偏估计，但是方差会以 <script type="math/tex">1/n^2</script> 的比例缩小。这个方法理论上可以获得于 GD 方法同样的收敛速度。另外，在实践中，需要在内存里保存所有样本的上一次梯度值。</p>

<h2 id="svrg--5reference">4. SVRG 算法 <a href="#Reference">[5]</a></h2>

<p>因为 SAG 算法需要保存所有样本的梯度值，在实际的大规模工业应用中并不实用，因此就有了 SVRG 算法。</p>

<p><strong>SVRG 算法的迭代过程</strong> (图片来自文章 <a href="#Reference">[4]</a>)</p>

<p><img src="/css/pics/2018-05-07-SVRG-SVRG.png" alt="svrg算法" /></p>

<p>SVRG 算法在每一轮迭代的内部有一个内部的迭代，在进行内部迭代前用当前的 <script type="math/tex">w_k</script> 值计算一次所有样本的平均梯度 <script type="math/tex">\nabla R_n(w_k)</script>，内部迭代的初始值被赋予为当前的 <script type="math/tex">w_k</script>，内部迭代中每次的梯度采用如下方式计算：</p>

<script type="math/tex; mode=display">\tilde g_j \, \leftarrow \, \nabla f_{i_j}(\tilde w_j)-(\nabla f_{i_j}(w_k)-\nabla R_n(w_k))</script>

<p>按照上文对方差缩减方法的描述，对此公式为什么能降低梯度估计的方差就可以有个直观的解释：因为 <script type="math/tex">\nabla f_{i_j}(w_k)</script> 的期望就是 <script type="math/tex">\nabla R_n(w_k)</script>，因此可以将 <script type="math/tex">(\nabla f_{i_j}(w_k)-\nabla R_n(w_k))</script> 视为梯度估计 <script type="math/tex">\nabla f_{i_j}(w_k)</script> 的 bias，那么在每一次的迭代中，算法都对基于当前参数 <script type="math/tex">\tilde w_j</script> 做的梯度估计 <script type="math/tex">\nabla f_{i_j}(\tilde w_j)</script> 进行了一次修正。在 SGD 的收敛性分析中，假定了样本梯度的方差是有个常数上界的 (见文章 <a href="#Reference">[4]</a> 的 Assumption 4.3(c))，正是这个常数上界的存在导致 SGD 算法无法线性收敛，SVRG 利用它新的更新方式可以让估计的梯度方差有个不断减小的上界 (见文章 <a href="#Reference">[4]</a> 的 Theorem 5.1)，这就是 SVRG 算法的核心思想，这也是为什么这个算法被称为 SVRG（stochastic variance reduced gradient）的原因，SVRG 算法在目标函数光滑和强凸的情况下做到线性收敛速度。更多更为正式更为数学的分析可参考 bottou 大神写的综述文章 <a href="#Reference">[4]</a>。</p>

<p>SVRG 算法的问题是虽然不用存储所有样本的梯度了，但是计算量上去了，因为它在每次的大迭代里面还有一轮小迭代，每次都要算两遍梯度，整体的计算量已经和 GD 一样了，而且还多一个超参数 <script type="math/tex">m</script> 要调。</p>

<h2 id="saga--6reference">5. SAGA 算法 <a href="#Reference">[6]</a></h2>

<p>SAGA 算法其实是介于 SAG 和 SVRG 之间的一种算法，但作者声称在强凸的条件下其收敛速度要快于 SAG 和 SVRG，且两倍于 SDCA，并同时适用于非强凸的情形。</p>

<p><strong>SAGA 算法的迭代过程</strong> (图片来自文章 <a href="#Reference">[4]</a>)</p>

<p><img src="/css/pics/2018-05-07-SVRG-SAGA.png" alt="saga算法" /></p>

<p>同样，如果我们仔细审视它的梯度计算过程：</p>

<script type="math/tex; mode=display">\tilde g_k \, \leftarrow \, \nabla f_{i_k}(w_k) - \nabla f_{i_k}(w_{k-1}) + \nabla R_n(w_{k-1})</script>

<p>会发现，相比于 SAG 算法，SAGA 算法只是把前面的一个 <script type="math/tex">1/n</script> 去掉了，使得对梯度的估计变成无偏的了，当然同时它的方差相比于 SAG 也大了 <script type="math/tex">n^2</script>。</p>

<h2 id="scsg--8reference">6. SCSG 算法 <a href="#Reference">[8]</a></h2>

<p>SVRG 的主要特征就是利用全部数据的梯度来对 SGD 的方差进行控制。因此 SVRG 的计算成本（Computation Cost）是 <script type="math/tex">O((n+m)T)</script>。这里 <script type="math/tex">n</script> 是数据的总数，<script type="math/tex">m</script> 是 Step-size，而 <script type="math/tex">T</script> 是论数。SVRG 的通讯成本也是这么多，这里面的主要成本在于每一轮都需要对全局数据进行访问。</p>

<p>Stochastically Controlled Stochastic Gradient（SCSG）算法就是对 SVRG 进行了两个改进：</p>

<ul>
  <li>每一轮并不用全局的数据进行梯度的计算，而是从一个全局的子集 Batch 中估计梯度，子集的大小是 B。</li>
  <li>每一轮 SGD 的更新数目 <script type="math/tex">N</script> 也不是一个定值，而是一个和之前那个子集大小有关系，基于 Geometric Distribution 的随机数。</li>
</ul>

<p>剩下的更新步骤和 SVRG 一模一样。</p>

<p>然而，这样的改变之后，新算法的计算成本成为了 <script type="math/tex">O((B+N)T)</script>。也就是说，这是一个不依赖全局数据量大小的数值。而通过分析，作者们也比较了 SCSG 的通讯成本和一些原本就为了通讯成本而设计的算法，在很多情况下，SCSG 的通讯成本更优。通过 MNIST 数据集的实验发现，SCSG 达到相同的准确度，需要比 SVRG 更少的轮数，和每一轮更少的数据。可以说，这个算法可能会成为 SVRG 的简单替代。</p>

<h3 id="section-1">7. 其它方法</h3>

<p>S2GD(Semi-Stochastic Gradient Descent Methods)</p>

<p>Mini-Batch Semi-Stochastic Gradient Descent in the Proximal Setting. 将S2GD扩展到mini-batch上，因此允许并行运行，但是需要更多的同步，只能允许小的batch</p>

<p>SDCA(Stochastic dual coordinate ascent methods for regularized loss) <a href="#Reference">[7]</a></p>

<p>Finito(Finito: A faster, permutable incremental gradientmethod for big data problems)</p>

<h2 id="section-2">8. 应用于在线学习</h2>

<p>online-SVRG</p>

<h2 id="reference">Reference</h2>

<p>[1] Cauchy, Augustin. “Méthode générale pour la résolution des systemes d’équations simultanées.” Comp. Rend. Sci. Paris 25.1847 (1847): 536-538.</p>

<p>[2] Robbins, Herbert, and Sutton Monro. “A stochastic approximation method.” The annals of mathematical statistics (1951): 400-407.</p>

<p>[3] Kiefer, Jack, and Jacob Wolfowitz. “Stochastic estimation of the maximum of a regression function.” The Annals of Mathematical Statistics 23.3 (1952): 462-466.</p>

<p>[4] Bottou, Léon, Frank E. Curtis, and Jorge Nocedal. “Optimization methods for large-scale machine learning.” <em>SIAM Review</em> 60.2 (2018): 223-311.</p>

<p>[5] Johnson, Rie, and Tong Zhang. “Accelerating stochastic gradient descent using predictive variance reduction.” <em>Advances in Neural Information Processing Systems</em>. 2013.</p>

<p>[6] Defazio, Aaron, Francis Bach, and Simon Lacoste-Julien. “Saga: A fast incremental gradient method with support for non-strongly convex composite objectives.” <em>Advances in Neural Information Processing Systems</em>. 2014.</p>

<p>[7] Shalev-Shwartz, Shai, and Tong Zhang. “Accelerated mini-batch stochastic dual coordinate ascent.” <em>Advances in Neural Information Processing Systems</em>. 2013.</p>

<p>[8] Lei, Lihua, and Michael Jordan. “Less than a Single Pass: Stochastically Controlled Stochastic Gradient.” <em>Artificial Intelligence and Statistics</em>. 2017.</p>

<p>[9] Nitanda, Atsushi. “Stochastic proximal gradient descent with acceleration techniques.” Advances in Neural Information Processing Systems. 2014.</p>

<p>[10] Roux, Nicolas L., Mark Schmidt, and Francis R. Bach. “A stochastic gradient method with an exponential convergence _rate for finite training sets.” Advances in Neural Information Processing Systems. 2012.</p>

      </article>
      <hr>
      <div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_copy" data-cmd="copy" title="分享到复制网址"></a></div>
      <script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>
    </div>
    <!--div class="post-area post comment"-->
      <!-- 多说评论框 start -->
      
      <!--div class="ds-thread" data-thread-key="/2018/05/11/SVRG论文阅读笔记" data-title="SVRG算法的阅读理解和实践" data-url="caoxiaoqing.github.io/2018/05/11/SVRG%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"></div-->
      
      <!-- 多说评论框 end -->
    <!--/div-->
  </div>

  <div id="content" class="col-sm-3">
    <!-- <div id="myAffix" class="shadow-bottom-center hidden-xs" data-spy="affix" data-offset-top="0" data-offset-bottom="-20"> -->
    <div id="myAffix" class="shadow-bottom-center hidden-xs" >
      <div class="categories-list-header">
        Content
      </div>
      <div class="content-text"></div>
    </div>
  </div>
</div>
    </div>

    
    <div id="top" data-toggle="tooltip" data-placement="left" title="回到顶部">
      <a href="javascript:;">
        <div class="arrow"></div>
        <div class="stick"></div>
      </a>
    </div>

    <footer class="">
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <a href="mailto:CaoXiaoqing2008@gmail/163.com"><span class="glyphicon glyphicon-envelope"></span> CaoXiaoqing2008@gmail/163.com</a>
        <span class="point"> · </span>
        
          
          <a href="https://github.com/caoxiaoqing">
            <span class="icon">
              <svg viewBox="0 0 16 16">
                <path fill="#aaa" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            GitHub
            <!-- <span>caoxiaoqing</span> -->
          </a>
          
          
          <span class="point"> · </span>
          <!--span><a href="/feed.xml">RSS</a></span-->
          <!--span class="point"> · </span-->
          <span>曹孝卿的博客 生活爱好者</span>
          <span class="point"> · </span>
          <span>&copy; 2018 CaoXiaoqing</span>

      </div>
    </div>
  </div>
</footer>


    <!-- 多说公共JS代码 start (一个网页只需插入一次) -->
  <!--script type="text/javascript">
    var duoshuoQuery = {short_name:"cxqblog"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script-->
<!-- 多说公共JS代码 end -->

<!-- 在新窗口中打开 -->
  <script type="text/javascript">
    function addBlankTargetForLinks () {
      $('a[href^="http"]').each(function(){
          $(this).attr('target', '_blank');
      });
    }
    $(document).bind('DOMNodeInserted', function(event) {
      addBlankTargetForLinks();
    });
  </script>
  </body>
</html>
