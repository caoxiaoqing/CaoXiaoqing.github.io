<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>SVRG算法的阅读理解和实践</title>
    <meta name="description" content="最佳拜读了下大名鼎鼎的 SVRG 算法 [5]，读完后把前前后后涉及到的方法都看了一遍，这里做个简单的综述和阅读理解，并描述了如何将方差缩减思想应用于在线学习。">

    <link rel="shortcut icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://at.alicdn.com/t/font_8v3czwksspqlg14i.css">
    <link rel="stylesheet" href="/css/main.css ">
    <link rel="canonical" href="https://caoxiaoqing.github.io/2018/05/11/SVRG%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">
    <link rel="alternate" type="application/rss+xml" title="CXQ" href="https://caoxiaoqing.github.io/feed.xml ">


    <script>
    // 百度统计代码
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?bf7bdaf2bdac297bf8b3a6797ec58ce0";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
    </script>



<script type="text/javascript" 
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


</head>


  <body>

    <header id="top">
    <div class="wrapper">
        <a href="/" class="brand">CXQ</a>
        <small>free your mind</small>
        <button id="headerMenu" class="menu"><i class="fa fa-bars"></i></button>
        <nav id="headerNav">
            <ul>
                <li>
                    
                    <a href="/">
                    
                        <i class="fa fa-home"></i>Home
                    </a>
                </li>

                
                    
                    <li>
                        
                        <a href="/archive/">
                        
                            <i class="fa fa-archive"></i>Archives
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/category/">
                        
                            <i class="fa fa-th-list"></i>Categories
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/tag/">
                        
                            <i class="fa fa-tags"></i>Tags
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/collection/">
                        
                            <i class="fa fa-bookmark"></i>Collections
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/about/">
                        
                            <i class="fa fa-heart"></i>About
                        </a>
                    </li>
                    
                
                    
                
                    
                
                    
                
            </ul>
        </nav>
    </div>
</header>


        <div class="page clearfix" post>
    <div class="left">
        <h1>SVRG算法的阅读理解和实践</h1>
        <div class="label">

            <div class="label-card">
                <i class="fa fa-calendar"></i>2018-05-11
            </div>

            <div class="label-card">
                <i class="fa fa-user"></i>caoxiaoqing
                
            </div>

            <div class="label-card">
                
            </div>

            <div class="label-card">
            


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#算法" title="Category: 算法" rel="category">算法</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


            </div>

            <div class="label-card">
            
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <!--a href="/tag/#%E7%AE%97%E6%B3%95" title="Tag: 算法" rel="tag">算法</a-->
        <a href="/tag/#算法" title="Tag: 算法" rel="tag">算法</a>&nbsp;
    
        <!--a href="/tag/#SVRG" title="Tag: SVRG" rel="tag">SVRG</a-->
        <a href="/tag/#SVRG" title="Tag: SVRG" rel="tag">SVRG</a>&nbsp;
    
        <!--a href="/tag/#%E6%96%B9%E5%B7%AE%E7%BC%A9%E5%87%8F" title="Tag: 方差缩减" rel="tag">方差缩减</a-->
        <a href="/tag/#方差缩减" title="Tag: 方差缩减" rel="tag">方差缩减</a>&nbsp;
    
        <!--a href="/tag/#SGD" title="Tag: SGD" rel="tag">SGD</a-->
        <a href="/tag/#SGD" title="Tag: SGD" rel="tag">SGD</a>
    
  

</span>

            </div>

        </div>
        <hr>
        <article itemscope itemtype="http://schema.org/BlogPosting">
        <ul id="markdown-toc">
  <li><a href="#section" id="markdown-toc-section">1. 背景介绍</a></li>
  <li><a href="#noise-reduction-methods" id="markdown-toc-noise-reduction-methods">2. 方差缩减方法 Noise Reduction Methods</a></li>
  <li><a href="#sag--10reference" id="markdown-toc-sag--10reference">3. SAG 算法 <a href="#Reference">[10]</a></a></li>
  <li><a href="#svrg--5reference" id="markdown-toc-svrg--5reference">4. SVRG 算法 <a href="#Reference">[5]</a></a></li>
  <li><a href="#saga--6reference" id="markdown-toc-saga--6reference">5. SAGA 算法 <a href="#Reference">[6]</a></a></li>
  <li><a href="#scsg--8reference" id="markdown-toc-scsg--8reference">6. SCSG 算法 <a href="#Reference">[8]</a></a></li>
  <li><a href="#section-1" id="markdown-toc-section-1">7. 其它方法</a></li>
  <li><a href="#section-2" id="markdown-toc-section-2">8. 应用于在线学习</a></li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ul>

<p>最佳拜读了下大名鼎鼎的 SVRG 算法 <a href="#Reference">[5]</a>，读完后把前前后后涉及到的方法都看了一遍，这里做个简单的综述和阅读理解，并描述了如何将方差缩减思想应用于在线学习。</p>

<h2 id="section">1. 背景介绍</h2>

<p>考虑优化问题：</p>

<script type="math/tex; mode=display">min \, R_n(w)=\frac{1}{n}\sum_{i=1}^{n}f_i(w)</script>

<p>当我们采用 Gradient Descent (GD) 方法时，<script type="math/tex">w</script> 的更新公式是：</p>

<script type="math/tex; mode=display">w_{k+1}=w_{k}-\eta_k\nabla R_n(w_{k})=w_{k}-\frac{\eta_k}{n}\sum_{i=1}^{n}\nabla f_{i}(w_{k})</script>

<p>梯度下降方法可以追溯到 Cauchy 1847 年的论文 <a href="#Reference">[1]</a>。梯度下降对于样本数目比较多的时候有一个很大的劣势，那就是每次需要求解所有样本的梯度，导致计算量大增，所以实际生产环境中，往往采用随机梯度下降算法（Stochastic Gradient Descent），一般简写做 SGD，它于 1951 和 1952 年在文献 <a href="#Reference">[2,3]</a> 中被提出。SGD 每次迭代的时候均匀随机得选择一个样本或者 mini-batch 做更新。当我们采用 SGD 方法来进行计算的时候，<script type="math/tex">w</script> 的更新公式是</p>

<script type="math/tex; mode=display">w_{k+1}=w_{k}-\eta_k\nabla f_{i_k}(w_{k})</script>

<p>相对于梯度下降，SGD 的好处非常明显，就是可以减少每次更新的计算代价，不过也正是因为每次都是随机的使用一个样本或一个 mini batch 来估计梯度，因此<strong>对梯度估计的方差就大了</strong>。这给 SGD 带来的问题是收敛速度不如梯度下降，从收敛速度分析上看，梯度下降则可以在目标函数强凸的情况下做到 <script type="math/tex">% <![CDATA[
O(\rho^T) (\rho<1) %]]></script> 的线性收敛（linear convergence），在目标函数为凸函数的情况下可以做到次线性收敛 <script type="math/tex">O(1/T)</script>（收敛速度是衡量优化算法计算复杂度的基本工具，可以参考 <a href="https://en.wikipedia.org/wiki/Rate_of_convergence" title="Rate_of_convergence">wiki</a> 或者 <a href="https://zhuanlan.zhihu.com/p/27644403">这里</a>）。而 SGD 能够在目标函数强凸并且递减步长的情况下只做到 <script type="math/tex">O(1/T)</script> 的次线性收敛（sublinear convergence）。也就是说，如果想快速得到一个可以勉强接受的解，SGD 比梯度下降更加合适，但是如果想得到一个精确度高的解，应当选择梯度下降，因为为了达到同样的精度，SGD 需要的总迭代次数要大于梯度下降。</p>

<p>至于为什么对梯度估计的方差过大会降低收敛速度，以及 SGD 为什么一定要步长递减，具体原因可参考文章 <a href="#Reference">[4]</a> 的 Theorem 4.6 和 Theorem 4.7 以及 Theorem 4.8，Theorem 4.9 和 Theorem 4.10，简单来说就是如果步长不是递减的，SGD 会收敛到最优解的一个领域中，对梯度估计的噪声让它最终无法进一步收敛。既然影响 SGD 收敛速度的主要原因之一是在所计算的梯度的方差，那就想办法降低这个方差，然后自然就可以提升算法的收敛速度了，这一类方法就被称为方差缩减方法（Noise Reduction Methods）。</p>

<h2 id="noise-reduction-methods">2. 方差缩减方法 Noise Reduction Methods</h2>

<p>在所谓的方差缩减方法中，又可以分为 3 小类，第一类动态采样方法（dynamic sampling methods）是通过在计算梯度时逐步增加样本量来减少梯度估计的方差；第二类迭代平均方法（iterate averaging methods）则是通过对得到的 <script type="math/tex">w</script> 进行历史平均来减少其方差；第三类梯度聚合方法（gradient aggregation methods）则是通过存储历史梯度，在每次估计梯度时用历史梯度来做修正，前两类方法太过暴力而不优雅，因此我们作为优雅的人主要讨论第三类方法。</p>

<p>我们先来直观的感受下什么叫方差缩减，比如你要通过 monte carlo 采样的方法来估计随机变量 <script type="math/tex">X</script> 的期望 <script type="math/tex">\mathbb{E}X</script>，同时假设你已经能够比较容易地估计与随机变量 <script type="math/tex">X</script> 强相关的随机变量 <script type="math/tex">Y</script> 的期望 <script type="math/tex">\mathbb{E}Y</script>，一个利用方差缩减思想来近似估计 <script type="math/tex">\mathbb{E}X</script> 的估计量 (estimator) 是：</p>

<script type="math/tex; mode=display">\theta_{\alpha}=\alpha(X-Y)+\mathbb{E}Y</script>

<p>我们可以看看这个估计量的期望和方差：</p>

<script type="math/tex; mode=display">\mathbb{E}\theta_{\alpha}=\alpha\mathbb{E}X+(1-\alpha)\mathbb{E}Y \\
Var(\theta_{\alpha})=\alpha^2[Var(X)+Var(Y)-2Cov(X,Y)]</script>

<p>可以发现，当 <script type="math/tex">\alpha=1</script> 时，<script type="math/tex">\theta_{\alpha}</script> 是 <script type="math/tex">\mathbb{E}X</script> 的无偏估计，且当 <script type="math/tex">Cov(X,Y)</script> 足够大的时候，<script type="math/tex">\theta_{\alpha}</script> 的方差也比直接对 <script type="math/tex">X</script> 做估计要来的小，这就是方差缩减的基本思想。后面要提到的各种方差缩减类算法比如大名鼎鼎的 SVRG 等都是这个<strong>套路，套路，套路</strong>。</p>

<h2 id="sag--10reference">3. SAG 算法 <a href="#Reference">[10]</a></h2>

<p>SAG 算法想啊，既然 GD 用上所有数据的梯度就能做到线性收敛，SGD 一次只能用一个样本或者一小批样本来计算梯度，因此对梯度估计的方差影响了最终的收敛速度，那我们能不能在 SGD 上也能用上所有样本的梯度呢？于是就有了 SAG 算法的更新方式：</p>

<script type="math/tex; mode=display">% <![CDATA[
w_{k+1}=w_k-\frac{\eta_k}{n}\sum_{i=1}^{n}y_i^{k} \\
y_i^{k}=\left\{
\begin{aligned}
&\nabla f_i(w_k), \, &\text{if}\,i_k=i\\
&\nabla f_i(w_{k-1}) \, &\text{otherwise}
\end{aligned}
\right. %]]></script>

<p>可以把上面的梯度计算方式写成一个式子</p>

<script type="math/tex; mode=display">\tilde g_k \, \leftarrow \, \frac{\nabla f_{i_k}(w_k) - \nabla f_{i_k}(w_{k-1})}{n} + \nabla R_n(w_{k-1})</script>

<p>这样就容易看出这个方法就是上面 <script type="math/tex">\alpha=1/n</script> 时的一个应用，虽然这种估计不是无偏估计，但是方差会以 <script type="math/tex">1/n^2</script> 的比例缩小。这个方法理论上可以获得于 GD 方法同样的收敛速度。另外，在实践中，需要在内存里保存所有样本的上一次梯度值。</p>

<h2 id="svrg--5reference">4. SVRG 算法 <a href="#Reference">[5]</a></h2>

<p>因为 SAG 算法需要保存所有样本的梯度值，在实际的大规模工业应用中并不实用，因此就有了 SVRG 算法。</p>

<p><strong>SVRG 算法的迭代过程</strong> (图片来自文章 <a href="#Reference">[4]</a>)</p>

<p><img src="/media/pics/2018-05-07-SVRG-SVRG.png" alt="svrg算法" /></p>

<p>SVRG 算法在每一轮迭代的内部有一个内部的迭代，在进行内部迭代前用当前的 <script type="math/tex">w_k</script> 值计算一次所有样本的平均梯度 <script type="math/tex">\nabla R_n(w_k)</script>，内部迭代的初始值被赋予为当前的 <script type="math/tex">w_k</script>，内部迭代中每次的梯度采用如下方式计算：</p>

<script type="math/tex; mode=display">\tilde g_j \, \leftarrow \, \nabla f_{i_j}(\tilde w_j)-(\nabla f_{i_j}(w_k)-\nabla R_n(w_k))</script>

<p>按照上文对方差缩减方法的描述，对此公式为什么能降低梯度估计的方差就可以有个直观的解释：因为 <script type="math/tex">\nabla f_{i_j}(w_k)</script> 的期望就是 <script type="math/tex">\nabla R_n(w_k)</script>，因此可以将 <script type="math/tex">(\nabla f_{i_j}(w_k)-\nabla R_n(w_k))</script> 视为梯度估计 <script type="math/tex">\nabla f_{i_j}(w_k)</script> 的 bias，那么在每一次的迭代中，算法都对基于当前参数 <script type="math/tex">\tilde w_j</script> 做的梯度估计 <script type="math/tex">\nabla f_{i_j}(\tilde w_j)</script> 进行了一次修正。在 SGD 的收敛性分析中，假定了样本梯度的方差是有个常数上界的 (见文章 <a href="#Reference">[4]</a> 的 Assumption 4.3(c))，正是这个常数上界的存在导致 SGD 算法无法线性收敛，SVRG 利用它新的更新方式可以让估计的梯度方差有个不断减小的上界 (见文章 <a href="#Reference">[4]</a> 的 Theorem 5.1)，这就是 SVRG 算法的核心思想，这也是为什么这个算法被称为 SVRG（stochastic variance reduced gradient）的原因，SVRG 算法在目标函数光滑和强凸的情况下做到线性收敛速度。更多更为正式更为数学的分析可参考 bottou 大神写的综述文章 <a href="#Reference">[4]</a>。</p>

<p>SVRG 算法的问题是虽然不用存储所有样本的梯度了，但是计算量上去了，因为它在每次的大迭代里面还有一轮小迭代，每次都要算两遍梯度，整体的计算量已经和 GD 一样了，而且还多一个超参数 <script type="math/tex">m</script> 要调。</p>

<h2 id="saga--6reference">5. SAGA 算法 <a href="#Reference">[6]</a></h2>

<p>SAGA 算法其实是介于 SAG 和 SVRG 之间的一种算法，但作者声称在强凸的条件下其收敛速度要快于 SAG 和 SVRG，且两倍于 SDCA，并同时适用于非强凸的情形。</p>

<p><strong>SAGA 算法的迭代过程</strong> (图片来自文章 <a href="#Reference">[4]</a>)</p>

<p><img src="/media/pics/2018-05-07-SVRG-SAGA.png" alt="saga算法" /></p>

<p>同样，如果我们仔细审视它的梯度计算过程：</p>

<script type="math/tex; mode=display">\tilde g_k \, \leftarrow \, \nabla f_{i_k}(w_k) - \nabla f_{i_k}(w_{k-1}) + \nabla R_n(w_{k-1})</script>

<p>会发现，相比于 SAG 算法，SAGA 算法只是把前面的一个 <script type="math/tex">1/n</script> 去掉了，使得对梯度的估计变成无偏的了，当然同时它的方差相比于 SAG 也大了 <script type="math/tex">n^2</script>。</p>

<h2 id="scsg--8reference">6. SCSG 算法 <a href="#Reference">[8]</a></h2>

<p>SVRG 的主要特征就是利用全部数据的梯度来对 SGD 的方差进行控制。因此 SVRG 的计算成本（Computation Cost）是 <script type="math/tex">O((n+m)T)</script>。这里 <script type="math/tex">n</script> 是数据的总数，<script type="math/tex">m</script> 是 Step-size，而 <script type="math/tex">T</script> 是论数。SVRG 的通讯成本也是这么多，这里面的主要成本在于每一轮都需要对全局数据进行访问。</p>

<p>Stochastically Controlled Stochastic Gradient（SCSG）算法就是对 SVRG 进行了两个改进：</p>

<ul>
  <li>每一轮并不用全局的数据进行梯度的计算，而是从一个全局的子集 Batch 中估计梯度，子集的大小是 B。</li>
  <li>每一轮 SGD 的更新数目 <script type="math/tex">N</script> 也不是一个定值，而是一个和之前那个子集大小有关系，基于 Geometric Distribution 的随机数。</li>
</ul>

<p>剩下的更新步骤和 SVRG 一模一样。</p>

<p>然而，这样的改变之后，新算法的计算成本成为了 <script type="math/tex">O((B+N)T)</script>。也就是说，这是一个不依赖全局数据量大小的数值。而通过分析，作者们也比较了 SCSG 的通讯成本和一些原本就为了通讯成本而设计的算法，在很多情况下，SCSG 的通讯成本更优。通过 MNIST 数据集的实验发现，SCSG 达到相同的准确度，需要比 SVRG 更少的轮数，和每一轮更少的数据。可以说，这个算法可能会成为 SVRG 的简单替代。</p>

<h2 id="section-1">7. 其它方法</h2>

<p>S2GD(Semi-Stochastic Gradient Descent Methods)</p>

<p>Mini-Batch Semi-Stochastic Gradient Descent in the Proximal Setting. 将S2GD扩展到mini-batch上，因此允许并行运行，但是需要更多的同步，只能允许小的batch</p>

<p>SDCA(Stochastic dual coordinate ascent methods for regularized loss) <a href="#Reference">[7]</a></p>

<p>Finito(Finito: A faster, permutable incremental gradientmethod for big data problems)</p>

<h2 id="section-2">8. 应用于在线学习</h2>

<p>以上算法都只是适用于有限数据集的，我所关注的在线学习面临的是无限数据集，因此上述方法都不适用。但是有了上面的套路，我们也可以轻松地设计出适用在线学习的方差缩减方法，比如下面这个算法：</p>

<p><img src="/media/pics/2018-05-07-SVRG-online-v1.png" alt="online SVRG算法" /></p>

<p>对上面 online SVRG v1 算法做几点说明：</p>

<ul>
  <li><script type="math/tex">g_{old}</script> 来自上一次的梯度，这里可以不要求是同一个 mini batch 的数据，其中的 <script type="math/tex">w_k</script> 也可以是被其他 worker 更新后的值，因此，这个算法可以适用于多 worker 异步更新的场合，因为我们可以假设在很短的一个时间内，所有 worker 拿到的数据都是独立同分布的。</li>
  <li>为了获得上文第二部分所说的 <script type="math/tex">\mathbb{E}Y</script>，也就是上文其他算法中的 <script type="math/tex">\nabla R_n(w_k)</script>，我们使用了一个滑动加权平均的方式，其中参数 <script type="math/tex">d</script> 是为了控制历史值与当前值的权重分配。</li>
</ul>

<p>为什么我们还要指定学习率 <script type="math/tex">\eta</script> 和衰减系数 <script type="math/tex">d</script> 呢？已经有很多算法可以免去这两个超参数了，我们可以把他们的思想拿过来，和上面的算法进行综合，得到一个真正免超参，同时又能降低梯度估计方差，提升学习率的算法，这就是我们后续提出的 online SVRG v2 算法，关于这个算法我们下一篇再写，这里写不下了。</p>

<h2 id="reference">Reference</h2>

<p>[1] Cauchy, Augustin. “Méthode générale pour la résolution des systemes d’équations simultanées.” Comp. Rend. Sci. Paris 25.1847 (1847): 536-538.</p>

<p>[2] Robbins, Herbert, and Sutton Monro. “A stochastic approximation method.” The annals of mathematical statistics (1951): 400-407.</p>

<p>[3] Kiefer, Jack, and Jacob Wolfowitz. “Stochastic estimation of the maximum of a regression function.” The Annals of Mathematical Statistics 23.3 (1952): 462-466.</p>

<p>[4] Bottou, Léon, Frank E. Curtis, and Jorge Nocedal. “Optimization methods for large-scale machine learning.” <em>SIAM Review</em> 60.2 (2018): 223-311.</p>

<p>[5] Johnson, Rie, and Tong Zhang. “Accelerating stochastic gradient descent using predictive variance reduction.” <em>Advances in Neural Information Processing Systems</em>. 2013.</p>

<p>[6] Defazio, Aaron, Francis Bach, and Simon Lacoste-Julien. “Saga: A fast incremental gradient method with support for non-strongly convex composite objectives.” <em>Advances in Neural Information Processing Systems</em>. 2014.</p>

<p>[7] Shalev-Shwartz, Shai, and Tong Zhang. “Accelerated mini-batch stochastic dual coordinate ascent.” <em>Advances in Neural Information Processing Systems</em>. 2013.</p>

<p>[8] Lei, Lihua, and Michael Jordan. “Less than a Single Pass: Stochastically Controlled Stochastic Gradient.” <em>Artificial Intelligence and Statistics</em>. 2017.</p>

<p>[9] Nitanda, Atsushi. “Stochastic proximal gradient descent with acceleration techniques.” Advances in Neural Information Processing Systems. 2014.</p>

<p>[10] Roux, Nicolas L., Mark Schmidt, and Francis R. Bach. “A stochastic gradient method with an exponential convergence _rate for finite training sets.” Advances in Neural Information Processing Systems. 2012.</p>

        </article>
        <hr>

        
        
            
            
                
                    
                
                    
                
                    
                
                    
                
            
                
                    
                
                    
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
                    
                
                    
                
            
                
                    
                
                    
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
                    
                
                    
                
            
        
            
            
                
                    
                        
                        <h2 id="similar_posts">Similar Posts</h2>
                        <ul>
                        
                        <li class="relatedPost">
                            <a href="/2016/03/20/MCMC(1)-Bayes/">MCMC方法(一):贝叶斯推断
                            
                            </a>
                        </li>
                        
                        
                    
                
                    
                
                    
                
                    
                
            
                
                    
                
                    
                
                    
                
                    
                
            
        
            
            
        
        
            </ul>
        

        <div class="post-recent">
    <div class="pre">
        
        <p><strong>上一篇</strong> <a href="/2018/05/08/%E6%84%8F%E8%AF%86%E8%AF%9E%E7%94%9F%E4%BA%8E%E4%B8%8A%E5%B8%9D%E6%B2%89%E9%BB%98%E6%97%B6/">意识诞生于上帝沉默时</a></p>
        
    </div>
    <div class="nex">

        
    </div>
</div>


        <h2 id="comments">Comments</h2>
        <div id="container"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: 'location.href', // 可选。默认为 location.href
  owner: 'caoxiaoqing',
  repo: 'https://github.com/caoxiaoqing/CaoXiaoqing.github.io',
  oauth: {
    client_id: 'f6c98a7a3fd7c7cf3b6b',
    client_secret: '3ffd1d22c828ecafb8ae282bffa8fe1b3af0ada8',
  },
})
gitment.render('container')
</script>




<div id="disqus_thread"></div>
<script>
    /**
     * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
     * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
     */

    var disqus_config = function() {
        this.page.url = 'https://caoxiaoqing.github.io/2018/05/11/SVRG%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/'; // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'https://caoxiaoqing.github.io/2018/05/11/SVRG%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };

    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document,
            s = d.createElement('script');

        s.src = '//caoxiaoqing.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>




    </div>
    <button class="anchor"><i class="fa fa-anchor"></i></button>
    <div class="right">
        <div class="wrap">

            <!-- Content -->
            <div class="side content">
                <div>
                    Content
                </div>
                <ul id="content-side" class="content-ul">
                    
                    <li><a href="#similar_posts">Similar Posts</a></li>
                    
                    <li><a href="#comments">Comments</a></li>
                </ul>
            </div>
            <!-- 其他div框放到这里 -->
            <!-- <div class="side">bbbb</div> -->
        </div>
    </div>
</div>
<script>
/**
 * target _blank
 */
(function() {
    var aTags = document.querySelectorAll('article a:not([id])')
    for (var i = 0; i < aTags.length; i++) {
        aTags[i].setAttribute('target', '_blank')
    }
}());
</script>
<script src="/js/pageContent.js " charset="utf-8"></script>


    <footer class="site-footer">


    <div class="wrapper">

        <p class="description">
             曹孝卿的博客 
        </p>
        <p class="contact">
            Contact me at: 
            <a href="https://github.com/caoxiaoqing" title="GitHub"><i class="fa fa-github" aria-hidden="true"></i></a>  
            <a href="mailto:CaoXiaoqing2008@gmail.com" title="email"><i class="fa fa-envelope-o" aria-hidden="true"></i></a>        
        </p>
        <p>
            本站总访问量<span id="busuanzi_value_site_pv"></span>次，本站访客数<span id="busuanzi_value_site_uv"></span>人次，本文总阅读量<span id="busuanzi_value_page_pv"></span>次
        </p>
        <p class="power">
            <span>
                Site powered by <a href="https://jekyllrb.com/">Jekyll</a> & <a href="https://pages.github.com/">Github Pages</a>.
            </span>
            <span>
                Theme designed by <a href="https://github.com/Gaohaoyang">HyG</a>.
            </span>
        </p>
    </div>
</footer>
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <div class="back-to-top">
    <a href="#top" data-scroll>
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
    </a>
</div>

    <script src=" /js/main.js " charset="utf-8"></script>
    <script src=" /js/smooth-scroll.min.js " charset="utf-8"></script>
    <script type="text/javascript">
      smoothScroll.init({
        speed: 500, // Integer. How fast to complete the scroll in milliseconds
        easing: 'easeInOutCubic', // Easing pattern to use
        offset: 20, // Integer. How far to offset the scrolling anchor location in pixels
      });
    </script>
    <!-- <script src=" /js/scroll.min.js " charset="utf-8"></script> -->
  </body>

</html>
