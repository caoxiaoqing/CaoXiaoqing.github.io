<!DOCTYPE html>
<html>

  <head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>SVRG算法的阅读理解和实践</title>
	<meta name="description" content="SVRG算法的阅读理解和实践">
	
	<link rel="canonical" href="/2018/05/10/SVRG%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">
	<link rel="alternate" type="application/rss+xml" title="CaoXiaoqing's Blog" href="/feed.xml" />
	
	<!-- <link rel="stylesheet" href="/css/main.css"> -->

	<link rel="stylesheet" type="text/css" href="http://apps.bdimg.com/libs/bootstrap/3.3.0/css/bootstrap.min.css">
	<!-- <link rel="stylesheet" type="text/css" href="http://apps.bdimg.com/libs/fontawesome/4.2.0/css/font-awesome.min.css"> -->
	<!-- <link rel="stylesheet" type="text/css" href="/static/css/bootstrap.min.css"> -->
	

	<link rel="stylesheet" type="text/css" href="/static/css/index.css">
	
	<!-- <script type="text/javascript" src="/static/js/jquery-1.11.1.min.js"></script>
	<script type="text/javascript" src="/static/js/bootstrap.min.js"></script> -->

	<script type="text/javascript" src="http://apps.bdimg.com/libs/jquery/2.1.1/jquery.min.js"></script>
	<script type="text/javascript" src="http://apps.bdimg.com/libs/bootstrap/3.3.0/js/bootstrap.min.js"></script>

	<script type="text/javascript" src="/static/js/index.js"></script>
	
	<link rel="stylesheet" type="text/css" href="http://apps.bdimg.com/libs/highlight.js/8.4/styles/monokai_sublime.min.css">
	<!-- <link rel="stylesheet" type="text/css" href="http://apps.bdimg.com/libs/highlight.js/8.4/styles/railscasts.min.css"> -->
	<!-- <link rel="stylesheet" type="text/css" href="http://apps.bdimg.com/libs/highlight.js/8.4/styles/monokai.min.css"> -->
	<!-- <script type="text/javascript" src="http://apps.bdimg.com/libs/highlight.js/8.4/languages/dos.min.js"></script> -->
	<script type="text/javascript" src="http://apps.bdimg.com/libs/highlight.js/8.4/highlight.min.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
	
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

	<!--script>
	var _hmt = _hmt || [];
	(function() {
	  var hm = document.createElement("script");
	  hm.src = "//hm.baidu.com/hm.js?b636473d6ffa17615f94e5db1459ea81";
	  var s = document.getElementsByTagName("script")[0]; 
	  s.parentNode.insertBefore(hm, s);
	})();
	</script-->

</head>

 <!--  <body data-spy="scroll" data-target="#myAffix"> -->
  <body>

    <header>

<!-- navbar -->
  <nav class="navbar navbar-inverse">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">CaoXiaoqing's Blog</a>
      <p class="navbar-text">Free Your Mind</p>
    </div>
    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav navbar-right">

        
          <li>
        
          <a href="/"><span class="glyphicon glyphicon-th-large"></span> Home</a></li>

        
          
        
          
        
      </ul>
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

</header>


    <div id="main" class="container main">
      <div class="row">
  <div id="myArticle" class="col-sm-9">
    <div class="post-area post">
      <header>
        <h1>SVRG算法的阅读理解和实践</h1>
        <p>May 10, 2018 • CaoXiaoqing</p>
      </header>
      <hr>
      <article>
        <p>最佳拜读了下大名鼎鼎的 SVRG 算法 <a href="#Reference">[5]</a>，本文做个简单的阅读理解，并描述了如何将 SVRG 算法改造成一种在线算法，当然我这里不保证改造后算法的各种理论性质。</p>

<ul id="markdown-toc">
  <li><a href="#section" id="markdown-toc-section">1. 背景介绍</a></li>
  <li><a href="#noise-reduction-methods" id="markdown-toc-noise-reduction-methods">2. 方差缩减方法 Noise Reduction Methods</a></li>
  <li><a href="#svrg-" id="markdown-toc-svrg-">3. SVRG 算法</a></li>
  <li><a href="#svrg--1" id="markdown-toc-svrg--1">4. SVRG 算法的进化</a></li>
  <li><a href="#svrg-for-online-learning" id="markdown-toc-svrg-for-online-learning">5. SVRG for online learning</a></li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ul>

<h2 id="section">1. 背景介绍</h2>

<p>考虑优化问题：</p>

<script type="math/tex; mode=display">min \, F(w)=\frac{1}{n}\sum_{i=1}^{n}f_i(w)</script>

<p>当我们采用 Gradient Descent (GD) 方法时，<script type="math/tex">w</script> 的更新公式是：</p>

<script type="math/tex; mode=display">w^{(t)}=w^{(t-1)}-\eta_t\nabla F(w^{(t-1)})=w^{(t-1)}-\frac{\eta_t}{n}\sum_{i=1}^{n}\nabla f_{i}(w^{(t-1)})</script>

<p>梯度下降方法可以追溯到 Cauchy 1847 年的论文 <a href="#Reference">[1]</a>。梯度下降对于样本数目比较多的时候有一个很大的劣势，那就是每次需要求解所有样本的梯度，导致计算量大增，所以实际生产环境中，往往采用随机梯度下降算法（Stochastic Gradient Descent），一般简写做 SGD，它于 1951 和 1952 年在文献 <a href="#Reference">[2,3]</a> 中被提出。SGD 每次迭代的时候均匀随机得选择一个样本或者 mini-batch 做更新。当我们采用 SGD 方法来进行计算的时候，<script type="math/tex">w</script> 的更新公式是</p>

<script type="math/tex; mode=display">w^{(t)}=w^{(t-1)}-\eta_t\nabla f_{i_t}(w^{(t-1)})</script>

<p>相对于梯度下降，SGD 的好处非常明显，就是可以减少每次更新的计算代价，不过也正是因为每次都是随机的使用一个样本或一个 mini batch 来估计梯度，因此<strong>对梯度估计的方差就大了</strong>。这给 SGD 带来的问题是收敛速度不如梯度下降，也就是说为了达到同样的精度，SGD 需要的总迭代次数要大于梯度下降。从收敛速度分析上看，梯度下降则可以在目标函数强凸的情况下做到 <script type="math/tex">% <![CDATA[
O(\rho^T) (\rho<1) %]]></script> 的线性收敛（linear convergence），在目标函数为凸函数的情况下可以做到次线性收敛 <script type="math/tex">O(1/T)</script>（收敛速度是衡量优化算法计算复杂度的基本工具，可以参考 <a href="https://en.wikipedia.org/wiki/Rate_of_convergence" title="Rate_of_convergence">wiki</a> 或者 <a href="https://zhuanlan.zhihu.com/p/27644403">这里</a>）。而 SGD 能够在目标函数强凸并且递减步长的情况下只做到 <script type="math/tex">O(1/T)</script> 的次线性收敛（sublinear convergence）。至于为什么对梯度估计的方差过大会降低收敛速度，以及 SGD 为什么一定要步长递减，具体原因可参考文章 <a href="#Reference">[4]</a> 的 Theorem 4.6 和 Theorem 4.7 以及 Theorem 4.8，Theorem 4.9 和 Theorem 4.10，简单来说就是如果步长不是递减的，SGD 会收敛到最优解的一个领域中，对梯度估计的噪声让它最终无法进一步收敛。</p>

<p>总结起来就是，如果想快速得到一个可以勉强接受的解，SGD 比梯度下降更加合适，但是如果想得到一个精确度高的解，应当选择梯度下降。SGD 后来衍生出了非常多的变种，尤其是一类分析 regret 的 online 算法，包括 Adagrad、Dual Averaging、FTRL 等。但是是否可以把 SGD 做到和梯度下降一样的线性收敛呢？直到 2012 和 2013 年，SAG 与 SVRG 算法发表在 NIPS 上回答了这个问题，<strong>成为近几年 SGD 类算法的最大突破</strong>。</p>

<h2 id="noise-reduction-methods">2. 方差缩减方法 Noise Reduction Methods</h2>

<p>既然影响 SGD 收敛速度的主要原因之一是在所计算的梯度的方差，那就想办法降低这个方差，这一类方法就被称为方差缩减方法（Noise Reduction Methods）。在所谓的方差缩减方法中，又可以分为 3 小类，第一类动态采样方法（dynamic sampling methods）是通过在计算梯度时逐步增加样本量来减少梯度估计的方差；第二类迭代平均方法（iterate averaging methods）则是通过对得到的 <script type="math/tex">w</script> 进行历史平均来减少其方差；第三类梯度聚合方法（gradient aggregation methods）则是通过存储历史梯度，在每次估计梯度时用历史梯度来做修正，我们主要要谈的 SVRG 算法就属于这一类。</p>

<h2 id="svrg-">3. SVRG 算法</h2>

<p>在 SVRG 算法之前，有两个第三类的方差缩减方法分别称为 SAG 和 SDCA <a href="#Reference">[7]</a> ，但是这两种方法都要求存储所有的梯度，在实际的大规模工业应用中并不实用。SVRG 算法则不用存储所有的梯度，并且在目标函数光滑和强凸的情况下做到线性收敛速度。</p>

<p><strong>SVRG 算法</strong>（这里的 <script type="math/tex">\psi</script> 就是上文中的 <script type="math/tex">f</script>）</p>

<p><img src="../css/pics/2018-05-07-SVRG-SVRG.jpg" alt="svrg算法" /></p>

<p>SVRG 算法在每一轮迭代的内部有一个内部的迭代，在进行内部迭代前用当前的 <script type="math/tex">\tilde w</script> 值计算一次所有样本的平均梯度 <script type="math/tex">\tilde\mu</script>，内部迭代的初始值被赋予为当前的 <script type="math/tex">\tilde w</script>，内部迭代中每次的梯度采用如下方式计算：</p>

<script type="math/tex; mode=display">g \, \leftarrow \, \nabla \psi_{i_t}(w_{t-1})-\nabla\psi_{i_t}(\tilde w)+\tilde \mu</script>

<p>对此公式为什么能降低梯度估计的方差有个直观的解释，因为 <script type="math/tex">\nabla\psi_{i_t}(\tilde w)</script> 的期望就是 <script type="math/tex">\tilde\mu</script>，因此可以将 <script type="math/tex">-\nabla\psi_{i_t}(\tilde w)+\tilde \mu</script> 视为梯度估计 <script type="math/tex">\nabla\psi_{i_t}(\tilde w)</script> 的 bias，那么在每一次的迭代中，算法都对基于当前参数 <script type="math/tex">w_{t-1}</script> 做的梯度估计 <script type="math/tex">\nabla \psi_{i_t}(w_{t-1})</script> 进行了一次修正。在 SGD 的收敛性分析中，假定了样本梯度的方差是有个常数上界的 (见文章 <a href="#Reference">[4]</a> 的 Assumption 4.3(c))，正是这个常数上界的存在导致 SGD 算法无法线性收敛，SVRG 利用它新的更新方式可以让估计的梯度方差有个不断减小的上界 (见文章 <a href="#Reference">[4]</a> 的 Theorem 5.1)，这就是 SVRG 算法的核心思想，这也是为什么这个算法被称为 SVRG（stochastic variance reduced gradient）的原因，更多更为正式更为数学的分析可参考 bottou 大神写的综述文章 <a href="#Reference">[4]</a>。</p>

<h2 id="svrg--1">4. SVRG 算法的进化</h2>

<p><strong>mini-batch SVRG 算法</strong> <a href="#Reference">[9]</a></p>

<p>看作者名字就知道这是一位日本同学的文章，是 mini-batch 版本的 SVRG，并同时采用 Nesterov 加速。</p>

<p><strong>SAGA算法</strong> <a href="#Reference">[6]</a></p>

<p><strong>SCSG 算法</strong> <a href="#Reference">[8]</a>。</p>

<h2 id="svrg-for-online-learning">5. SVRG for online learning</h2>

<h2 id="reference">Reference</h2>

<p>[1] Cauchy, Augustin. “Méthode générale pour la résolution des systemes d’équations simultanées.” Comp. Rend. Sci. Paris 25.1847 (1847): 536-538.</p>

<p>[2] Robbins, Herbert, and Sutton Monro. “A stochastic approximation method.” The annals of mathematical statistics (1951): 400-407.</p>

<p>[3] Kiefer, Jack, and Jacob Wolfowitz. “Stochastic estimation of the maximum of a regression function.” The Annals of Mathematical Statistics 23.3 (1952): 462-466.</p>

<p>[4] Bottou, Léon, Frank E. Curtis, and Jorge Nocedal. “Optimization methods for large-scale machine learning.” <em>SIAM Review</em> 60.2 (2018): 223-311.</p>

<p>[5] Johnson, Rie, and Tong Zhang. “Accelerating stochastic gradient descent using predictive variance reduction.” <em>Advances in Neural Information Processing Systems</em>. 2013.</p>

<p>[6] Defazio, Aaron, Francis Bach, and Simon Lacoste-Julien. “Saga: A fast incremental gradient method with support for non-strongly convex composite objectives.” <em>Advances in Neural Information Processing Systems</em>. 2014.</p>

<p>[7] Shalev-Shwartz, Shai, and Tong Zhang. “Accelerated mini-batch stochastic dual coordinate ascent.” <em>Advances in Neural Information Processing Systems</em>. 2013.</p>

<p>[8] Lei, Lihua, and Michael Jordan. “Less than a Single Pass: Stochastically Controlled Stochastic Gradient.” <em>Artificial Intelligence and Statistics</em>. 2017.</p>

<p>[9] Nitanda, Atsushi. “Stochastic proximal gradient descent with acceleration techniques.” Advances in Neural Information Processing Systems. 2014.</p>

      </article>
      <hr>
      <div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_copy" data-cmd="copy" title="分享到复制网址"></a></div>
      <script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>
    </div>
    <!--div class="post-area post comment"-->
      <!-- 多说评论框 start -->
      
      <!--div class="ds-thread" data-thread-key="/2018/05/10/SVRG论文阅读笔记" data-title="SVRG算法的阅读理解和实践" data-url="caoxiaoqing.github.io/2018/05/10/SVRG%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"></div-->
      
      <!-- 多说评论框 end -->
    <!--/div-->
  </div>

  <div id="content" class="col-sm-3">
    <!-- <div id="myAffix" class="shadow-bottom-center hidden-xs" data-spy="affix" data-offset-top="0" data-offset-bottom="-20"> -->
    <div id="myAffix" class="shadow-bottom-center hidden-xs" >
      <div class="categories-list-header">
        Content
      </div>
      <div class="content-text"></div>
    </div>
  </div>
</div>
    </div>

    
    <div id="top" data-toggle="tooltip" data-placement="left" title="回到顶部">
      <a href="javascript:;">
        <div class="arrow"></div>
        <div class="stick"></div>
      </a>
    </div>

    <footer class="">
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <a href="mailto:CaoXiaoqing2008@gmail/163.com"><span class="glyphicon glyphicon-envelope"></span> CaoXiaoqing2008@gmail/163.com</a>
        <span class="point"> · </span>
        
          
          <a href="https://github.com/caoxiaoqing">
            <span class="icon">
              <svg viewBox="0 0 16 16">
                <path fill="#aaa" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            GitHub
            <!-- <span>caoxiaoqing</span> -->
          </a>
          
          
          <span class="point"> · </span>
          <!--span><a href="/feed.xml">RSS</a></span-->
          <!--span class="point"> · </span-->
          <span>曹孝卿的博客 生活爱好者</span>
          <span class="point"> · </span>
          <span>&copy; 2018 CaoXiaoqing</span>

      </div>
    </div>
  </div>
</footer>


    <!-- 多说公共JS代码 start (一个网页只需插入一次) -->
  <!--script type="text/javascript">
    var duoshuoQuery = {short_name:"cxqblog"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script-->
<!-- 多说公共JS代码 end -->

<!-- 在新窗口中打开 -->
  <script type="text/javascript">
    function addBlankTargetForLinks () {
      $('a[href^="http"]').each(function(){
          $(this).attr('target', '_blank');
      });
    }
    $(document).bind('DOMNodeInserted', function(event) {
      addBlankTargetForLinks();
    });
  </script>
  </body>
</html>
