<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CaoXiaoqing&#39;s Blog</title>
    <description>曹孝卿的博客 生活爱好者</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 14 May 2018 15:18:15 +0800</pubDate>
    <lastBuildDate>Mon, 14 May 2018 15:18:15 +0800</lastBuildDate>
    <generator>Jekyll v3.1.2</generator>
    
      <item>
        <title>SVRG算法的阅读理解和实践</title>
        <description>&lt;p&gt;最佳拜读了下大名鼎鼎的 SVRG 算法 &lt;a href=&quot;#Reference&quot;&gt;[5]&lt;/a&gt;，读完后把前前后后涉及到的方法都看了一遍，这里做个简单的综述和阅读理解，并描述了如何将方差缩减方法改造成一种在线算法。&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;1. 背景介绍&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#noise-reduction-methods&quot; id=&quot;markdown-toc-noise-reduction-methods&quot;&gt;2. 方差缩减方法 Noise Reduction Methods&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#sag--10reference&quot; id=&quot;markdown-toc-sag--10reference&quot;&gt;3. SAG 算法 &lt;a href=&quot;#Reference&quot;&gt;[10]&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#svrg--5reference&quot; id=&quot;markdown-toc-svrg--5reference&quot;&gt;4. SVRG 算法 &lt;a href=&quot;#Reference&quot;&gt;[5]&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#saga--6reference&quot; id=&quot;markdown-toc-saga--6reference&quot;&gt;5. SAGA 算法 &lt;a href=&quot;#Reference&quot;&gt;[6]&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#scsg--8reference&quot; id=&quot;markdown-toc-scsg--8reference&quot;&gt;6. SCSG 算法 &lt;a href=&quot;#Reference&quot;&gt;[8]&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;7. 其它方法&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-2&quot; id=&quot;markdown-toc-section-2&quot;&gt;8. 应用于在线学习&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#reference&quot; id=&quot;markdown-toc-reference&quot;&gt;Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section&quot;&gt;1. 背景介绍&lt;/h2&gt;

&lt;p&gt;考虑优化问题：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;min \, R_n(w)=\frac{1}{n}\sum_{i=1}^{n}f_i(w)&lt;/script&gt;

&lt;p&gt;当我们采用 Gradient Descent (GD) 方法时，&lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; 的更新公式是：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_{k+1}=w_{k}-\eta_k\nabla R_n(w_{k})=w_{k}-\frac{\eta_k}{n}\sum_{i=1}^{n}\nabla f_{i}(w_{k})&lt;/script&gt;

&lt;p&gt;梯度下降方法可以追溯到 Cauchy 1847 年的论文 &lt;a href=&quot;#Reference&quot;&gt;[1]&lt;/a&gt;。梯度下降对于样本数目比较多的时候有一个很大的劣势，那就是每次需要求解所有样本的梯度，导致计算量大增，所以实际生产环境中，往往采用随机梯度下降算法（Stochastic Gradient Descent），一般简写做 SGD，它于 1951 和 1952 年在文献 &lt;a href=&quot;#Reference&quot;&gt;[2,3]&lt;/a&gt; 中被提出。SGD 每次迭代的时候均匀随机得选择一个样本或者 mini-batch 做更新。当我们采用 SGD 方法来进行计算的时候，&lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; 的更新公式是&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_{k+1}=w_{k}-\eta_k\nabla f_{i_k}(w_{k})&lt;/script&gt;

&lt;p&gt;相对于梯度下降，SGD 的好处非常明显，就是可以减少每次更新的计算代价，不过也正是因为每次都是随机的使用一个样本或一个 mini batch 来估计梯度，因此&lt;strong&gt;对梯度估计的方差就大了&lt;/strong&gt;。这给 SGD 带来的问题是收敛速度不如梯度下降，从收敛速度分析上看，梯度下降则可以在目标函数强凸的情况下做到 &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
O(\rho^T) (\rho&lt;1) %]]&gt;&lt;/script&gt; 的线性收敛（linear convergence），在目标函数为凸函数的情况下可以做到次线性收敛 &lt;script type=&quot;math/tex&quot;&gt;O(1/T)&lt;/script&gt;（收敛速度是衡量优化算法计算复杂度的基本工具，可以参考 &lt;a href=&quot;https://en.wikipedia.org/wiki/Rate_of_convergence&quot; title=&quot;Rate_of_convergence&quot;&gt;wiki&lt;/a&gt; 或者 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/27644403&quot;&gt;这里&lt;/a&gt;）。而 SGD 能够在目标函数强凸并且递减步长的情况下只做到 &lt;script type=&quot;math/tex&quot;&gt;O(1/T)&lt;/script&gt; 的次线性收敛（sublinear convergence）。也就是说，如果想快速得到一个可以勉强接受的解，SGD 比梯度下降更加合适，但是如果想得到一个精确度高的解，应当选择梯度下降，因为为了达到同样的精度，SGD 需要的总迭代次数要大于梯度下降。&lt;/p&gt;

&lt;p&gt;至于为什么对梯度估计的方差过大会降低收敛速度，以及 SGD 为什么一定要步长递减，具体原因可参考文章 &lt;a href=&quot;#Reference&quot;&gt;[4]&lt;/a&gt; 的 Theorem 4.6 和 Theorem 4.7 以及 Theorem 4.8，Theorem 4.9 和 Theorem 4.10，简单来说就是如果步长不是递减的，SGD 会收敛到最优解的一个领域中，对梯度估计的噪声让它最终无法进一步收敛。既然影响 SGD 收敛速度的主要原因之一是在所计算的梯度的方差，那就想办法降低这个方差，然后自然就可以提升算法的收敛速度了，这一类方法就被称为方差缩减方法（Noise Reduction Methods）。&lt;/p&gt;

&lt;h2 id=&quot;noise-reduction-methods&quot;&gt;2. 方差缩减方法 Noise Reduction Methods&lt;/h2&gt;

&lt;p&gt;在所谓的方差缩减方法中，又可以分为 3 小类，第一类动态采样方法（dynamic sampling methods）是通过在计算梯度时逐步增加样本量来减少梯度估计的方差；第二类迭代平均方法（iterate averaging methods）则是通过对得到的 &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; 进行历史平均来减少其方差；第三类梯度聚合方法（gradient aggregation methods）则是通过存储历史梯度，在每次估计梯度时用历史梯度来做修正，前两类方法太过暴力而不优雅，因此我们作为优雅的人主要讨论第三类方法。&lt;/p&gt;

&lt;p&gt;我们先来直观的感受下什么叫方差缩减，比如你要通过 monte carlo 采样的方法来估计随机变量 &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; 的期望 &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}X&lt;/script&gt;，同时假设你已经能够比较容易地估计与随机变量 &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; 强相关的随机变量 &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; 的期望 &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}Y&lt;/script&gt;，一个利用方差缩减思想来近似估计 &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}X&lt;/script&gt; 的估计量 (estimator) 是：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{\alpha}=\alpha(X-Y)+\mathbb{E}Y&lt;/script&gt;

&lt;p&gt;我们可以看看这个估计量的期望和方差：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}\theta_{\alpha}=\alpha\mathbb{E}X+(1-\alpha)\mathbb{E}Y \\
Var(\theta_{\alpha})=\alpha^2[Var(X)+Var(Y)-2Cov(X,Y)]&lt;/script&gt;

&lt;p&gt;可以发现，当 &lt;script type=&quot;math/tex&quot;&gt;\alpha=1&lt;/script&gt; 时，&lt;script type=&quot;math/tex&quot;&gt;\theta_{\alpha}&lt;/script&gt; 是 &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}X&lt;/script&gt; 的无偏估计，且当 &lt;script type=&quot;math/tex&quot;&gt;Cov(X,Y)&lt;/script&gt; 足够大的时候，&lt;script type=&quot;math/tex&quot;&gt;\theta_{\alpha}&lt;/script&gt; 的方差也比直接对 &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; 做估计要来的小，这就是方差缩减的基本思想。后面要提到的各种方差缩减类算法比如大名鼎鼎的 SVRG 等都是这个&lt;strong&gt;套路，套路，套路&lt;/strong&gt;。&lt;/p&gt;

&lt;h2 id=&quot;sag--10reference&quot;&gt;3. SAG 算法 &lt;a href=&quot;#Reference&quot;&gt;[10]&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;SAG 算法想啊，既然 GD 用上所有数据的梯度就能做到线性收敛，SGD 一次只能用一个样本或者一小批样本来计算梯度，因此对梯度估计的方差影响了最终的收敛速度，那我们能不能在 SGD 上也能用上所有样本的梯度呢？于是就有了 SAG 算法的更新方式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
w_{k+1}=w_k-\frac{\eta_k}{n}\sum_{i=1}^{n}y_i^{k} \\
y_i^{k}=\left\{
\begin{aligned}
&amp;\nabla f_i(w_k), \, &amp;\text{if}\,i_k=i\\
&amp;\nabla f_i(w_{k-1}) \, &amp;\text{otherwise}
\end{aligned}
\right. %]]&gt;&lt;/script&gt;

&lt;p&gt;可以把上面的梯度计算方式写成一个式子&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tilde g_k \, \leftarrow \, \frac{\nabla f_{i_k}(w_k) - \nabla f_{i_k}(w_{k-1})}{n} + \nabla R_n(w_{k-1})&lt;/script&gt;

&lt;p&gt;这样就容易看出这个方法就是上面 &lt;script type=&quot;math/tex&quot;&gt;\alpha=1/n&lt;/script&gt; 时的一个应用，虽然这种估计不是无偏估计，但是方差会以 &lt;script type=&quot;math/tex&quot;&gt;1/n^2&lt;/script&gt; 的比例缩小。这个方法理论上可以获得于 GD 方法同样的收敛速度。另外，在实践中，需要在内存里保存所有样本的上一次梯度值。&lt;/p&gt;

&lt;h2 id=&quot;svrg--5reference&quot;&gt;4. SVRG 算法 &lt;a href=&quot;#Reference&quot;&gt;[5]&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;因为 SAG 算法需要保存所有样本的梯度值，在实际的大规模工业应用中并不实用，因此就有了 SVRG 算法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SVRG 算法的迭代过程&lt;/strong&gt; (图片来自文章 &lt;a href=&quot;#Reference&quot;&gt;[4]&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/css/pics/2018-05-07-SVRG-SVRG.png&quot; alt=&quot;svrg算法&quot; /&gt;&lt;/p&gt;

&lt;p&gt;SVRG 算法在每一轮迭代的内部有一个内部的迭代，在进行内部迭代前用当前的 &lt;script type=&quot;math/tex&quot;&gt;w_k&lt;/script&gt; 值计算一次所有样本的平均梯度 &lt;script type=&quot;math/tex&quot;&gt;\nabla R_n(w_k)&lt;/script&gt;，内部迭代的初始值被赋予为当前的 &lt;script type=&quot;math/tex&quot;&gt;w_k&lt;/script&gt;，内部迭代中每次的梯度采用如下方式计算：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tilde g_j \, \leftarrow \, \nabla f_{i_j}(\tilde w_j)-(\nabla f_{i_j}(w_k)-\nabla R_n(w_k))&lt;/script&gt;

&lt;p&gt;按照上文对方差缩减方法的描述，对此公式为什么能降低梯度估计的方差就可以有个直观的解释：因为 &lt;script type=&quot;math/tex&quot;&gt;\nabla f_{i_j}(w_k)&lt;/script&gt; 的期望就是 &lt;script type=&quot;math/tex&quot;&gt;\nabla R_n(w_k)&lt;/script&gt;，因此可以将 &lt;script type=&quot;math/tex&quot;&gt;(\nabla f_{i_j}(w_k)-\nabla R_n(w_k))&lt;/script&gt; 视为梯度估计 &lt;script type=&quot;math/tex&quot;&gt;\nabla f_{i_j}(w_k)&lt;/script&gt; 的 bias，那么在每一次的迭代中，算法都对基于当前参数 &lt;script type=&quot;math/tex&quot;&gt;\tilde w_j&lt;/script&gt; 做的梯度估计 &lt;script type=&quot;math/tex&quot;&gt;\nabla f_{i_j}(\tilde w_j)&lt;/script&gt; 进行了一次修正。在 SGD 的收敛性分析中，假定了样本梯度的方差是有个常数上界的 (见文章 &lt;a href=&quot;#Reference&quot;&gt;[4]&lt;/a&gt; 的 Assumption 4.3(c))，正是这个常数上界的存在导致 SGD 算法无法线性收敛，SVRG 利用它新的更新方式可以让估计的梯度方差有个不断减小的上界 (见文章 &lt;a href=&quot;#Reference&quot;&gt;[4]&lt;/a&gt; 的 Theorem 5.1)，这就是 SVRG 算法的核心思想，这也是为什么这个算法被称为 SVRG（stochastic variance reduced gradient）的原因，SVRG 算法在目标函数光滑和强凸的情况下做到线性收敛速度。更多更为正式更为数学的分析可参考 bottou 大神写的综述文章 &lt;a href=&quot;#Reference&quot;&gt;[4]&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;SVRG 算法的问题是虽然不用存储所有样本的梯度了，但是计算量上去了，因为它在每次的大迭代里面还有一轮小迭代，每次都要算两遍梯度，整体的计算量已经和 GD 一样了，而且还多一个超参数 &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; 要调。&lt;/p&gt;

&lt;h2 id=&quot;saga--6reference&quot;&gt;5. SAGA 算法 &lt;a href=&quot;#Reference&quot;&gt;[6]&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;SAGA 算法其实是介于 SAG 和 SVRG 之间的一种算法，但作者声称在强凸的条件下其收敛速度要快于 SAG 和 SVRG，且两倍于 SDCA，并同时适用于非强凸的情形。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SAGA 算法的迭代过程&lt;/strong&gt; (图片来自文章 &lt;a href=&quot;#Reference&quot;&gt;[4]&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/css/pics/2018-05-07-SVRG-SAGA.png&quot; alt=&quot;saga算法&quot; /&gt;&lt;/p&gt;

&lt;p&gt;同样，如果我们仔细审视它的梯度计算过程：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tilde g_k \, \leftarrow \, \nabla f_{i_k}(w_k) - \nabla f_{i_k}(w_{k-1}) + \nabla R_n(w_{k-1})&lt;/script&gt;

&lt;p&gt;会发现，相比于 SAG 算法，SAGA 算法只是把前面的一个 &lt;script type=&quot;math/tex&quot;&gt;1/n&lt;/script&gt; 去掉了，使得对梯度的估计变成无偏的了，当然同时它的方差相比于 SAG 也大了 &lt;script type=&quot;math/tex&quot;&gt;n^2&lt;/script&gt;。&lt;/p&gt;

&lt;h2 id=&quot;scsg--8reference&quot;&gt;6. SCSG 算法 &lt;a href=&quot;#Reference&quot;&gt;[8]&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;SVRG 的主要特征就是利用全部数据的梯度来对 SGD 的方差进行控制。因此 SVRG 的计算成本（Computation Cost）是 &lt;script type=&quot;math/tex&quot;&gt;O((n+m)T)&lt;/script&gt;。这里 &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; 是数据的总数，&lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; 是 Step-size，而 &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; 是论数。SVRG 的通讯成本也是这么多，这里面的主要成本在于每一轮都需要对全局数据进行访问。&lt;/p&gt;

&lt;p&gt;Stochastically Controlled Stochastic Gradient（SCSG）算法就是对 SVRG 进行了两个改进：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;每一轮并不用全局的数据进行梯度的计算，而是从一个全局的子集 Batch 中估计梯度，子集的大小是 B。&lt;/li&gt;
  &lt;li&gt;每一轮 SGD 的更新数目 &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; 也不是一个定值，而是一个和之前那个子集大小有关系，基于 Geometric Distribution 的随机数。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;剩下的更新步骤和 SVRG 一模一样。&lt;/p&gt;

&lt;p&gt;然而，这样的改变之后，新算法的计算成本成为了 &lt;script type=&quot;math/tex&quot;&gt;O((B+N)T)&lt;/script&gt;。也就是说，这是一个不依赖全局数据量大小的数值。而通过分析，作者们也比较了 SCSG 的通讯成本和一些原本就为了通讯成本而设计的算法，在很多情况下，SCSG 的通讯成本更优。通过 MNIST 数据集的实验发现，SCSG 达到相同的准确度，需要比 SVRG 更少的轮数，和每一轮更少的数据。可以说，这个算法可能会成为 SVRG 的简单替代。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;7. 其它方法&lt;/h2&gt;

&lt;p&gt;S2GD(Semi-Stochastic Gradient Descent Methods)&lt;/p&gt;

&lt;p&gt;Mini-Batch Semi-Stochastic Gradient Descent in the Proximal Setting. 将S2GD扩展到mini-batch上，因此允许并行运行，但是需要更多的同步，只能允许小的batch&lt;/p&gt;

&lt;p&gt;SDCA(Stochastic dual coordinate ascent methods for regularized loss) &lt;a href=&quot;#Reference&quot;&gt;[7]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Finito(Finito: A faster, permutable incremental gradientmethod for big data problems)&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;8. 应用于在线学习&lt;/h2&gt;

&lt;p&gt;以上算法都只是适用于有限数据集的，我所关注的在线学习面临的是无限数据集，因此上述方法都不适用。但是有了上面的套路，我们也可以轻松地设计出适用在线学习的方差缩减方法，比如下面这个算法：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/css/pics/2018-05-07-SVRG-online-v1.png&quot; alt=&quot;online SVRG算法&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对上面 online SVRG v1 算法做几点说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;g_{old}&lt;/script&gt; 来自上一次的梯度，这里可以不要求是同一个 mini batch 的数据，其中的 &lt;script type=&quot;math/tex&quot;&gt;w_k&lt;/script&gt; 也可以是被其他 worker 更新后的值，因此，这个算法可以适用于多 worker 异步更新的场合，因为我们可以假设在很短的一个时间内，所有 worker 拿到的数据都是独立同分布的。&lt;/li&gt;
  &lt;li&gt;为了获得上文第二部分所说的 &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}Y&lt;/script&gt;，也就是上文其他算法中的 &lt;script type=&quot;math/tex&quot;&gt;\nabla R_n(w_k)&lt;/script&gt;，我们使用了一个滑动加权平均的方式，其中参数 &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; 是为了控制历史值与当前值的权重分配。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为什么我们还要指定学习率 &lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt; 和衰减系数 &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; 呢？已经有很多算法可以免去这两个超参数了，我们可以把他们的思想拿过来，和上面的算法进行综合，得到一个真正免超参，同时又能降低梯度估计方差，提升学习率的算法，这就是我们后续提出的 online SVRG v2 算法，关于这个算法我们下一篇再写，这里写不下了。&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;p&gt;[1] Cauchy, Augustin. “Méthode générale pour la résolution des systemes d’équations simultanées.” Comp. Rend. Sci. Paris 25.1847 (1847): 536-538.&lt;/p&gt;

&lt;p&gt;[2] Robbins, Herbert, and Sutton Monro. “A stochastic approximation method.” The annals of mathematical statistics (1951): 400-407.&lt;/p&gt;

&lt;p&gt;[3] Kiefer, Jack, and Jacob Wolfowitz. “Stochastic estimation of the maximum of a regression function.” The Annals of Mathematical Statistics 23.3 (1952): 462-466.&lt;/p&gt;

&lt;p&gt;[4] Bottou, Léon, Frank E. Curtis, and Jorge Nocedal. “Optimization methods for large-scale machine learning.” &lt;em&gt;SIAM Review&lt;/em&gt; 60.2 (2018): 223-311.&lt;/p&gt;

&lt;p&gt;[5] Johnson, Rie, and Tong Zhang. “Accelerating stochastic gradient descent using predictive variance reduction.” &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;. 2013.&lt;/p&gt;

&lt;p&gt;[6] Defazio, Aaron, Francis Bach, and Simon Lacoste-Julien. “Saga: A fast incremental gradient method with support for non-strongly convex composite objectives.” &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;. 2014.&lt;/p&gt;

&lt;p&gt;[7] Shalev-Shwartz, Shai, and Tong Zhang. “Accelerated mini-batch stochastic dual coordinate ascent.” &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;. 2013.&lt;/p&gt;

&lt;p&gt;[8] Lei, Lihua, and Michael Jordan. “Less than a Single Pass: Stochastically Controlled Stochastic Gradient.” &lt;em&gt;Artificial Intelligence and Statistics&lt;/em&gt;. 2017.&lt;/p&gt;

&lt;p&gt;[9] Nitanda, Atsushi. “Stochastic proximal gradient descent with acceleration techniques.” Advances in Neural Information Processing Systems. 2014.&lt;/p&gt;

&lt;p&gt;[10] Roux, Nicolas L., Mark Schmidt, and Francis R. Bach. “A stochastic gradient method with an exponential convergence _rate for finite training sets.” Advances in Neural Information Processing Systems. 2012.&lt;/p&gt;
</description>
        <pubDate>Fri, 11 May 2018 08:00:00 +0800</pubDate>
        <link>/2018/05/11/SVRG%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
        <guid isPermaLink="true">/2018/05/11/SVRG%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
        
        
        <category>算法</category>
        
      </item>
    
      <item>
        <title>意识诞生于上帝沉默时</title>
        <description>
&lt;p&gt;&lt;em&gt;本文来源于微信公众号‘机器之心’（almosthuman2014）&lt;/em&gt;机器之心&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;本文选自Nautilus，作者VERONIQUE GREENWOOD，机器之心子牙、孟婷 、Salmoner翻译，微胖校对。&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;译者按：加拿大科幻小说之父Robert J. Sawyer在他的关于网络觉醒故事的全新三部曲《WWW》之《Wake》中讨论了意识起源，其中， 故事主人公 Caitlin花了大量时间去思索Julian Jaynes的著作《The Origin of Consciousness in the Breakdown of the Bicameral Mind》。本文正好对这部神作做了一番介绍，著作主要研究人类意识的起源。该书作者Julian Jaynes认为，直到史诗描述的时代为止，人类思维与现代人的思维大相径庭，当时的人类，缺乏自知、自我意识。他认为，人类服从他们认定为神祗的声音，直到人类思维产生了自我意识为止。其中，《伊利亚特》提供了非常重要的证据：几乎所有行为，都是神灵推动，早期译文中人物也明显缺乏内省。与主流将意识视为生物现象（比如，行为主义）不同，Julian Jaynes将意识起源归于文化（特别是语言的兴起）。如今，他的「奇谈怪论」却成了神经科学家们、哲学家们的热议话题甚至是灵感来源。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;20世纪70年代初，朱利安·杰恩斯（Julian Jaynes）带着两个行李箱，开始了在普林斯顿的生活。在当时学生眼中，他绝对是个怪人,有些学生知道他是心理学讲师，低沉的男中音讲起课来滔滔不绝。五十出头的朱利安·杰恩斯嗜酒成性，没有终身教职，显然对终身教职也没什么兴趣。他属于学校边缘人物，那时还是普林斯顿的学生，现在已是佛罗里达州立大学的心理学教授Roy Baumeister回忆道，「学校没有定期付给他报酬」。但是，杰恩斯一直专注于自己的著作，多年来，坚持不懈。&lt;/p&gt;

&lt;p&gt;六岁时，杰恩斯就开始被意识体验的奇妙性深深吸引。盯着一朵黄色连翘花时，他会琢磨：如何肯定别人和他所看到的，是一样的黄色？在青年时期，因为反对支援战争，他在宾夕法尼亚监狱坐了三年牢。在服刑期间的一个春天，他观察到在监狱操场的草地上有一条蠕虫：是什么将没有思想的泥土与蠕虫分别开？又是什么让蠕虫与我有所区别？终其一生，他都在思考这类问题，他这这本著作也会激发整整一代人开始思索同样的问题。&lt;/p&gt;

&lt;p&gt;当《二分心智的崩塌：人类意识的起源（The Origin of Consciousness in the Breakdown of the Bicameral Mind）》终于在1976年出版时，看上去并不像是一本畅销书，然而，它的确十分好卖。书评见于各类科学杂志和心理学期刊、《时代周刊》、《纽约时报》和《洛杉矶时报》。1978年，该书获得国家图书奖的提名。不断再版发行过程中，杰恩斯也在各地进行巡回演讲。1997年，杰恩斯死于中风，而他的书留传于世。2000年，另一新版上市，销售至今。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://img.chuansong.me/mmbiz/KmXPKA19gW8GFicqX570olcSZKYmlD1tCd6wpQk2XR35Pwf8BXWEIkXlHxkXr230ic2Vapg0iaNfV7iaCu3JkxvvkQ/0?wx_fmt=jpeg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在书的开头，杰恩斯问：「意识是自我本身，无所不包，但又什么都不是。——它到底是什么？它来自哪里？它的意义何在？」杰恩斯从历史视角来回答这个问题：人类直到大约3000年前才具有完全的自我意识，在此之前，人类依赖二分心智（bicameral mind）——每当遭遇到困境，一个半脑会听见来自另一半脑的指引，这种指引被视为神的声音。人类社会日趋复杂，这种二分心智也最终坍塌，人类现代自我意识被唤醒，最终具有了内在叙事（ an internal narrative）的能力，杰恩斯认为，一切变化源于语言。&lt;/p&gt;

&lt;p&gt;这个非同寻常的观点也与当时主流想法格格不入。「古希腊人没有自我意识（ ancient Greeks were not self-aware）」的观点让不少人反感。艾伦脑科学研究所（ Allen Institute for Brain Science）的首席科学主任Christof Koch谈到：「通过把意识的来源归结于文化，杰恩斯否认了意识是一种生物现象。」&lt;/p&gt;

&lt;p&gt;但是，koch和其他神经科学学者和哲学家们都承认，这本狂放不羁的书有它独特的力量。哲学家Daniel Dennett如此评论道，「他是一位传统的、有着思想深度和雄心壮志的业余学者，始终追随着内心的好奇」。杰恩斯试图描述和解释『内在声音（ a inner voice）』——我们栖居于斯的内在世界，他的研究引发广泛共鸣。世界各地神经科学实验室间纷纷兴起意识研究，不过，当时科学界还无法深入涉足主观体验研究。但是，杰恩斯在这一课题上表现出色，开启了一扇通往有关什么是「活着」、如何感受「活着」的大门。&lt;/p&gt;

&lt;p&gt;杰恩斯是马萨诸塞州西牛顿村的一位神教牧师的儿子。虽然在他两岁时父亲就去世了，但是，父亲留下的48卷布道文一直陪伴着杰恩斯，如父犹在，谆谆教诲。在大学，杰恩斯尝试了哲学和文学，不过，最后决定学习心理学，毕竟，这门学科寻求的是有关物质世界的真实数据。1941年，他进入研究生院，不久，美国加入二战。虽然杰恩斯认为服兵役是不道德的，他还是被分配到了一个公民战争支援营。他当即写信给美国总检察长表示他准备离营，这个战争支援营和他的原则不相容：「我们能够在罪恶的体制逻辑下毁灭罪恶吗？耶稣不这样想……我也不这样想。」结果，他被送往监狱，在那里，他有充足的时间去思考关于意识的问题。「杰恩斯是一个很有原则的人，有些人或许会认为他冲动鲁莽」一个从前的学生和邻居回忆道，「他似乎能从与风车的搏斗中获得力量。」&lt;/p&gt;

&lt;p&gt;三年后，杰恩斯获释。他深信动物实验有助于解释意识的最初进化，于是他在耶鲁大学研究生院渡过了接下来的三年。有一阵子，他相信，如果一个动物能够从经历中学习，它就拥有对这个经历的体验，这意味着意识的存在。他让一群草履虫通过迷宫，这个迷宫是用蜡在胶木板上刻出来的，每当它们走错通道，他就吓唬它们。「我天真地假定我是在修撰『意识进化的编年史』，我又继续研究了具有突触神经网络的其它物种，比如，扁形虫、蚯蚓、鱼和爬行动物，它们的确能够『学习』。」他在书中讲述道。「『这也太荒谬了！』当时，我很不安，直到几年之后，我才明白这个假定完全无意义。」许多动物都能够被训练，但它们并不内省。这就是让杰恩斯十分纠结的地方。&lt;/p&gt;

&lt;p&gt;与此同时，他也在Frank Beach教授的指导下从事传统的动物母性行为研究。在那个年代，对意识感兴趣并从事这方面的研究，非常困难。当时主流心理学理论之一的行为主义探索的是人和动物对刺激的外显反应。那时，对难以捉摸的思想世界的沉思过时了，以电击作为条件的实验流行起来，行为主义可以被看作是对更早期、更不严谨的心理学思潮的合乎理性的反弹。但是，在杰恩斯大部分的人生中，内在体验远不是苍白的。在心理学界的某些领域，说自己研究意识表明你对神秘主义感兴趣。&lt;/p&gt;

&lt;p&gt;1949年，杰恩斯没有拿到博士学位就离开了学校，他拒绝提交他的学位论文。人们并不清楚他这样做的真正原因，有人说，他不能接受论文评审委员会提出的论文修改要求，有的说，他被学术界的等级制度给惹怒了，有的说，他只是厌倦了继续研究。还有种说法是他不愿交25美元的投稿费。（1977年，那时杰恩斯的书正在销售，他在耶鲁拿到了博士学位。）然而，非常清楚的是，研究的停滞不前让他深感挫败。后来，他曾写道：以穿梭在迷宫里的老鼠而不是人类意识为基础的心理学，不过是「冒充科学的蹩脚诗」。&lt;/p&gt;

&lt;p&gt;随后，他踏上了一段奇异之旅。1949年秋，他搬到了英国，当起剧作家和演员。随后15年里，他频繁来往于大洋两岸，在戏剧和助理教学角色之间切换，1964年，他终于在普林斯顿大学安顿下来。此后，他进行了大量阅读，深入思考「意识是什么」、「它是如何产生的」。1969年，他开始构思著述，试图从文化的根本流变来解释意识的起源，撇开了他一直依循的「进化论」。从他过去几十年来收集的材料看来，这本著述将会综合运用科学、考古学、人类学和文学等领域的知识。他相信，这条研究进路会直捣问题要害。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://img.chuansong.me/mmbiz/KmXPKA19gW8GFicqX570olcSZKYmlD1tCz6RFBegh3NhJ0moiaEuia2JreOaEsz0OuiackPaloicFhNDOVBQe7Q7BDA/0?wx_fmt=jpeg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;一部书的奇迹：虽然1997年去世的杰恩斯从来没有完成过其他书，但是，这本书将会让他名留千古。约翰·厄普代克在《纽约客》中写道：杰恩斯「推论直到公元前2000年人类都没有意识，而是自发地遵从上帝的声音，我们都震惊了。但是，通过了解所有他在古代文学、现代行为主义和诸如催眠、着魔、舌语现象、预言、诗歌和精神分裂症等异常的心理现象中找到的确凿证据，我们不得不接受这一非凡的论题。」&lt;/p&gt;

&lt;p&gt;这本书从第一个字开始就显示出了其高远的视角，「啊，这是一个充满了不可见的影象和可闻的沉默的世界，这是个思想的虚幻国度！」杰恩斯开篇写道，「这是一个没有独白和预告的隐秘剧院，是一个充满所有情绪、沉思、谜团的无形大厦，是一个容纳失望和发现的无边胜地。」&lt;/p&gt;

&lt;p&gt;为了探索这个内在国度的起源，杰恩斯首先透彻地概括了「意识不是什么」。它不是事物的天然属性。它也不仅仅是学习的过程。奇怪的是，它也不需要大量更为复杂的思维活动。拼拼图、网球发球、甚至是弹钢琴等活动都需要有意识的注意集中，但是，在掌握了一项技能之后，它又消退到模糊的潜意识世界中。思考它反而让它变得难以操作。在杰恩斯看来，此刻发生在你身上的一切，似乎大部分都不是意识的一部分，除非你注意到它们。你能感觉到刚才椅子在推你吗？或者，既然你已经问了自己这个问题，你现在能感觉到吗？&lt;/p&gt;

&lt;p&gt;杰恩斯告诉读者，对未来哲学和认知科学学生们来说，书中谈到的意识问题是一个挑战。「它比我们意识到的那部分精神生活还要少，因为我们意识不到那些没有意识到的事。」随后，他十分精彩地论证了他的观点，「这就跟要求我们在一个黑暗的房间里用手电筒寻找那些完全处于光照之外的东西一样。手电筒指向之处都会有光，光亮似乎无处不在。意识看似弥漫在整个精神世界中，但事实并非如此。」&lt;/p&gt;

&lt;p&gt;不过，最让杰恩斯吃惊的是：知识甚至创造灵感似乎也不由我们控制。你可以在非思考状态下判断两个杯子哪个更重，你一拿起它们你就知道了。在解决问题的情况下，无论创造与否，我们得到了思考所需要的信息，但是，我们依旧无法解决问题。在随后的洗澡或者散步过程中，我们却想到了解决办法。杰恩斯告诉他的邻居，虽然他现在正看着圣约翰河上的冰块移动，但是，他的理论终会成形。是那些我们没有意识到的东西在推动理论的形成。&lt;/p&gt;

&lt;p&gt;杰恩斯尝试这样解释：习惯、本能以及其他过程能处理的事情远比我们认为的多得多，意识仅仅是漂浮在这片汪洋大海上的一层薄冰。「如果我们的推理是正确的，」他写道，「这类人很可能存在，他们同样做着大部分我们会做的事，比如，说话、判断、推理、解决问题等，但是，他们对此却毫无意识。」&lt;/p&gt;

&lt;p&gt;杰恩斯相信，在他所定义的意识成为可能之前，必须要有语言的存在。所以，他决定阅读早期的著作文本，包括《伊利亚特》和《奥德赛》，寻找无法自省之人存在的蛛丝马迹——那些如同汪洋大海而非漂浮薄冰的人。他坚信自己在《伊利亚特》中发现了这些人。他写道，《伊利亚特》中的人物是不可内省的，他们不会采取独立行动（根据自己的意思采取行动——译者），仅仅接受神的指引。没有这些声音，英雄就会像木偶一样呆立在特洛伊的海滩上，纹丝不动。&lt;/p&gt;

&lt;p&gt;众所周知，说话能力位于大脑左半球，而不是分散在左右两个半球。杰恩斯认为，大脑右半球缺乏语言能力是因为它曾经被用于其他的功能——特别是，用以向左脑语言中心发出警告。这些以幻觉方式出现的信息帮助人类渡过难关，例如，治国决策或者是否踏上冒险之旅。&lt;/p&gt;

&lt;p&gt;杰恩斯写道，本能和声音的结合（即所谓的二分意识），只要人类社会等级森严，人类能够自我管理相当长一段时间。但是，大约3000年前，人口过剩，自然灾害、战争的压力大大超过警告之声的极限。二分心智分崩离析，之前能够听到的警告之声完全消失，自我意识开始零零碎碎地回到人们的意识当中。一个更灵活却更艰巨的应对日常生活的方式由此诞生——当神沉默之后混乱接踵而至之时，这种新的应对方式再适合不过了。他说，《奥德赛》中的人物会有内在思维活动（不过，他认为，这是由于后来译者添加上去的，反应的是现代心智。——译者）。伴随着内在叙述以及对来自更高权力指引的渴望，现代心智（ modern mind）出现了。&lt;/p&gt;

&lt;p&gt;该书余下的400页介绍了杰恩斯在旧约、玛雅石雕和苏美尔著作找到的证据，证明了世界范围内二分心智的分离。他提到大约公元前1230年的一座石雕，亚述王跪在一个空的神的宝座前。这一时期发生在今希腊境内的持续不断的大迁移，被杰恩斯视为二分心智崩塌引发的骚乱。杰恩斯反思了这一转变（二分心智的崩塌——译者）如何可能在今天重演。有感于当下形势，他十分敬畏地写道「公元2000年末，某种意义上，我们仍深处这场通向新的心智的转折中。我们都处在二分坍塌这场转折的残余之中。」「我们的国王、总统和官员过去还听着神的指示，现在只能带着对沉默神灵的宣誓而开始他们的任期。」&lt;/p&gt;

&lt;p&gt;这本书全面深刻而且古怪。但是，《二分心智的崩塌：人类意识的起源（The Origin of Consciousness in the Breakdown of the Bicameral Mind） 》非常具有吸引力。部分原因可能在于，许多读者以前从未想过意识是什么。或许这是许多人第一次伸手触碰自己的意识，发现它们并不像自己所期望的那样。杰恩斯的书在特殊的时代确实带来了震撼，此时，这种震撼可能产生独特的强大效力。20世纪70年代，越来越多的人开始对意识的问题感兴趣。鲍迈斯特（Baumeister）非常欣赏杰恩斯，他在这本书出版之前就阅读了它，他说杰恩斯掀起了「精神阶段」的新时代运动。&lt;/p&gt;

&lt;p&gt;这本书的语言运用非常好。它具有纳博科夫式的丰富性。他的散文优雅、有力量且具有可信度。它听起来像预言，但感觉却十分真实，并且具有令人难以置信的重量。真理与美丽以一种人类难以分开的方式交织在一起。负责 Storycollider故事系列（springer出品的一个科学主题的故事系列——译者）物理学家本·莉莉（ Ben Lillie）回忆他发现杰恩斯书的情景时说：「那时我身处一群喜欢黑色着装、在年鉴和报纸办公室闲逛谈论知识的人当中」莉莉说，「有人在读这本书，我不记得谁是最先读它的人，反正不是我。突然之间我们都认为它听起来不错，就都开始阅读。因为书中的观点是在挑战通识，读者会感觉自己像个反抗者。」&lt;/p&gt;

&lt;p&gt;人们很容易发现这本书的逻辑漏洞：首当其冲的就是《伊利亚特》中的人物会自省，虽然杰恩斯认为这些是后来添加或者误译的。但是，这些漏洞不会削弱书的震撼力。对于Aeon（一个在线科学和哲学杂志，也是机器之心非常欣赏的一个在线杂志——译者）创始人保罗•海恩斯这样的读者来说，这本书的吸引力并不主要在于杰恩斯的中心论点。他说，「吸引我的是他的方法和风格，以及文本的启发和怀旧情绪，而不是论点细节，尽管这些细节也很有趣。」「杰恩斯准备从自己的角度探索这个前沿问题，对意识现象的解释并未损及它的神秘性。」&lt;/p&gt;

&lt;p&gt;在此期间，过去四十年里，风向变了，科学研究者都寻找最好的问题去发问。诸如艾伦脑科学研究所（Allen Institute for Brain Science）和瑞士联邦理工大学的大脑与心智学院（Brain-Mind Institute of the Swiss Federal Institute of Technology）的一些重大项目，都试图理解脑部结构和功能，试图解开许多疑问，包括何为意识、它是如何生成并直达神经元的。行为经济学已经兴起，作为一个完整的研究领域，它描述和使用了我们对自己行为毫无意识的这些行为方式——这正是杰恩斯这本书的主题——以及这门学科创始人的一些洞见，学科创始人丹尼尔•卡尼曼(Daniel Kahneman)和弗农 •史密斯(Vernon L. Smith)，均为诺奖得主。&lt;/p&gt;

&lt;p&gt;Eric Schwitzgebel，一位加州大学的哲学教授做了项实验，调查我们如何意识到那些我们并未关注的事情，实验回应了杰恩斯的观点：意识本质上是察知（ consciousness is essentially awareness）。 Schwitzgebel 说，「你意识到的唯一的事情就是你刚才打算去做的事情，这种说法不是不合理。但是，我们也能合理的认为，（与此同时）许多事情正在背景里和意识周边上演着。在关注的背后，你正经验着所有这些。」正是这些问题使杰恩斯成为哲学和神经科学的焦点话题。但是，与此同时，杰恩斯的书依然处在科学边缘。 Schwitzgebel 说，「古希腊人没有自我意识，这个观点仍远在主流之外。」&lt;/p&gt;

&lt;p&gt;丹尼特（Dennett）称《二分心智的崩塌：人类意识的起源（The Origin of Consciousness in the Breakdown of the Bicameral Mind）》是本「非凡而奇异的书」，他向杰恩斯释放出了最大的善意。他说，「有很多确实好的想法蛰伏于无用的垃圾之中」。他尤其认为杰恩斯的这个观点让他深为叹服：动物与人类的心智是不同的，这一差异源于语言。&lt;/p&gt;

&lt;p&gt;丹内特说，「黑猩猩与人类意识的差异如此之大，以至于需要特殊解释，解释需要大量调用人类自然语言的区别。」他挖苦地承认，「问题偏离主流」「我并没有设法就此影响主流 。」&lt;/p&gt;

&lt;p&gt;那些无时无刻不被研究意识的神经系统科学家提及的问题，也要归功于杰恩斯的疯狂想法。Antonio Damasio是一位神经科学的教授，南加州大学大脑与创造研究所的主任（the director of the Brain and Creativity Institute at the University of Southern California）。他在2010年的新书《忆起自我（Self Comes to Mind）》中，支持杰恩斯关于发生在人类意识中的相对较近的过去事情的看法。他写道，「随着关于人类和宇宙知识的积累，持续反思能很好的改变自我结构，促使意识过程中相对独立层面产生更为紧密的缝合；大脑活动的协调——首先是被价值所驱动，接着是被理性（reason）——正在对我们有利。」但是，这种支持实属罕见。更为常见的反响是，比如加州大学的荣誉教授、神经思想家 Patricia S. Churchland认为，「他的书是空想。我认为，它并未实质增进我们对意识性质的理解，也未帮助我们理解意识如何从大脑活动中得以产生。」&lt;/p&gt;

&lt;p&gt;杰恩斯把他的理论当成是一个科学界的贡献，却对科研界的反应感到失望，尽管公众对他作品很感兴趣。杰恩斯越发地酗酒。他的第二本书，他打算在这本书中进一步推进那些观点，再也没有完成。&lt;/p&gt;

&lt;p&gt;尽管作品古怪，但这份遗产还是流传下来。多年之后，丹尼特有时会在他的谈话中提到，他认为杰恩斯意识到了某些事情。后来——每次人群褪去，公开讨论结束之后——几乎都会有人畏缩不前。「现在，我能走出壁橱。」他或者她会说，「我认为杰恩特也是极好的。」&lt;/p&gt;

&lt;p&gt;Marcel Kuijsten从事IT，他经营着一家叫做朱利安·杰恩斯社团（the Julian Jaynes Society） 的组织，成员大概有五、六百人，来自世界各地。这个组织的有一个线上成员论坛，讨论杰恩特的理论，2013年，他们首次主办了一次为期两天的会议，地点在西维吉尼亚。他说，「这是一个难忘的经历。」&lt;/p&gt;

&lt;p&gt;Kuijsten 认为，许多攻击杰恩斯的人根本不愿花气力去理解讨论的观点，他也承认杰恩斯的论述很难让人转过弯来（ hard to get one’s mind around）。他说，「人们带着根深蒂固、先入为主的有关意识的观念来理解杰恩斯。也许，他们只是读了书的背面」。但是，Kuijsten考虑长远。他说，「我不打算在这里改变其他人的想法，这无疑是在浪费时间。我想给那些读过书的和想讨论的人们提供最好信息和资源。」&lt;/p&gt;

&lt;p&gt;为了这个目的，Kuijsten和社团发布了一些关于杰恩斯作品的的书以及有关他和他的工作的新文章。无论何时，只要任何有关杰恩斯提出的问题的新发现出版了，Kuijsten都会在网上做好记录。2009年，他强调，脑成像研究表明，听觉幻象源于大脑右侧，随之而来的行为则是在左侧，这听起来与Jaynes的二分心智的理论相似。他希望随着时间流逝，人们能够根据新科学重新审视杰恩斯的观点。&lt;/p&gt;

&lt;p&gt;最终，杰恩特的书里提到的广博问题也会同样困惑着神经学家和门外汉。什么时候以及为什么我们开始了内心表述？我们日复一日的无意识经历有多少？有意识和无意识过程之间的界线在哪里？这些问题依旧悬而未决。或许，杰恩斯的奇怪假设永远无助于回答这些问题。但是，许多人——读者，科学家和哲学家——都会感谢他的尝试。&lt;/p&gt;
</description>
        <pubDate>Tue, 08 May 2018 06:08:54 +0800</pubDate>
        <link>/2018/05/08/%E6%84%8F%E8%AF%86%E8%AF%9E%E7%94%9F%E4%BA%8E%E4%B8%8A%E5%B8%9D%E6%B2%89%E9%BB%98%E6%97%B6/</link>
        <guid isPermaLink="true">/2018/05/08/%E6%84%8F%E8%AF%86%E8%AF%9E%E7%94%9F%E4%BA%8E%E4%B8%8A%E5%B8%9D%E6%B2%89%E9%BB%98%E6%97%B6/</guid>
        
        
        <category>冥想</category>
        
      </item>
    
      <item>
        <title>矩阵求导</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;1 摘要&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;2 标量对矩阵的求导&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-2&quot; id=&quot;markdown-toc-section-2&quot;&gt;3 矩阵对矩阵的求导&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-3&quot; id=&quot;markdown-toc-section-3&quot;&gt;4 反向传播算法的完整向量形式推导&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section&quot;&gt;1 摘要&lt;/h2&gt;

&lt;p&gt;矩阵求导的技术，在统计学、控制论、机器学习等领域有广泛的应用。鉴于我看过的一些资料或言之不详、或繁乱无绪，本文来做个科普，分作两部分，第一部分讲标量对矩阵的求导术，第二部分讲矩阵对矩阵的求导术。本文使用小写字母 $x$ 表示标量，粗体小写字母 $\vec{x}$ 表示向量，大写字母 $\vec{X}$ 表示矩阵。本文第一部分标量对矩阵的求导整理自：https://zhuanlan.zhihu.com/p/24709748，本文第二部分矩阵对矩阵的求导整理自：https://zhuanlan.zhihu.com/p/24863977&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;2 标量对矩阵的求导&lt;/h2&gt;

&lt;p&gt;首先来琢磨一下定义，标量 $f$ 对矩阵 $\vec{X}$ 的导数，定义为
&lt;script type=&quot;math/tex&quot;&gt;\frac{\partial f}{\partial \vec{X}} = \left[\frac{\partial f }{\partial \vec{X}_{ij}}\right].&lt;/script&gt;
即 $f$ 对 $\vec{X}$ 逐元素求导排成与 $\vec{X}$ 尺寸相同的矩阵。然而，这个定义在计算中并不好用，实用上的原因是在对较复杂的函数难以逐元素求导；哲理上的原因是逐元素求导破坏了整体性。试想，为何要将 $f$ 看做矩阵 $\vec{X}$ 而不是各元素 $\vec{X}_{ij}$ 的函数呢？答案是用矩阵运算更整洁。所以在求导时不宜拆开矩阵，而是要找一个从整体出发的算法。为此，我们来回顾，一元微积分中的导数（标量对标量的导数）与微分有联系：
&lt;script type=&quot;math/tex&quot;&gt;\mathrm{d}f = f&#39;(x)\mathrm{d}x.&lt;/script&gt;
多元微积分中的梯度（标量对向量的导数）也与微分有联系：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \mathrm{d}f = \sum_{i} \frac{\partial f}{\partial x_i}\mathrm{d}x_i = \frac{\partial f}{\partial \vec{x}}^T \mathrm{d}\vec{x}.
\end{equation}&lt;/script&gt;

&lt;p&gt;这里第一个等号是全微分公式，第二个等号表达了梯度 $\frac{\partial f}{\partial \vec{x}}$ 与微分的联系。受此启发，我们将矩阵导数与微分建立联系：
&lt;script type=&quot;math/tex&quot;&gt;\begin{equation}
  \mathrm{d}f = \sum_{i,j} \frac{\partial f}{\partial \vec{X}{ij}}\mathrm{d}\vec{X}_{ij} = \mathrm{tr}\left(\frac{\partial f}{\partial \vec{X}}^T \mathrm{d}\vec{X}\right).
\end{equation}&lt;/script&gt;
这里 $\mathrm{tr}$ 代表迹(trace)是方阵对角线元素之和，满足性质：对尺寸相同的矩阵 $\vec{A}, \vec{B}$，有 $\mathrm{tr}(\vec{A}^T\vec{B}) = \sum_{i,j}\vec{A}&lt;em&gt;{ij}\vec{B}&lt;/em&gt;{ij}$，即 $\mathrm{tr}(\vec{A}^T\vec{B})$ 是矩阵 $\vec{A}, \vec{B}$ 的内积，因此上式与原定义相容。&lt;/p&gt;

&lt;p&gt;然后来建立运算法则。回想遇到较复杂的一元函数如 $f = \log(2+\sin x)e^{\sqrt{x}}$，我们是如何求导的呢？通常不是从定义开始求极限，而是先建立了初等函数求导和四则运算、复合等法则，再来运用这些法则。故而，我们来创立常用的矩阵微分的运算法则：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;加减法：$\mathrm{d}(\vec{X} \pm \vec{Y}) = \mathrm{d}\vec{X} \pm \mathrm{d}\vec{Y}$；&lt;/li&gt;
  &lt;li&gt;矩阵乘法：$\mathrm{d}(\vec{X}\vec{Y}) = \mathrm{d}\vec{X}\vec{Y} + \vec{X}\mathrm{d}\vec{Y}$；&lt;/li&gt;
  &lt;li&gt;转置：$\mathrm{d}(\vec{X}^T) = (\mathrm{d}\vec{X})^T$；&lt;/li&gt;
  &lt;li&gt;迹：$\mathrm{d}\mathrm{tr}(\vec{X}) = \mathrm{tr}(\mathrm{d}\vec{X})$；&lt;/li&gt;
  &lt;li&gt;逆：$\mathrm{d}\vec{X}^{-1} = -\vec{X}^{-1}\mathrm{d}\vec{X}\vec{X}^{-1}$，此式可在 $\vec{X}\vec{X}^{-1}=\vec{I}$ 两侧求微分来证明；&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;行列式：$\mathrm{d}&lt;/td&gt;
          &lt;td&gt;\vec{X}&lt;/td&gt;
          &lt;td&gt;= \mathrm{tr}(\vec{X}^{#}\mathrm{d}\vec{X})$，其中 $\vec{X}^{#}$ 表示 $\vec{X}$ 的伴随矩阵，在 $\vec{X}$ 可逆时又可以写作 $\mathrm{d}&lt;/td&gt;
          &lt;td&gt;\vec{X}&lt;/td&gt;
          &lt;td&gt;=&lt;/td&gt;
          &lt;td&gt;\vec{X}&lt;/td&gt;
          &lt;td&gt;\mathrm{tr}(\vec{X}^{-1}\mathrm{d}\vec{X})$，此式可用Laplace展开来证明，详见张贤达《矩阵分析与应用》第279页；&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;逐元素乘法：$\mathrm{d}(\vec{X}\odot \vec{Y}) = \mathrm{d}\vec{X}\odot \vec{Y} + \vec{X}\odot \mathrm{d}\vec{Y}$，$\odot$ 表示尺寸相同的矩阵 $\vec{X},\vec{Y}$ 逐元素相乘；&lt;/li&gt;
  &lt;li&gt;逐元素函数：$\mathrm{d}\sigma(\vec{X}) = \sigma’(\vec{X})\odot \mathrm{d}\vec{X}$，$\sigma(\vec{X}) = \left[\sigma(\vec{X}_{ij})\right]$ 是逐元素运算的标量函数。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们试图利用矩阵导数与微分的联系 $\mathrm{d}f = \mathrm{tr}\left(\frac{\partial f}{\partial \vec{X}}^T \mathrm{d}\vec{X}\right)$，在求出左侧的微分 $\mathrm{d}f$ 后，该如何写成右侧的形式并得到导数呢？这需要一些迹技巧(trace trick)：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;标量套上迹：$a = \mathrm{tr}(a)$；&lt;/li&gt;
  &lt;li&gt;转置：$\mathrm{tr}(\vec{A}^T) = \mathrm{tr}(\vec{A})$；&lt;/li&gt;
  &lt;li&gt;线性：$\mathrm{tr}(\vec{A}\pm \vec{B}) = \mathrm{tr}(\vec{A})\pm \mathrm{tr}(\vec{B})$；&lt;/li&gt;
  &lt;li&gt;矩阵乘法交换：$\mathrm{tr}(\vec{AB}) = \mathrm{tr}(\vec{BA})$，两侧都等于 $\sum_{i,j}\vec{A}&lt;em&gt;{ij}\vec{B}&lt;/em&gt;{ji}$；&lt;/li&gt;
  &lt;li&gt;矩阵乘法/逐元素乘法交换：$\mathrm{tr}(\vec{A}^T(\vec{B}\odot \vec{C})) = \mathrm{tr}((\vec{A}\odot \vec{B})^T\vec{C})$，两侧都等于 $\sum_{i,j}\vec{A}&lt;em&gt;{ij}\vec{B}&lt;/em&gt;{ij}\vec{C}_{ij}$。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;观察一下可以断言，若标量函数 $f$ 是矩阵 $\vec{X}$ 经加减乘法、行列式、逆、逐元素函数等运算构成，则使用相应的运算法则对 $f$ 求微分，再使用迹技巧给 $\mathrm{d}f$ 套上迹并将其它项交换至 $\mathrm{d}\vec{X}$ 左侧，即能得到导数。&lt;/p&gt;

&lt;p&gt;在建立法则的最后，来谈一谈复合：假设已求得 $\frac{\partial f}{\partial \vec{Y}}$，而 $\vec{Y}$ 是 $\vec{X}$ 的函数，如何求 $\frac{\partial f}{\partial \vec{X}}$ 呢？在微积分中有标量求导的链式法则 $\frac{\partial f}{\partial x} = \frac{\partial f}{\partial y} \frac{\partial y}{\partial x}$，但这里我们不能沿用链式法则，因为矩阵对矩阵的导数 $\frac{\partial \vec{Y}}{\partial \vec{X}}$ 截至目前仍是未定义的。于是我们继续追本溯源，链式法则是从何而来？源头仍然是微分。我们直接从微分入手建立复合法则：先写出 $\mathrm{d}f = \mathrm{tr}\left(\frac{\partial f}{\partial \vec{Y}}^T \mathrm{d}\vec{Y}\right)$，再将 $\mathrm{d}\mathbf{Y}$ 用 $\mathrm{d}\vec{X}$ 表示出来代入，并使用迹技巧将其他项交换至 $\mathrm{d}\vec{X}$ 左侧，即可得到 $\frac{\partial f}{\partial \vec{X}}$。&lt;/p&gt;

&lt;p&gt;接下来演示一些算例。特别提醒要依据已经建立的运算法则来计算，不能随意套用微积分中标量导数的结论，比如认为 $\vec{A}\vec{X}$ 对 $\vec{X}$ 的导数为 $\vec{A}$，这是没有根据、意义不明的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;例1&lt;/strong&gt;：$f = \vec{a}^T \vec{X}\vec{b}$，求 $\frac{\partial f}{\partial \vec{X}}$。&lt;/p&gt;

&lt;p&gt;解：先使用矩阵乘法法则求微分：$\mathrm{d}f = \vec{a}^T \mathrm{d}\vec{X}\vec{b}$，再套上迹并做交换：$\mathrm{d}f = \mathrm{tr}(\vec{a}^T\mathrm{d}\vec{X}\vec{b}) = \mathrm{tr}(\vec{b}\vec{a}^T\mathrm{d}\vec{X})$，对照导数与微分的联系，得到 $\frac{\partial f}{\partial \vec{X}} = \vec{a}\vec{b}^T$。&lt;/p&gt;

&lt;p&gt;注意：这里不能用 $\frac{\partial f}{\partial \vec{X}} =\vec{a}^T \frac{\partial \vec{X}}{\partial \vec{X}}\vec{b}=?$，导数与乘常数矩阵的交换是不合法则的运算（而微分是合法的）。有些资料在计算矩阵导数时，会略过求微分这一步，这是逻辑上解释不通的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;例2【线性回归】&lt;/strong&gt;：$l = |\vec{X}\vec{w}- \vec{y}|^2$，求 $\frac{\partial l}{\partial \vec{w}}$。&lt;/p&gt;

&lt;p&gt;解：严格来说这是标量对向量的导数，不过可以把向量看做矩阵的特例。将向量范数写成 $l = (\vec{X}\vec{w}- \vec{y})^T(\vec{X}\vec{w}- \vec{y})$，求微分，使用矩阵乘法、转置等法则：$\mathrm{d}l = (\vec{X}\mathrm{d}\vec{w})^T(\vec{X}\vec{w}-\vec{y})+(\vec{X}\vec{w}-\vec{y})^T(\vec{X}\mathrm{d}\vec{w}) = 2(\vec{X}\vec{w}-\vec{y})^T\vec{X}\mathrm{d}\vec{w}$。 对照导数与微分的联系，得到 $\frac{\partial l}{\partial \vec{w}}= 2\vec{X}^T(\vec{X}\vec{w}-\vec{y})$。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;例3【多元 logistic 回归】&lt;/strong&gt;：$l = -\vec{y}^T\log \mathrm{softmax}(\vec{Wx})$，求 $\frac{\partial l}{\partial \vec{W}}$。其中 $\vec{y}$ 是除一个元素为 1 外其它元素为 0 的向量；$\mathrm{softmax}(\vec{a}) = \frac{\exp(\vec{a})}{\vec{1}^T\exp(\vec{a})}$，其中 $\exp(\vec{a})$ 表示逐元素求指数，$\vec{1}$ 代表全 1 向量。&lt;/p&gt;

&lt;p&gt;解：首先将 softmax 函数代入并写成 $l = -\vec{y}^T \left(\log (\exp(\vec{Wx}))-\vec{1}\log(\vec{1}^T\exp(\vec{Wx}))\right) = -\vec{y}^T\vec{Wx} + \log(\vec{1}^T\exp(\vec{Wx}))$，这里要注意逐元素 $\log$ 满足等式 $\log(\vec{u}/c) = \log(\vec{u}) - \vec{1}\log(c)$，以及 $\vec{y}$ 满足 $\vec{y}^T \vec{1} = 1$。求微分，使用矩阵乘法、逐元素函数等法则：$\mathrm{d}l = -\vec{y}^T\mathrm{d}\vec{Wx}+\frac{\vec{1}^T\left(\exp(\vec{Wx})\odot(\mathrm{d}\vec{Wx})\right)}{\vec{1}^T\exp(\vec{Wx})}$。
再套上迹并做交换，注意可化简 $\vec{1}^T\left(\exp(\vec{Wx})\odot(\mathrm{d}\vec{Wx})\right) = \exp(\vec{Wx})^T\mathrm{d}\vec{Wx}$，这是根据等式 $\vec{1}^T (\vec{u}\odot \vec{v}) = \vec{u}^T \vec{v}$，故 $\mathrm{d}l = \mathrm{tr}\left(-\vec{y}^T\mathrm{d}\vec{Wx}+\frac{\exp(\vec{Wx})^T\mathrm{d}\vec{Wx}}{\vec{1}^T\exp(\vec{Wx})}\right) =\mathrm{tr}(\vec{x}(\mathrm{softmax}(\vec{Wx})-\vec{y})^T\mathrm{d}\vec{W})$。对照导数与微分的联系，得到 $\frac{\partial l}{\partial \vec{W}}= (\mathrm{softmax}(\vec{Wx})-\vec{y})\vec{x}^T$。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;另解&lt;/strong&gt;：定义 $\vec{a} = \vec{Wx}$，则 $l = -\vec{y}^T\log\mathrm{softmax}(\vec{a})$，先如上求出 $\frac{\partial l}{\partial \vec{a}} = \mathrm{softmax}(\vec{a})-\vec{y}$，再利用复合法则：$\mathrm{d}l = \mathrm{tr}\left(\frac{\partial l}{\partial \vec{a}}^T\mathrm{d}\vec{a}\right) = \mathrm{tr}\left(\frac{\partial l}{\partial \vec{a}}^T\mathrm{d}\vec{Wx}\right) = \mathrm{tr}\left(\vec{x}\frac{\partial l}{\partial \vec{a}}^T\mathrm{d}\vec{W}\right)$，得到 $\frac{\partial l}{\partial \vec{W}}= \frac{\partial l}{\partial\vec{a}}\vec{x}^T$。&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;例4【方差的最大似然估计】&lt;/strong&gt;：样本 $\vec{x}_1,\dots, \vec{x}_n\sim N(\vec{\mu}, \Sigma)$，其中 $\Sigma$ 是对称正定矩阵，求方差 $\Sigma$ 的最大似然估计。写成数学式是：$l = \log&lt;/td&gt;
      &lt;td&gt;\Sigma&lt;/td&gt;
      &lt;td&gt;+\frac{1}{n}\sum_{i=1}^n(\vec{x}_i-\vec{\bar{x}})^T\Sigma^{-1}(\vec{x}_i-\vec{\bar{x}})$，求 $\frac{\partial l }{\partial \Sigma}$ 的零点。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;解：首先求微分，使用矩阵乘法、行列式、逆等运算法则，第一项是 $\mathrm{d}\log&lt;/td&gt;
      &lt;td&gt;\Sigma&lt;/td&gt;
      &lt;td&gt;=&lt;/td&gt;
      &lt;td&gt;\Sigma&lt;/td&gt;
      &lt;td&gt;^{-1}\mathrm{d}&lt;/td&gt;
      &lt;td&gt;\Sigma&lt;/td&gt;
      &lt;td&gt;= \mathrm{tr}(\Sigma^{-1}\mathrm{d}\Sigma)$，第二项是 $\frac{1}{n}\sum_{i=1}^n(\vec{x}&lt;em&gt;i-\vec{\bar{x}})^T\mathrm{d}\Sigma^{-1}(\vec{x}_i-\vec{\bar{x}}) = -\frac{1}{n}\sum&lt;/em&gt;{i=1}^n(\vec{x}&lt;em&gt;i-\vec{\bar{x}})^T\Sigma^{-1}\mathrm{d}\Sigma\Sigma^{-1}(\vec{x}_i-\vec{\bar{x}})$。再给第二项套上迹做交换：$\mathrm{d}l = \mathrm{tr}\left(\left(\Sigma^{-1}-\Sigma^{-1}\vec{S}\Sigma^{-1}\right)\mathrm{d}\Sigma\right)$，其中 $\vec{S} = \frac{1}{n}\sum&lt;/em&gt;{i=1}^n(\vec{x}_i-\vec{\bar{x}})(\vec{x}_i-\vec{\bar{x}})^T$  定义为样本方差。对照导数与微分的联系，有 $\frac{\partial l }{\partial \Sigma}=(\Sigma^{-1}-\Sigma^{-1}\vec{S}\Sigma^{-1})^T$，其零点即 $\Sigma$ 的最大似然估计为 $\Sigma = \vec{S}$。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;最后一例留给经典的神经网络。神经网络的求导术是学术史上的重要成果，还有个专门的名字叫做 BP 算法，我相信如今很多人在初次推导 BP 算法时也会颇费一番脑筋，事实上使用矩阵求导术来推导并不复杂。为简化起见，我们推导二层神经网络的 BP 算法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;例5【二层神经网络】&lt;/strong&gt;：$l = -\vec{y}^T\log\mathrm{softmax}(\vec{W}_2\sigma(\vec{W}_1\vec{x}))$，求 $\frac{\partial l}{\partial \vec{W}_1}$ 和 $\frac{\partial l}{\partial \vec{W}_2}$。其中 $\vec{y}$ 是除一个元素为 $1$ 外其它元素为 $0$ 的向量，$\mathrm{softmax}(\vec{a}) = \frac{\exp(\vec{a})}{\vec{1}^T\exp(\vec{a})}$ 同例 3，$\sigma(\cdot)$ 是逐元素 sigmoid 函数 $\sigma(a) = \frac{1}{1+\exp(-a)}$。&lt;/p&gt;

&lt;p&gt;解：定义 $\vec{a}_1=\vec{W}_1\vec{x}$，$\vec{h}_1 = \sigma(\vec{a}_1)$，$\vec{a}_2 = \vec{W}_2 \vec{h}_1$，则 $l =-\vec{y}^T\log\mathrm{softmax}(\vec{a}_2)$。 在例 3 中已求出 $\frac{\partial l}{\partial \vec{a}_2} = \mathrm{softmax}(\vec{a}_2)-\vec{y}$。使用复合法则，注意此处 $\vec{h}_1, \vec{W}_2$ 都是变量：$\mathrm{d}l = \mathrm{tr}\left(\frac{\partial l}{\partial \vec{a}_2}^T\mathrm{d}\vec{a}_2\right) = \mathrm{tr}\left(\frac{\partial l}{\partial \vec{a}_2}^T\mathrm{d}\vec{W}_2 \vec{h}_1\right) + \mathrm{tr}\left(\frac{\partial l}{\partial \vec{a}_2}^T\vec{W}_2 \mathrm{d}\vec{h}_1\right)$，使用矩阵乘法交换的迹技巧从第一项得到 $\frac{\partial l}{\partial \vec{W}_2}= \frac{\partial l}{\partial\vec{a}_2}\vec{h}_1^T$，从第二项得到 $\frac{\partial l}{\partial \vec{h}_1}= \vec{W}_2^T\frac{\partial l}{\partial\vec{a}_2}$。接下来求 $\frac{\partial l}{\partial \vec{a}_1}$，继续使用复合法则，并利用矩阵乘法和逐元素乘法交换的迹技巧：$\mathrm{tr}\left(\frac{\partial l}{\partial\vec{h}_1}^T\mathrm{d}\vec{h}_1\right) = \mathrm{tr}\left(\frac{\partial l}{\partial\vec{h}_1}^T(\sigma’(\vec{a}_1)\odot \mathrm{d}\vec{a}_1)\right) = \mathrm{tr}\left(\left(\frac{\partial l}{\partial\vec{h}_1}\odot \sigma’(\vec{a}_1)\right)^T\mathrm{d}\vec{a}_1\right)$，得到 $\frac{\partial l}{\partial \vec{a}_1}= \frac{\partial l}{\partial\vec{h}_1}\odot\sigma’(\vec{a}_1)$。 为求 $\frac{\partial l}{\partial \vec{W}_1}$，再用一次复合法则：$\mathrm{tr}\left(\frac{\partial l}{\partial\vec{a}_1}^T\mathrm{d}\vec{a}_1\right) = \mathrm{tr}\left(\frac{\partial l}{\partial\vec{a}_1}^T\mathrm{d}\vec{W}_1\vec{x}\right) = \mathrm{tr}\left(\vec{x}\frac{\partial l}{\partial\vec{a}_1}^T\mathrm{d}\vec{W}_1\right)$，得到 $\frac{\partial l}{\partial \vec{W}_1}= \frac{\partial l}{\partial\vec{a}_1}\vec{x}^T$。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;3 矩阵对矩阵的求导&lt;/h2&gt;

&lt;p&gt;矩阵对矩阵的求导采用了向量化的思路，常应用于二阶方法求解优化问题。&lt;/p&gt;

&lt;p&gt;首先来琢磨一下定义。矩阵对矩阵的导数，需要什么样的定义？第一，矩阵 $\vec{F}(p\times q)$ 对矩阵 $\vec{X}(m\times n)$ 的导数应包含所有 $mnpq$ 个偏导数 $\frac{\partial \vec{F}&lt;em&gt;{kl}}{\partial \vec{X}&lt;/em&gt;{ij}}$，从而不损失信息；第二，导数与微分有简明的联系，因为在计算导数和应用中需要这个联系；第三，导数有简明的从整体出发的算法。我们先定义向量 $\vec{f}(p\times 1)$ 对向量 $\vec{x}(m\times1)$ 的导数
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{equation}
    \frac{\partial \vec{f}}{\partial \vec{x}} =
    \begin{bmatrix}
    \frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_1} &amp; \cdots &amp; \frac{\partial f_p}{\partial x_1}\\ \frac{\partial f_1}{\partial x_2} &amp; \frac{\partial f_2}{\partial x_2} &amp; \cdots &amp; \frac{\partial f_p}{\partial x_2}\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ \frac{\partial f_1}{\partial x_m} &amp; \frac{\partial f_2}{\partial x_m} &amp; \cdots &amp; \frac{\partial f_p}{\partial x_m}\\ \end{bmatrix}(m\times p)，
\end{equation} %]]&gt;&lt;/script&gt;
有 $\mathrm{d}\vec{f} = \frac{\partial \vec{f} }{\partial \vec{x} }^T \mathrm{d}\vec{x}$；再定义矩阵的（按列优先）向量化
&lt;script type=&quot;math/tex&quot;&gt;\begin{equation}
    \mathrm{vec}(\vec{X}) = [\vec{X}_{11}, \ldots, \vec{X}_{m1}, \vec{X}_{12}, \ldots, \vec{X}_{m2}, \ldots, \vec{X}_{1n}, \ldots, \vec{X}_{mn}]^T(mn\times1),
\end{equation}&lt;/script&gt;
并定义矩阵 $\vec{F}$ 对矩阵 $\vec{X}$ 的导数：
&lt;script type=&quot;math/tex&quot;&gt;\begin{equation}
    \frac{\partial \vec{F}}{\partial \vec{X}} = \frac{\partial \mathrm{vec}(\vec{F})}{\partial \mathrm{vec}(\vec{X})}(mn\times pq).
\end{equation}&lt;/script&gt;
导数与微分有联系 $\mathrm{vec}(\mathrm{d}\vec{F}) = \frac{\partial \vec{F}}{\partial \vec{X}}^T \mathrm{vec}(\mathrm{d}\vec{X})$。几点说明如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;按此定义，标量 $f$ 对矩阵 $\vec{X}(m\times n)$ 的导数 $\frac{\partial f}{\partial \vec{X}}$ 是 $mn\times1$ 向量，与上篇的定义不兼容，不过二者容易相互转换。为避免混淆，我们用 $\nabla_{\vec{X}} f$ 表示上篇定义的 $m\times n$ 矩阵，则有 $\frac{\partial f}{\partial \vec{X}}=\mathrm{vec}(\nabla_{\vec{X}} f)$。虽然本篇的技术可以用于标量对矩阵求导这种特殊情况，但使用上篇中的技术更方便。读者可以通过上篇中的算例试验两种方法的等价转换；&lt;/li&gt;
  &lt;li&gt;标量对矩阵的二阶导数，又称 Hessian 矩阵，定义为 $\nabla^2_{\vec{X}} f = \frac{\partial^2 f}{\partial \vec{X}^2} = \frac{\partial \nabla_{\vec{X}} f}{\partial \vec{X}}(mn\times mn)$，是对称矩阵。对向量 $\frac{\partial f}{\partial \vec{X}}$ 或矩阵 $\nabla_{\vec{X}} f$ 求导都可以得到 Hessian 矩阵，但从矩阵 $\nabla_{\vec{X}} f$ 出发更方便；&lt;/li&gt;
  &lt;li&gt;$\frac{\partial \vec{F}}{\partial \vec{X}} = \frac{\partial\mathrm{vec} (\vec{F})}{\partial \vec{X}} = \frac{\partial \vec{F}}{\partial \mathrm{vec}(\vec{X})} = \frac{\partial\mathrm{vec}(\vec{F})}{\partial \mathrm{vec}(\vec{X})}$，求导时矩阵被向量化，弊端是这在一定程度破坏了矩阵的结构，会导致结果变得形式复杂；好处是多元微积分中关于梯度、Hessian 矩阵的结论可以沿用过来，只需将矩阵向量化。例如优化问题中，牛顿法的更新 $\Delta \vec{X}$，满足 $\mathrm{vec}(\Delta \vec{X}) = -(\nabla^2_{\vec{X}} f)^{-1}\mathrm{vec}(\nabla_{\vec{X}} f)$；&lt;/li&gt;
  &lt;li&gt;在资料中，矩阵对矩阵的导数还有其它定义，比如 $\frac{\partial \vec{F}}{\partial \vec{X}} = \left&lt;a href=&quot;mp\times nq&quot;&gt;\frac{\partial \vec{F}_{kl}}{\partial \vec{X}}\right&lt;/a&gt;$，它能兼容上篇中的标量对矩阵导数的定义，但微分与导数的联系（$\mathrm{d}\vec{F}$ 等于 $\frac{\partial \vec{F}}{\partial \vec{X}}$ 中每个 $m\times n$ 子块分别与 $\mathrm{d}\vec{X}$ 做内积）不够简明，不便于计算和应用。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然后来建立运算法则。仍然要利用导数与微分的联系 $\mathrm{vec}(\mathrm{d}\vec{F}) = \frac{\partial \vec{F}}{\partial \vec{X}}^T \mathrm{vec}(\mathrm{d}\vec{X})$，求微分的方法与上篇相同，而从微分得到导数需要一些向量化的技巧：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;线性：$\mathrm{vec}(\vec{A}+\vec{B}) = \mathrm{vec}(\vec{A}) + \mathrm{vec}(\vec{B})$；&lt;/li&gt;
  &lt;li&gt;矩阵乘法：$\mathrm{vec}(\vec{AXB}) = (\vec{B}^T \otimes \vec{A}) \mathrm{vec}(\vec{X})$，其中 $\otimes$ 表示 Kronecker 积，$\vec{A}(m\times n)$ 与 $\vec{B}(p\times q)$ 的 Kronecker 积是 $\vec{A}\otimes \vec{B} = &lt;a href=&quot;mp\times nq&quot;&gt;\vec{A}_{ij}\vec{B}&lt;/a&gt;$。此式证明见张贤达《矩阵分析与应用》第 107-108 页；&lt;/li&gt;
  &lt;li&gt;转置：$\mathrm{vec}(\vec{A}^T) = \vec{K}&lt;em&gt;{mn}\mathrm{vec}(\vec{A})$，$\vec{A}$ 是 $m\times n$ 矩阵，其中 $\vec{K}&lt;/em&gt;{mn}(mn\times mn)$ 是交换矩阵(commutation matrix)；&lt;/li&gt;
  &lt;li&gt;逐元素乘法：$\mathrm{vec}(\vec{A}\odot \vec{X}) = \mathrm{diag}(\vec{A})\mathrm{vec}(\vec{X})$，其中 $\mathrm{diag}(\vec{A})(mn\times mn)$ 是用 $A$ 的元素（按列优先）排成的对角阵。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;观察一下可以断言，若矩阵函数 $\vec{F}$ 是矩阵 $\vec{X}$ 经加减乘法、行列式、逆、逐元素函数等运算构成，则使用相应的运算法则对 $\vec{F}$ 求微分，再做向量化并使用技巧将其它项交换至 $\mathrm{vec}(\mathrm{d}\vec{X})$  左侧，即能得到导数。&lt;/p&gt;

&lt;p&gt;再谈一谈复合：假设已求得 $\frac{\partial \vec{F}}{\partial \vec{Y}}$，而 $\vec{Y}$ 是 $\vec{X}$ 的函数，如何求 $\frac{\partial \vec{F}}{\partial \vec{X}}$ 呢？从导数与微分的联系入手，$\mathrm{vec}(\mathrm{d}\vec{F}) = \frac{\partial \vec{F}}{\partial Y}^T\mathrm{vec}(\mathrm{d}\vec{Y}) = \frac{\partial \vec{F}}{\partial \vec{Y}}^T\frac{\partial \vec{Y}}{\partial \vec{X}}^T\mathrm{vec}(\mathrm{d}\vec{X})$，可以推出链式法则 $\frac{\partial \vec{F}}{\partial \vec{X}} = \frac{\partial \vec{Y}}{\partial \vec{X}}\frac{\partial \vec{F}}{\partial \vec{Y}}$。&lt;/p&gt;

&lt;p&gt;和标量对矩阵的导数相比，矩阵对矩阵的导数形式更加复杂，从不同角度出发常会得到形式不同的结果。有一些 Kronecker 积和交换矩阵相关的恒等式，可用来做等价变形：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$(\vec{A}\otimes \vec{B})^T = \vec{A}^T \otimes \vec{B}^T$；&lt;/li&gt;
  &lt;li&gt;$\mathrm{vec}(\vec{ab}^T) = \vec{b}\otimes\vec{a}$；&lt;/li&gt;
  &lt;li&gt;$(\vec{A}\otimes \vec{B})(\vec{C}\otimes \vec{D}) = (\vec{AC})\otimes (\vec{BD})$。可以对 $\vec{F} = \vec{D}^T\vec{B}^T\vec{XAC}$ 求导来证明，一方面，直接求导得到 $\frac{\partial \vec{F}}{\partial \vec{X}} = (\vec{AC}) \otimes (\vec{BD})$；另一方面，引入 $\vec{Y} = \vec{B}^T\vec{XA}$，有 $\frac{\partial \vec{F}}{\partial \vec{Y}} = \vec{C} \otimes \vec{D}$，$\frac{\partial \vec{Y}}{\partial \vec{X}} = \vec{A} \otimes \vec{B}$，用链式法则得到 $\frac{\partial \vec{F}}{\partial \vec{X}} = (\vec{A}\otimes \vec{B})(\vec{C} \otimes \vec{D})$；&lt;/li&gt;
  &lt;li&gt;$\vec{K}&lt;em&gt;{mn} = \vec{K}&lt;/em&gt;{nm}^T, \vec{K}&lt;em&gt;{mn}\vec{K}&lt;/em&gt;{nm} = \vec{I}$；&lt;/li&gt;
  &lt;li&gt;$\vec{K}&lt;em&gt;{pm}(\vec{A}\otimes \vec{B}) \vec{K}&lt;/em&gt;{nq} = \vec{B}\otimes \vec{A}$，$\vec{A}$ 是 $m\times n$ 矩阵，$\vec{B}$ 是 $p\times q$ 矩阵。可以对 $\vec{AXB}^T$ 做向量化来证明，一方面，$\mathrm{vec}(\vec{AXB}^T) = (\vec{B}\otimes \vec{A})\mathrm{vec}(\vec{X})$；另一方面，$\mathrm{vec}(\vec{AXB}^T) = \vec{K}&lt;em&gt;{pm}\mathrm{vec}(\vec{BX}^T\vec{A}^T) = \vec{K}&lt;/em&gt;{pm}(\vec{A}\otimes \vec{B})\mathrm{vec}(\vec{X}^T) = \vec{K}&lt;em&gt;{pm}(\vec{A}\otimes \vec{B}) \vec{K}&lt;/em&gt;{nq}\mathrm{vec}(\vec{X})$。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;接下来演示一些算例。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;例1&lt;/strong&gt;：$\vec{F} = \vec{AX}$，$\vec{X}$ 是 $m\times n$ 矩阵，求 $\frac{\partial \vec{F}}{\partial \vec{X}}$。&lt;/p&gt;

&lt;p&gt;解：先求微分：$\mathrm{d}\vec{F}=\vec{A}\mathrm{d}\vec{X}$，再做向量化，使用矩阵乘法的技巧，注意在 $\mathrm{d}\vec{X}$ 右侧添加单位阵：$\mathrm{vec}(\mathrm{d}\vec{F}) = \mathrm{vec}(\vec{A}\mathrm{d}\vec{X}) = (\vec{I}_n\otimes \vec{A})\mathrm{vec}(\mathrm{d}\vec{X})$，对照导数与微分的联系得到 $\frac{\partial \vec{F}}{\partial \vec{X}} = \vec{I}_n\otimes \vec{A}^T$。&lt;/p&gt;

&lt;p&gt;特例：如果 $\vec{X}$ 退化为向量，$\vec{f} = \vec{A} \vec{x}$，则根据向量的导数与微分的关系 $\mathrm{d}\vec{f} = \frac{\partial \vec{f}}{\partial \vec{x}}^T \mathrm{d}\vec{x}$，得到  $\frac{\partial \vec{f}}{\partial \vec{x}} = \vec{A}^T$。&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;例2&lt;/strong&gt;：$\vec{f} = \log&lt;/td&gt;
      &lt;td&gt;\vec{X}&lt;/td&gt;
      &lt;td&gt;$，$\vec{X}$ 是 $n\times n$ 矩阵，求 $\nabla_{\vec{X}} \vec{f}$ 和 $\nabla^2_{\vec{X}} \vec{f}$。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;解：使用第一部分中的技术可求得 $\nabla_{\vec{X}} \vec{f} = \vec{X}^{-1T}$。为求 $\nabla^2_{\vec{X}} \vec{f}$，先求微分：$\mathrm{d}\nabla_{\vec{X}} \vec{f} = -(\vec{X}^{-1}\mathrm{d}\vec{XX}^{-1})^T$，再做向量化，使用转置和矩阵乘法的技巧
 $\mathrm{vec}(\mathrm{d}\nabla_{\vec{X}} \vec{f})= -\vec{K}&lt;em&gt;{nn}\mathrm{vec}(\vec{X}^{-1}\mathrm{d}\vec{XX}^{-1}) = -\vec{K}&lt;/em&gt;{nn}(\vec{X}^{-1T}\otimes \vec{X}^{-1})\mathrm{vec}(\mathrm{d}\vec{X})$，对照导数与微分的联系，得到 $\nabla^2_{\vec{X}} \vec{f} = -\vec{K}&lt;em&gt;{nn}(\vec{X}^{-1T}\otimes \vec{X}^{-1})$，注意它是对称矩阵。在 $\vec{X}$ 是对称矩阵时，可简化为 $\nabla^2&lt;/em&gt;{\vec{X}} \vec{f} = -\vec{X}^{-1}\otimes \vec{X}^{-1}$。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;例3&lt;/strong&gt;：$\vec{F} = \vec{A}\exp(\vec{XB})$，$\vec{A}$ 是 $l\times m$，$\vec{X}$ 是 $m\times n$，$\vec{B}$ 是 $n\times p$ 矩阵，$\exp()$ 为逐元素函数，求 $\frac{\partial \vec{F}}{\partial \vec{X}}$。&lt;/p&gt;

&lt;p&gt;解：先求微分：$\mathrm{d}\vec{F} = \vec{A}(\exp(\vec{XB})\odot (\mathrm{d}\vec{XB}))$，再做向量化，使用矩阵乘法的技巧：$\mathrm{vec}(\mathrm{d}\vec{F}) = (\vec{I}_p\otimes \vec{A})\mathrm{vec}(\exp(\vec{XB})\odot (\mathrm{d}\vec{XB}))$，再用逐元素乘法的技巧：$\mathrm{vec}(\mathrm{d}\vec{F}) = (\vec{I}_p \otimes \vec{A}) \mathrm{diag}(\exp(\vec{XB}))\mathrm{vec}(\mathrm{d}\vec{XB})$，再用矩阵乘法的技巧：$\mathrm{vec}(\mathrm{d}\vec{F}) = (\vec{I}_p\otimes \vec{A})\mathrm{diag}(\exp(\vec{XB}))(\vec{B}^T\otimes I_m)\mathrm{vec}(\mathrm{d}\vec{X})$，对照导数与微分的联系得到 $\frac{\partial \vec{F}}{\partial \vec{X}} = (\vec{B}\otimes \vec{I}_m)\mathrm{diag}(\exp(\vec{XB}))(\vec{I}_p\otimes \vec{A}^T)$。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;例4【一元 logistic 回归】&lt;/strong&gt;：$l = -y \vec{x}^T \vec{w} + \log(1 + \exp(\vec{x}^T\vec{w}))$，求 $\nabla_{\vec{w}} l$ 和 $\nabla_{\vec{w}}^2 l$。其中 $y$ 是取值 0 或 1 的标量，$\vec{x}$ 和 $\vec{w}$ 是向量。&lt;/p&gt;

&lt;p&gt;解：使用上篇中的技术可求得 $\nabla_{\vec{w}} l = \vec{x}(\sigma(\vec{x}^T\vec{w}) - y)$，其中 $\sigma(a) = \frac{\exp(a)}{1+\exp(a)}$ 为 sigmoid 函数。为求 $\nabla_{\vec{w}}^2 l$，先求微分： $\mathrm{d}\nabla_{\vec{w}} l = \vec{x} \sigma’(\vec{x}^T\vec{w})\vec{x}^T \mathrm{d}\vec{w}$，其中 $\sigma’(a) = \frac{\exp(a)}{(1+\exp(a))^2}$ 为 sigmoid 函数的导数，对照导数与微分的联系，得到 $\nabla_{\vec{w}}^2 l = \vec{x}\sigma’(\vec{x}^T\vec{w})\vec{x}^T$。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;推广&lt;/strong&gt;：样本( $\vec{x}&lt;em&gt;1, y_1), \dots, (\vec{x}_n,y_n)$，$l = \sum&lt;/em&gt;{i=1}^N \left(-y_i \vec{x}&lt;em&gt;i^T\vec{w} + \log(1+\exp(\vec{x_i}^T\vec{w}))\right)$，求 $\nabla&lt;/em&gt;{\vec{w}} l$ 和 $\nabla_{\vec{w}}^2 l$。 有两种方法，方法一：先对每个样本求导，然后相加；方法二：定义矩阵 $\vec{X} = \begin{bmatrix}\vec{x}&lt;em&gt;1^T \ \vdots \ \vec{x}_n^T \end{bmatrix}$，向量 $\vec{y} = \begin{bmatrix}y_1 \ \vdots \ y_n\end{bmatrix}$，将 $l$ 写成矩阵形式 $l = -\vec{y}^T \vec{X}\vec{w} + \vec{1}^T\log(\vec{1} + \exp(\vec{X}\vec{w}))$，进而可以求得
 $\nabla&lt;/em&gt;{\vec{w}} l = \vec{X}^T(\sigma(\vec{X}\vec{w}) - \vec{y})$，$\nabla_{\vec{w}}^2 l = \vec{X}^T\text{diag}(\sigma’(\vec{X}\vec{w}))\vec{X}$。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;例5【多元 logistic 回归】&lt;/strong&gt;：$l = -\vec{y}^T\log \mathrm{softmax}(\vec{Wx})=&lt;script type=&quot;math/tex&quot;&gt;-\vec{y}^T\vec{Wx}+&lt;/script&gt;\log(\vec{1}^T\exp(\vec{Wx}))$，求 $\nabla_{\vec{W}} l$ 和 $\nabla_{\vec{W}}^2 l$。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;解&lt;/strong&gt;：上篇例 3 中已求得 $\nabla_{\vec{W}} l = (\mathrm{softmax}(\vec{Wx})-\vec{y})\vec{x}^T$。为求 $\nabla_{\vec{W}}^2 l$，先求微分：定义 $\vec{a} = \vec{Wx}，\mathrm{d}\mathrm{softmax}(\vec{a}) = \frac{\exp(\vec{a})\odot \mathrm{d}\vec{a}}{\vec{1}^T\exp(\vec{a})} - \frac{\exp(\vec{a}) (\vec{1}^T(\exp(\vec{a})\odot \mathrm{d}\vec{a}))}{(\vec{1}^T\exp(\vec{a}))^2}$，这里需要化简去掉逐元素乘法，第一项中 $\exp(\vec{a})\odot \mathrm{d}\vec{a} = \mathrm{diag}(\exp(\vec{a})) \mathrm{d}\vec{a} $，第二项中 $\vec{1}^T(\exp(\vec{a})\odot \mathrm{d}\vec{a}) = \exp(\vec{a})^T\mathrm{d}\vec{a}$，故有 $\mathrm{d}\mathrm{softmax}(\vec{a}) = \mathrm{softmax}’(\vec{a})\mathrm{d}\vec{a}$，其中
 $\mathrm{softmax}’(\vec{a}) = \frac{\mathrm{diag}(\exp(\vec{a}))}{\vec{1}^T\exp(\vec{a})} - \frac{\exp(\vec{a})\exp(\vec{a})^T}{(\vec{1}^T\exp(\vec{a}))^2} $，代入有 $\mathrm{d}\nabla_{\vec{W}} l = \mathrm{softmax}’(\vec{a})\mathrm{d}\vec{a}\vec{x}^T = \mathrm{softmax}’(\vec{Wx})\mathrm{d}\vec{Wx}\vec{x}^T$，做向量化并使用矩阵乘法的技巧，得到 $\nabla_{\vec{W}}^2 l = (\vec{x}\vec{x}^T) \otimes \mathrm{softmax}’(\vec{Wx})$。&lt;/p&gt;

&lt;p&gt;最后做个总结。我们发展了从整体出发的矩阵求导的技术，导数与微分的联系是计算的枢纽，标量对矩阵的导数与微分的联系是 $\mathrm{d}f = \mathrm{tr}(\nabla_{\vec{X}}^T f \mathrm{d}\vec{X})$，先对 $f$ 求微分，再使用迹技巧可求得导数，特别地，标量对向量的导数与微分的联系是 $\mathrm{d}f = \nabla_{\vec{x}}f^T \mathrm{d}\vec{x}$；矩阵对矩阵的导数与微分的联系是 $\mathrm{vec}(\mathrm{d}\vec{F}) = \frac{\partial \vec{F}}{\partial \vec{X}}^T \mathrm{vec}(\mathrm{d}\vec{X})$，先对 $\vec{F}$  求微分，再使用向量化的技巧可求得导数，特别地，向量对向量的导数与微分的联系是 $\mathrm{d}\vec{f} = \frac{\partial \vec{f}}{\partial \vec{x}}^T\mathrm{d}\vec{x}$。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;4 反向传播算法的完整向量形式推导&lt;/h2&gt;

&lt;p&gt;首先定义一些常用的变量：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$n^l$：第 $l$ 层的神经元个数；&lt;/li&gt;
  &lt;li&gt;$f_l(\cdot)$：第 $l$ 层的激活函数；&lt;/li&gt;
  &lt;li&gt;$\vec{W}^l\in \mathbb{R}^{n^l\times n^{l-1}}$：第 $l-1$ 层到第 $l$ 层的权重矩阵；&lt;/li&gt;
  &lt;li&gt;$\vec{b}^l\in \mathbb{R}^{n^l}$：第 $l-1$ 层到第 $l$ 层的偏置；&lt;/li&gt;
  &lt;li&gt;$\vec{z}^l\in \mathbb{R}^{n^l}$：第 $l$ 层的输入；&lt;/li&gt;
  &lt;li&gt;$\vec{a}^l\in \mathbb{R}^{n^l}$：第 $l$ 层的输出。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在每一层的正向传播过程计算过程如下所示：
$$
\begin{align}&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;\vec{z}^l &amp;amp;= \vec{W}^l\vec{a}^{(l-1)} + \vec{b}^l \\
\vec{a}^l &amp;amp;= f_l(\vec{z}^l) \end{align} $$ 为进行梯度计算，我们需要先写出目标函数： $$ \begin{equation} J(\vec{W},\vec{b})=\sum_{i=1}^{N}J(\vec{W},\vec{b};\vec{x}^i,y^i)+\frac{1}{2}\lambda||\vec{W}||_F^2 \end{equation} $$ 其中 $||\vec{W}||_F^2=\sum_{l=1}^L\sum_{j=1}^{n^{l+1}}\sum_{i=1}^{n^l}W_{ij}^l$。那么对 $\vec{W}$ 和 $\vec{b}$ 的更新公式理论上可以写成： $$ \begin{align}
\vec{W}^l &amp;amp;= \vec{W}^l-\alpha\frac{\partial J(\vec{W},\vec{b})}{\partial \vec{W}^l} \\
          &amp;amp;= \vec{W}^l-\alpha\bigg( \sum_{i=1}^N \frac{\partial J(\vec{W},\vec{b};\vec{x}^i,y^i)}{\partial \vec{W}^l} + \lambda\vec{W}^l \bigg) \label{eq1:dw}\\
\vec{b}^l &amp;amp;= \vec{b}^l-\alpha\frac{\partial J(\vec{W},\vec{b})}{\partial \vec{b}^l} \\
          &amp;amp;= \vec{b}^l-\alpha \sum_{i=1}^N \frac{\partial J(\vec{W},\vec{b};\vec{x}^i,y^i)}{\partial \vec{b}^l} \label{eq1:db} \end{align} $$ 我们考虑每一个单独样本的梯度，先假设 $\frac{\partial J(\vec{W},\vec{b};\vec{x},y)}{\partial \vec{z}^l}=\vec{\delta}^l$，根据第一部分矩阵导数与微分的联系，有 $\mathrm{d}J=\mathrm{tr}\bigg(\vec{\delta}^{lT}\mathrm{d}\vec{z}^l\bigg)$，而 $\vec{z}^l = \vec{W}^l\vec{a}^{(l-1)} + \vec{b}^l$，所以 $\mathrm{d}\vec{z}^l = \mathrm{d}\vec{W}^l\vec{a}^{(l-1)}$，因此， $$ \begin{align}
\mathrm{d}J &amp;amp;= \mathrm{tr}\bigg( \vec{\delta}^{lT}\mathrm{d}\vec{W}^l\vec{a}^{(l-1)} \bigg)\\
            &amp;amp;= \mathrm{tr}\bigg( \vec{a}^{(l-1)}\vec{\delta}^{lT}\mathrm{d}\vec{W}^l \bigg) \end{align} $$ 所以最后有 $$ \begin{equation}
\frac{\partial J(\vec{W},\vec{b};\vec{x},y)}{\partial \vec{W}^l}=\vec{\delta}^l\vec{a}^{(l-1)T}. \label{eq2:dw} \end{equation} $$ 同理有 $$ \begin{equation}
\frac{\partial J(\vec{W},\vec{b};\vec{x},y)}{\partial \vec{b}^l}=\vec{\delta}^l. \label{eq2:db} \end{equation} $$ 下面求 $\vec{\delta}^l$，假设已知 $\frac{\partial J(\vec{W},\vec{b};\vec{x},y)}{\partial \vec{z}^{(l+1)}}=\vec{\delta}^{(l+1)}$，则有 $\mathrm{d}J=\mathrm{tr}\bigg(\vec{\delta}^{(l+1)T}\mathrm{d}\vec{z}^{(l+1)}\bigg)$，而 $\vec{z}^{(l+1)} = \vec{W}^{(l+1)}\vec{a}^l + \vec{b}^{(l+1)}$，所以 $\mathrm{d}\vec{z}^{(l+1)} = \vec{W}^{(l+1)} \mathrm{d}\vec{a}^l$，又因为 $\vec{a}^l=f_l(\vec{z}^l)$ 是逐元素的，所以 $\mathrm{d}\vec{a}^l=\mathrm{diag}(f&#39;_l(\vec{z}^l))\mathrm{d}\vec{z}^l$，代入后有 $$ \begin{equation}
\mathrm{d}J=\mathrm{tr}\bigg( \vec{\delta}^{(l+1)T}\vec{W}^{(l+1)}\mathrm{diag}(f&#39;_l(\vec{z}^l))\mathrm{d}\vec{z}^l \bigg). \end{equation} $$ 所以有 $$ \begin{equation}
\vec{\delta}^l=\frac{\partial J(\vec{W},\vec{b};\vec{x},y)}{\partial \vec{z}^l}=\mathrm{diag}(f&#39;_l(\vec{z}^l))\vec{W}^{(l+1)T}\vec{\delta}^{(l+1)}. \label{eq2:delta} \end{equation} $$ 其中当 $f(\vec{x})=\mathrm{sigmoid}(\vec{x})$ 时，有 $f&#39;(\vec{x})=f(\vec{x})(1-f(\vec{x}))$，当 $f(\vec{x})=\mathrm{tanh}(\vec{x})$ 时，有 $f&#39;(\vec{x})=1-f(\vec{x})^2$。
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;最后考虑最后一层的 $\vec{\delta}^{(l+1)}$。当使用交叉熵损失函数时（以下 $\vec{a}$ 和 $\vec{z}$ 都略去上标 $(l+1)$）
&lt;script type=&quot;math/tex&quot;&gt;\begin{equation}
    J(\vec{W},\vec{b};\vec{x},y)=-\vec{y}^T\ln\vec{a}-(\vec{1}-\vec{y})^T\ln(1-\vec{a}).
\end{equation}&lt;/script&gt;
这时
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
    \mathrm{d}J &amp;= -\vec{y}^T\bigg(\frac{1}{\vec{a}} \odot \mathrm{d}\vec{a}\bigg)-(\vec{1}-\vec{y})^T\bigg(\frac{-1}{\vec{1}-\vec{a}}\odot \mathrm{d}\vec{a}\bigg) \\
                &amp;= -\bigg(\vec{y} \odot \frac{1}{\vec{a}} \bigg)^T \mathrm{d}\vec{a} + \bigg( (\vec{1}-\vec{y})\odot\frac{1}{\vec{1}-\vec{a}}\bigg)^T \mathrm{d}\vec{a} \\
                &amp;= \bigg(-\vec{y} \odot \frac{1}{\vec{a}} + (\vec{1}-\vec{y})\odot\frac{1}{\vec{1}-\vec{a}}\bigg)^T \mathrm{d}\vec{a}
\end{align} %]]&gt;&lt;/script&gt;
又因为 $\mathrm{d}\vec{a}=\mathrm{diag}(\mathrm{sigmoid}’(\vec{z}))\mathrm{d}\vec{z}=\mathrm{diag}(\vec{a(1-a)})\mathrm{d}\vec{z}​$，代入上式有
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
    \mathrm{d}J &amp;= \bigg(-\vec{y} \odot \frac{1}{\vec{a}} + (\vec{1-y})\odot\frac{1}{\vec{1-a}}\bigg)^T \mathrm{diag}(\vec{a(1-a)})\mathrm{d}\vec{z} \\
                &amp;= \bigg(-\mathrm{diag}(\vec{a(1-a)}) \vec{y} \odot \frac{1}{\vec{a}} + \mathrm{diag}(\vec{a(1-a)})(\vec{1-y})\odot\frac{1}{\vec{1-a}}\bigg)^T \mathrm{d}\vec{z} \\
                &amp;= (-\vec{y}+\vec{y}\odot\vec{a} + \vec{a} - \vec{a}\odot\vec{y})^T \mathrm{d}\vec{z} \\
                &amp;= (-\vec{y}+\vec{a})^T \mathrm{d}\vec{z} 
\end{align} %]]&gt;&lt;/script&gt;
所以有
&lt;script type=&quot;math/tex&quot;&gt;\begin{equation}
    \vec{\delta}^{(l+1)}=\frac{\partial J(\vec{W},\vec{b};\vec{x},y)}{\partial \vec{z}^{(l+1)}}=-\vec{y}+\vec{a}^{(l+1)}. \label{eq:delta}
\end{equation}&lt;/script&gt;
最后，将式 \ref{eq:delta} 和式 \ref{eq2:db} 代入式 \ref{eq1:db} 则得到偏置项 $\vec{b}$ 的更新公式，将式 \ref{eq:delta}，式 \ref{eq2:delta} 和式 \ref{eq2:dw} 代入式 \ref{eq1:dw} 则得到 $\vec{W}$ 的更新公式。&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Dec 2017 06:08:54 +0800</pubDate>
        <link>/2017/12/19/test/</link>
        <guid isPermaLink="true">/2017/12/19/test/</guid>
        
        
        <category>算法</category>
        
      </item>
    
      <item>
        <title>GitHub Pages上输出数学公式</title>
        <description>
&lt;p&gt;由于MathJax没有专门给Jekyll配备插件，所以，需要使用常规的JavaScript方法来调用。
首先，在”head.html”这个文件的&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;head&amp;gt;&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;/head&amp;gt;&lt;/code&gt;中添加JS链接：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;script type=&quot;text/javascript&quot;  
src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&amp;gt;
&amp;lt;/script&amp;gt;  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;其次，将Jekyll解析器改为Kramdown，以支持Latex标记。（在_config.yml文件中修改）
如此，配置部分结束。
文章撰写时，使用双$符号标记Latex语言。前后皆有双$。
如果公式（包括双$）前后各有一个空行，则产生换行居中的公式。&lt;/p&gt;

&lt;p&gt;比如如下代码&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$$\sum_{n=1}^{\infty}1/n^2=\frac{\pi^2}{6}$$
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;将产生的效果如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{n=1}^{\infty}1/n^2=\frac{\pi^2}{6}&lt;/script&gt;

&lt;p&gt;原文链接 &lt;a href=&quot;http://www.anaharb.com/2014/0215/Jekyll-MathJax/&quot;&gt;http://www.anaharb.com/2014/0215/Jekyll-MathJax/&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 20 Mar 2016 06:14:54 +0800</pubDate>
        <link>/2016/03/20/GitHub-Pages%E4%B8%8A%E8%BE%93%E5%87%BA%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/</link>
        <guid isPermaLink="true">/2016/03/20/GitHub-Pages%E4%B8%8A%E8%BE%93%E5%87%BA%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/</guid>
        
        
        <category>HelloWorld</category>
        
      </item>
    
      <item>
        <title>MCMC方法(一):贝叶斯推断</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;(1)贝叶斯统计简史&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;(2)条件概率和贝叶斯公式&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-2&quot; id=&quot;markdown-toc-section-2&quot;&gt;(3)全概率公式&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-3&quot; id=&quot;markdown-toc-section-3&quot;&gt;(4)贝叶斯推断&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section&quot;&gt;(1)贝叶斯统计简史&lt;/h3&gt;
&lt;p&gt;贝叶斯统计缘起于托马斯.贝叶斯（Tomas Bayes, 1702-1761），一位英国长老会牧师和业余数学家。在他去世后才发表的论文“论有关机遇问题的求解”中提出了现在贝叶斯统计的基本思想，但是贝叶斯定理的现代形式实际上归因于拉普拉斯在1812的工作，拉普拉斯重新发现了贝叶斯定理，并把它用来解决天体力学、医学甚至法学的问题。但自19世纪中叶起，随着频率学派（或者称作经典统计学派）的兴起，贝叶斯解释逐渐被统计学主流所拒绝。&lt;/p&gt;

&lt;p&gt;现代贝叶斯统计学的复兴肇始于Jeffreys(1939)，从1950年代开始，经过众多统计学家的努力，贝叶斯统计学逐渐发展壮大，并发展出了贝叶斯统计决策理论这个新分支。特别到1990年代以后，随着计算方法MCMC在贝叶斯统计领域的广泛应用，解决了贝叶斯统计学长期存在的计算困难的问题，贝叶斯统计学这才迎来了它的春天。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-1&quot;&gt;(2)条件概率和贝叶斯公式&lt;/h3&gt;
&lt;p&gt;由条件概率公式&lt;script type=&quot;math/tex&quot;&gt;P(A\cap B)=P(A|B)P(B)=P(B|A)P(A)&lt;/script&gt;，故而有贝叶斯公式:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A|B)= {P(A\cap B)\over P(B)}={P(B|A)P(A)\over P(B)}&lt;/script&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-2&quot;&gt;(3)全概率公式&lt;/h3&gt;
&lt;p&gt;假设事件&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;A&#39;&lt;/script&gt;共同构成了样本空间，那么就有&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(B)=P(B \cap A)+P(B \cap A&#39;)&lt;/script&gt;

&lt;p&gt;而通过上一节的推导，我们有&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(B \cap A)=P(B|A)P(A)&lt;/script&gt;

&lt;p&gt;所以，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(B)=P(B|A)P(A)+P(B|A&#39;)P(A&#39;)&lt;/script&gt;

&lt;p&gt;这就是全概率公式。&lt;/p&gt;

&lt;p&gt;将这个公式带入上一节的贝叶斯公式，就得到贝叶斯公式的另一种写法：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A|B)={P(B|A)P(A)\over P(B|A)P(A)+P(B|A&#39;)P(A&#39;)}&lt;/script&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-3&quot;&gt;(4)贝叶斯推断&lt;/h3&gt;
&lt;p&gt;上面介绍的就是贝叶斯公式的最简单表达了，复杂的无非也就是求和换成积分，原理都是一样的。通常将&lt;script type=&quot;math/tex&quot;&gt;P(A|B)&lt;/script&gt;称为后验概率(posterior probability)，将&lt;script type=&quot;math/tex&quot;&gt;P(A)&lt;/script&gt;称为先验概率(prior probability)，将&lt;script type=&quot;math/tex&quot;&gt;P(B|A)P(B)&lt;/script&gt;称为可能性函数(likelyhood function)。&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Sun, 20 Mar 2016 06:08:54 +0800</pubDate>
        <link>/2016/03/20/MCMC(1)-Bayes/</link>
        <guid isPermaLink="true">/2016/03/20/MCMC(1)-Bayes/</guid>
        
        
        <category>MCMC</category>
        
      </item>
    
      <item>
        <title>Hello World</title>
        <description>
&lt;p&gt;1988年来到这个世界，则象牙塔里待了21年，2015年一头扎进滚滚红尘&lt;/p&gt;

&lt;p&gt;20岁之前以为自己会成为一个牛叉的理论物理学家，于是学习数学&lt;/p&gt;

&lt;p&gt;然后发现有些事不是勤奋就能做到的&lt;/p&gt;

&lt;p&gt;于是开始数学应用&lt;/p&gt;

&lt;p&gt;世界上的事一部分是可以用数学模型解释的，另一部分则不可以&lt;/p&gt;

&lt;p&gt;我喜欢前者的纯粹，思考后者的复杂&lt;/p&gt;
</description>
        <pubDate>Sat, 19 Mar 2016 06:14:54 +0800</pubDate>
        <link>/2016/03/19/hello-world/</link>
        <guid isPermaLink="true">/2016/03/19/hello-world/</guid>
        
        
        <category>HelloWorld</category>
        
      </item>
    
  </channel>
</rss>
