---
layout: post
title:  矩阵求导
date:   2017-12-18 22:08:54
author: caoxiaoqing
categories: 算法
tags: 矩阵 求导
---

* content
{:toc}

矩阵求导的技术，在统计学、控制论、机器学习等领域有广泛的应用。本文来做个科普，分作两部分，第一部分讲标量对矩阵的求导术，第二部分讲矩阵对矩阵的求导术。本文使用小写字母 $$x$$ 表示标量，粗体小写字母 $$\boldsymbol{x}$$ 表示向量，大写字母 $$\boldsymbol{X}$$ 表示矩阵。本文第一部分标量对矩阵的求导整理自：[这里](https://zhuanlan.zhihu.com/p/24709748)，第二部分矩阵对矩阵的求导整理自：[那里](https://zhuanlan.zhihu.com/p/24863977) ，第三部分则是根据以上两部分提供的方法，完成了 BP 算法的完整向量形式推导。



## 1 标量对矩阵的求导

首先来琢磨一下定义，标量 $$f$$ 对矩阵 $$\boldsymbol{X}$$ 的导数，定义为

$$
\frac{\partial f}{\partial \boldsymbol{X}} = \left[\frac{\partial f }{\partial \boldsymbol{X}_{ij}}\right].
$$

即 $$f$$ 对 $$\boldsymbol{X}$$ 逐元素求导排成与 $$\boldsymbol{X}$$ 尺寸相同的矩阵。然而，这个定义在计算中并不好用，实用上的原因是在对较复杂的函数难以逐元素求导；哲理上的原因是逐元素求导破坏了整体性。试想，为何要将 $$f$$ 看做矩阵 $$\boldsymbol{X}$$ 而不是各元素 $$\boldsymbol{X}_{ij}$$ 的函数呢？答案是用矩阵运算更整洁。所以在求导时不宜拆开矩阵，而是要找一个从整体出发的算法。为此，我们来回顾，一元微积分中的导数（标量对标量的导数）与微分有联系：

$$
\mathrm{d}f = f'(x)\mathrm{d}x.
$$

多元微积分中的梯度（标量对向量的导数）也与微分有联系：

$$
\begin{equation}
  \mathrm{d}f = \sum_{i} \frac{\partial f}{\partial x_i}\mathrm{d}x_i = \frac{\partial f}{\partial \boldsymbol{x}}^T \mathrm{d}\boldsymbol{x}.
\end{equation}
$$

这里第一个等号是全微分公式，第二个等号表达了梯度 $$\frac{\partial f}{\partial \boldsymbol{x}}$$ 与微分的联系。受此启发，我们将矩阵导数与微分建立联系：

$$
\begin{equation}
  \mathrm{d}f = \sum_{i,j} \frac{\partial f}{\partial \boldsymbol{X}{ij}}\mathrm{d}\boldsymbol{X}_{ij} = \mathrm{tr}\left(\frac{\partial f}{\partial \boldsymbol{X}}^T \mathrm{d}\boldsymbol{X}\right).
\end{equation}
$$

这里 $$\mathrm{tr}$$ 代表迹(trace)是方阵对角线元素之和，满足性质：对尺寸相同的矩阵 $$\boldsymbol{A}, \boldsymbol{B}$$，有 $$\mathrm{tr}(\boldsymbol{A}^T\boldsymbol{B}) = \sum_{i,j}\boldsymbol{A}_{ij}\boldsymbol{B}_{ij}$$，即 $$\mathrm{tr}(\boldsymbol{A}^T\boldsymbol{B})$$ 是矩阵 $$\boldsymbol{A}, \boldsymbol{B}$$ 的内积，因此上式与原定义相容。

然后来建立运算法则。回想遇到较复杂的一元函数如 $$f = \log(2+\sin x)e^{\sqrt{x}}$$，我们是如何求导的呢？通常不是从定义开始求极限，而是先建立了初等函数求导和四则运算、复合等法则，再来运用这些法则。故而，我们来创立常用的矩阵微分的运算法则：

- 加减法：$$\mathrm{d}(\boldsymbol{X} \pm \boldsymbol{Y}) = \mathrm{d}\boldsymbol{X} \pm \mathrm{d}\boldsymbol{Y}$$；
- 矩阵乘法：$$\mathrm{d}(\boldsymbol{X}\boldsymbol{Y}) = \mathrm{d}\boldsymbol{X}\boldsymbol{Y} + \boldsymbol{X}\mathrm{d}\boldsymbol{Y}$$；
- 转置：$$\mathrm{d}(\boldsymbol{X}^T) = (\mathrm{d}\boldsymbol{X})^T$$；
- 迹：$$\mathrm{d}\mathrm{tr}(\boldsymbol{X}) = \mathrm{tr}(\mathrm{d}\boldsymbol{X})$$；
- 逆：$$\mathrm{d}\boldsymbol{X}^{-1} = -\boldsymbol{X}^{-1}\mathrm{d}\boldsymbol{X}\boldsymbol{X}^{-1}$$，此式可在 $$\boldsymbol{X}\boldsymbol{X}^{-1}=\boldsymbol{I}$$ 两侧求微分来证明；
- 行列式：$$\mathrm{d}\lvert\boldsymbol{X}\rvert = \mathrm{tr}(\boldsymbol{X}^{\#}\mathrm{d}\boldsymbol{X})$$，其中 $$\boldsymbol{X}^{\#}$$ 表示 $$\boldsymbol{X}$$ 的伴随矩阵，在 $$\boldsymbol{X}$$ 可逆时又可以写作 $$\mathrm{d}\lvert\boldsymbol{X}\rvert= \lvert\boldsymbol{X}\rvert\mathrm{tr}(\boldsymbol{X}^{-1}\mathrm{d}\boldsymbol{X})$$，此式可用Laplace展开来证明，详见张贤达《矩阵分析与应用》第279页；
- 逐元素乘法：$$\mathrm{d}(\boldsymbol{X}\odot \boldsymbol{Y}) = \mathrm{d}\boldsymbol{X}\odot \boldsymbol{Y} + \boldsymbol{X}\odot \mathrm{d}\boldsymbol{Y}$$，$$\odot$$ 表示尺寸相同的矩阵 $$\boldsymbol{X},\boldsymbol{Y}$$ 逐元素相乘；
- 逐元素函数：$$\mathrm{d}\sigma(\boldsymbol{X}) = \sigma'(\boldsymbol{X})\odot \mathrm{d}\boldsymbol{X}$$，$$\sigma(\boldsymbol{X}) = \left[\sigma(\boldsymbol{X}_{ij})\right]$$ 是逐元素运算的标量函数。

我们试图利用矩阵导数与微分的联系 $$\mathrm{d}f = \mathrm{tr}\left(\frac{\partial f}{\partial \boldsymbol{X}}^T \mathrm{d}\boldsymbol{X}\right)$$，在求出左侧的微分 $$\mathrm{d}f$$ 后，该如何写成右侧的形式并得到导数呢？这需要一些迹技巧(trace trick)：

- 标量套上迹：$$a = \mathrm{tr}(a)$$；
- 转置：$$\mathrm{tr}(\boldsymbol{A}^T) = \mathrm{tr}(\boldsymbol{A})$$；
- 线性：$$\mathrm{tr}(\boldsymbol{A}\pm \boldsymbol{B}) = \mathrm{tr}(\boldsymbol{A})\pm \mathrm{tr}(\boldsymbol{B})$$；
- 矩阵乘法交换：$$\mathrm{tr}(\boldsymbol{AB}) = \mathrm{tr}(\boldsymbol{BA})$$，两侧都等于 $$\sum_{i,j}\boldsymbol{A}_{ij}\boldsymbol{B}_{ji}$$；
- 矩阵乘法/逐元素乘法交换：$$\mathrm{tr}(\boldsymbol{A}^T(\boldsymbol{B}\odot \boldsymbol{C})) = \mathrm{tr}((\boldsymbol{A}\odot \boldsymbol{B})^T\boldsymbol{C})$$，两侧都等于 $$\sum_{i,j}\boldsymbol{A}_{ij}\boldsymbol{B}_{ij}\boldsymbol{C}_{ij}$$。

观察一下可以断言，若标量函数 $$f$$ 是矩阵 $$\boldsymbol{X}$$ 经加减乘法、行列式、逆、逐元素函数等运算构成，则使用相应的运算法则对 $$f$$ 求微分，再使用迹技巧给 $$\mathrm{d}f$$ 套上迹并将其它项交换至 $$\mathrm{d}\boldsymbol{X}$$ 左侧，即能得到导数。

在建立法则的最后，来谈一谈复合：假设已求得 $$\frac{\partial f}{\partial \boldsymbol{Y}}$$，而 $$\boldsymbol{Y}$$ 是 $$\boldsymbol{X}$$ 的函数，如何求 $$\frac{\partial f}{\partial \boldsymbol{X}}$$ 呢？在微积分中有标量求导的链式法则 $$\frac{\partial f}{\partial x} = \frac{\partial f}{\partial y} \frac{\partial y}{\partial x}$$，但这里我们不能沿用链式法则，因为矩阵对矩阵的导数 $$\frac{\partial \boldsymbol{Y}}{\partial \boldsymbol{X}}$$ 截至目前仍是未定义的。于是我们继续追本溯源，链式法则是从何而来？源头仍然是微分。我们直接从微分入手建立复合法则：先写出 $$\mathrm{d}f = \mathrm{tr}\left(\frac{\partial f}{\partial \boldsymbol{Y}}^T \mathrm{d}\boldsymbol{Y}\right)$$，再将 $$\mathrm{d}\mathbf{Y}$$ 用 $$\mathrm{d}\boldsymbol{X}$$ 表示出来代入，并使用迹技巧将其他项交换至 $$\mathrm{d}\boldsymbol{X}$$ 左侧，即可得到 $$\frac{\partial f}{\partial \boldsymbol{X}}$$。

接下来演示一些算例。特别提醒要依据已经建立的运算法则来计算，不能随意套用微积分中标量导数的结论，比如认为 $$\boldsymbol{A}\boldsymbol{X}$$ 对 $$\boldsymbol{X}$$ 的导数为 $$\boldsymbol{A}$$，这是没有根据、意义不明的。


**例1**：$$f = \boldsymbol{a}^T \boldsymbol{X}\boldsymbol{b}$$，求 $$\frac{\partial f}{\partial \boldsymbol{X}}$$。

解：先使用矩阵乘法法则求微分：$$\mathrm{d}f = \boldsymbol{a}^T \mathrm{d}\boldsymbol{X}\boldsymbol{b}$$，再套上迹并做交换：$$\mathrm{d}f = \mathrm{tr}(\boldsymbol{a}^T\mathrm{d}\boldsymbol{X}\boldsymbol{b}) = \mathrm{tr}(\boldsymbol{b}\boldsymbol{a}^T\mathrm{d}\boldsymbol{X})$$，对照导数与微分的联系，得到 $$\frac{\partial f}{\partial \boldsymbol{X}} = \boldsymbol{a}\boldsymbol{b}^T$$。

注意：这里不能用 $$\frac{\partial f}{\partial \boldsymbol{X}} =\boldsymbol{a}^T \frac{\partial \boldsymbol{X}}{\partial \boldsymbol{X}}\boldsymbol{b}=?$$，导数与乘常数矩阵的交换是不合法则的运算（而微分是合法的）。有些资料在计算矩阵导数时，会略过求微分这一步，这是逻辑上解释不通的。


**例2【线性回归】**：$$l = \|\boldsymbol{X}\boldsymbol{w}- \boldsymbol{y}\|^2$$，求 $$\frac{\partial l}{\partial \boldsymbol{w}}$$。

解：严格来说这是标量对向量的导数，不过可以把向量看做矩阵的特例。将向量范数写成 $$l = (\boldsymbol{X}\boldsymbol{w}- \boldsymbol{y})^T(\boldsymbol{X}\boldsymbol{w}- \boldsymbol{y})$$，求微分，使用矩阵乘法、转置等法则：$$\mathrm{d}l = (\boldsymbol{X}\mathrm{d}\boldsymbol{w})^T(\boldsymbol{X}\boldsymbol{w}-\boldsymbol{y})+(\boldsymbol{X}\boldsymbol{w}-\boldsymbol{y})^T(\boldsymbol{X}\mathrm{d}\boldsymbol{w}) = 2(\boldsymbol{X}\boldsymbol{w}-\boldsymbol{y})^T\boldsymbol{X}\mathrm{d}\boldsymbol{w}$$。 对照导数与微分的联系，得到 $$\frac{\partial l}{\partial \boldsymbol{w}}= 2\boldsymbol{X}^T(\boldsymbol{X}\boldsymbol{w}-\boldsymbol{y})$$。


**例3【多元 logistic 回归】**：$$l = -\boldsymbol{y}^T\log \mathrm{softmax}(\boldsymbol{Wx})$$$，求 $$\frac{\partial l}{\partial \boldsymbol{W}}$$。其中 $$\boldsymbol{y}$$ 是除一个元素为 1 外其它元素为 0 的向量；$$\mathrm{softmax}(\boldsymbol{a}) = \frac{\exp(\boldsymbol{a})}{\boldsymbol{1}^T\exp(\boldsymbol{a})}$$，其中 $$\exp(\boldsymbol{a})$$ 表示逐元素求指数，$$\boldsymbol{1}$$ 代表全 1 向量。

解：首先将 softmax 函数代入并写成 $$l = -\boldsymbol{y}^T \left(\log (\exp(\boldsymbol{Wx}))-\boldsymbol{1}\log(\boldsymbol{1}^T\exp(\boldsymbol{Wx}))\right) = -\boldsymbol{y}^T\boldsymbol{Wx} + \log(\boldsymbol{1}^T\exp(\boldsymbol{Wx}))$$，这里要注意逐元素 $$\log$$ 满足等式 $$\log(\boldsymbol{u}/c) = \log(\boldsymbol{u}) - \boldsymbol{1}\log(c)$$，以及 $$\boldsymbol{y}$$ 满足 $$\boldsymbol{y}^T \boldsymbol{1} = 1$$。求微分，使用矩阵乘法、逐元素函数等法则：$$\mathrm{d}l = -\boldsymbol{y}^T\mathrm{d}\boldsymbol{Wx}+\frac{\boldsymbol{1}^T\left(\exp(\boldsymbol{Wx})\odot(\mathrm{d}\boldsymbol{Wx})\right)}{\boldsymbol{1}^T\exp(\boldsymbol{Wx})}$$。
再套上迹并做交换，注意可化简 $$\boldsymbol{1}^T\left(\exp(\boldsymbol{Wx})\odot(\mathrm{d}\boldsymbol{Wx})\right) = \exp(\boldsymbol{Wx})^T\mathrm{d}\boldsymbol{Wx}$$，这是根据等式 $$\boldsymbol{1}^T (\boldsymbol{u}\odot \boldsymbol{v}) = \boldsymbol{u}^T \boldsymbol{v}$$，故 $$\mathrm{d}l = \mathrm{tr}\left(-\boldsymbol{y}^T\mathrm{d}\boldsymbol{Wx}+\frac{\exp(\boldsymbol{Wx})^T\mathrm{d}\boldsymbol{Wx}}{\boldsymbol{1}^T\exp(\boldsymbol{Wx})}\right) =\mathrm{tr}(\boldsymbol{x}(\mathrm{softmax}(\boldsymbol{Wx})-\boldsymbol{y})^T\mathrm{d}\boldsymbol{W})$$。对照导数与微分的联系，得到 $$\frac{\partial l}{\partial \boldsymbol{W}}= (\mathrm{softmax}(\boldsymbol{Wx})-\boldsymbol{y})\boldsymbol{x}^T$$。

**另解**：定义 $$\boldsymbol{a} = \boldsymbol{Wx}$$，则 $$l = -\boldsymbol{y}^T\log\mathrm{softmax}(\boldsymbol{a})$$，先如上求出 $$\frac{\partial l}{\partial \boldsymbol{a}} = \mathrm{softmax}(\boldsymbol{a})-\boldsymbol{y}$$，再利用复合法则：$$\mathrm{d}l = \mathrm{tr}\left(\frac{\partial l}{\partial \boldsymbol{a}}^T\mathrm{d}\boldsymbol{a}\right) = \mathrm{tr}\left(\frac{\partial l}{\partial \boldsymbol{a}}^T\mathrm{d}\boldsymbol{Wx}\right) = \mathrm{tr}\left(\boldsymbol{x}\frac{\partial l}{\partial \boldsymbol{a}}^T\mathrm{d}\boldsymbol{W}\right)$$，得到 $$\frac{\partial l}{\partial \boldsymbol{W}}= \frac{\partial l}{\partial\boldsymbol{a}}\boldsymbol{x}^T$$。


**例4【方差的最大似然估计】**：样本 $$\boldsymbol{x}_1,\dots, \boldsymbol{x}_n\sim N(\boldsymbol{\mu}, \Sigma)$$，其中 $$\Sigma$$ 是对称正定矩阵，求方差 $$\Sigma$$ 的最大似然估计。写成数学式是：$$l = \log\lvert\Sigma\rvert+\frac{1}{n}\sum_{i=1}^n(\boldsymbol{x}_i-\boldsymbol{\bar{x}})^T\Sigma^{-1}(\boldsymbol{x}_i-\boldsymbol{\bar{x}})$$，求 $$\frac{\partial l }{\partial \Sigma}$$ 的零点。

解：首先求微分，使用矩阵乘法、行列式、逆等运算法则，第一项是 $$\mathrm{d}\log\lvert\Sigma\rvert = \lvert\Sigma\rvert^{-1}\mathrm{d}\lvert\Sigma\rvert = \mathrm{tr}(\Sigma^{-1}\mathrm{d}\Sigma)$$，第二项是 $$\frac{1}{n}\sum_{i=1}^n(\boldsymbol{x}_i-\boldsymbol{\bar{x}})^T\mathrm{d}\Sigma^{-1}(\boldsymbol{x}_i-\boldsymbol{\bar{x}}) = -\frac{1}{n}\sum_{i=1}^n(\boldsymbol{x}_i-\boldsymbol{\bar{x}})^T\Sigma^{-1}\mathrm{d}\Sigma\Sigma^{-1}(\boldsymbol{x}_i-\boldsymbol{\bar{x}})$$。再给第二项套上迹做交换：$$\mathrm{d}l = \mathrm{tr}\left(\left(\Sigma^{-1}-\Sigma^{-1}\boldsymbol{S}\Sigma^{-1}\right)\mathrm{d}\Sigma\right)$$，其中 $$\boldsymbol{S} = \frac{1}{n}\sum_{i=1}^n(\boldsymbol{x}_i-\boldsymbol{\bar{x}})(\boldsymbol{x}_i-\boldsymbol{\bar{x}})^T$$，定义为样本方差。对照导数与微分的联系，有 $$\frac{\partial l }{\partial \Sigma}=(\Sigma^{-1}-\Sigma^{-1}\boldsymbol{S}\Sigma^{-1})^T$$，其零点即 $$\Sigma$$ 的最大似然估计为 $$\Sigma = \boldsymbol{S}$$。


最后一例留给经典的神经网络。神经网络的求导术是学术史上的重要成果，还有个专门的名字叫做 BP 算法，我相信如今很多人在初次推导 BP 算法时也会颇费一番脑筋，事实上使用矩阵求导术来推导并不复杂。为简化起见，我们推导二层神经网络的 BP 算法。

**例5【二层神经网络】**：$$l = -\boldsymbol{y}^T\log\mathrm{softmax}(\boldsymbol{W}_2\sigma(\boldsymbol{W}_1\boldsymbol{x}))$$，求 $$\frac{\partial l}{\partial \boldsymbol{W}_1}$$ 和 $$\frac{\partial l}{\partial \boldsymbol{W}_2}$$。其中 $$\boldsymbol{y}$$ 是除一个元素为 $$1$$ 外其它元素为 $$0$$ 的向量，$$\mathrm{softmax}(\boldsymbol{a}) = \frac{\exp(\boldsymbol{a})}{\boldsymbol{1}^T\exp(\boldsymbol{a})}$$ 同例 3，$$\sigma(\cdot)$$ 是逐元素 sigmoid 函数 $$\sigma(a) = \frac{1}{1+\exp(-a)}$$。

解：定义 $$\boldsymbol{a}_1=\boldsymbol{W}_1\boldsymbol{x}$$，$$\boldsymbol{h}_1 = \sigma(\boldsymbol{a}_1)$$，$$\boldsymbol{a}_2 = \boldsymbol{W}_2 \boldsymbol{h}_1$$，则 $$l =-\boldsymbol{y}^T\log\mathrm{softmax}(\boldsymbol{a}_2)$$。 在例 3 中已求出 $$\frac{\partial l}{\partial \boldsymbol{a}_2} = \mathrm{softmax}(\boldsymbol{a}_2)-\boldsymbol{y}$$。使用复合法则，注意此处 $$\boldsymbol{h}_1, \boldsymbol{W}_2$$ 都是变量：$$\mathrm{d}l = \mathrm{tr}\left(\frac{\partial l}{\partial \boldsymbol{a}_2}^T\mathrm{d}\boldsymbol{a}_2\right) = \mathrm{tr}\left(\frac{\partial l}{\partial \boldsymbol{a}_2}^T\mathrm{d}\boldsymbol{W}_2 \boldsymbol{h}_1\right) + \mathrm{tr}\left(\frac{\partial l}{\partial \boldsymbol{a}_2}^T\boldsymbol{W}_2 \mathrm{d}\boldsymbol{h}_1\right)$$，使用矩阵乘法交换的迹技巧从第一项得到 $$\frac{\partial l}{\partial \boldsymbol{W}_2}= \frac{\partial l}{\partial\boldsymbol{a}_2}\boldsymbol{h}_1^T$$，从第二项得到 $$\frac{\partial l}{\partial \boldsymbol{h}_1}= \boldsymbol{W}_2^T\frac{\partial l}{\partial\boldsymbol{a}_2}$$。接下来求 $$\frac{\partial l}{\partial \boldsymbol{a}_1}$$，继续使用复合法则，并利用矩阵乘法和逐元素乘法交换的迹技巧：$$\mathrm{tr}\left(\frac{\partial l}{\partial\boldsymbol{h}_1}^T\mathrm{d}\boldsymbol{h}_1\right) = \mathrm{tr}\left(\frac{\partial l}{\partial\boldsymbol{h}_1}^T(\sigma'(\boldsymbol{a}_1)\odot \mathrm{d}\boldsymbol{a}_1)\right) = \mathrm{tr}\left(\left(\frac{\partial l}{\partial\boldsymbol{h}_1}\odot \sigma'(\boldsymbol{a}_1)\right)^T\mathrm{d}\boldsymbol{a}_1\right)$$，得到 $$\frac{\partial l}{\partial \boldsymbol{a}_1}= \frac{\partial l}{\partial\boldsymbol{h}_1}\odot\sigma'(\boldsymbol{a}_1)$$。 为求 $$\frac{\partial l}{\partial \boldsymbol{W}_1}$$，再用一次复合法则：$$\mathrm{tr}\left(\frac{\partial l}{\partial\boldsymbol{a}_1}^T\mathrm{d}\boldsymbol{a}_1\right) = \mathrm{tr}\left(\frac{\partial l}{\partial\boldsymbol{a}_1}^T\mathrm{d}\boldsymbol{W}_1\boldsymbol{x}\right) = \mathrm{tr}\left(\boldsymbol{x}\frac{\partial l}{\partial\boldsymbol{a}_1}^T\mathrm{d}\boldsymbol{W}_1\right)$$，得到 $$\frac{\partial l}{\partial \boldsymbol{W}_1}= \frac{\partial l}{\partial\boldsymbol{a}_1}\boldsymbol{x}^T$$。

## 2 矩阵对矩阵的求导

矩阵对矩阵的求导采用了向量化的思路，常应用于二阶方法求解优化问题。

首先来琢磨一下定义。矩阵对矩阵的导数，需要什么样的定义？第一，矩阵 $$\boldsymbol{F}(p\times q)$$ 对矩阵 $$\boldsymbol{X}(m\times n)$$ 的导数应包含所有 $$mnpq$$ 个偏导数 $$\frac{\partial \boldsymbol{F}_{kl}}{\partial \boldsymbol{X}_{ij}}$$，从而不损失信息；第二，导数与微分有简明的联系，因为在计算导数和应用中需要这个联系；第三，导数有简明的从整体出发的算法。我们先定义向量 $$\boldsymbol{f}(p\times 1)$$ 对向量 $$\boldsymbol{x}(m\times1)$$ 的导数

$$
\begin{equation}
    \frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}} =
    \begin{bmatrix}
    \frac{\partial f_1}{\partial x_1} & \frac{\partial f_2}{\partial x_1} & \cdots & \frac{\partial f_p}{\partial x_1}\\ \frac{\partial f_1}{\partial x_2} & \frac{\partial f_2}{\partial x_2} & \cdots & \frac{\partial f_p}{\partial x_2}\\ \vdots & \vdots & \ddots & \vdots\\ \frac{\partial f_1}{\partial x_m} & \frac{\partial f_2}{\partial x_m} & \cdots & \frac{\partial f_p}{\partial x_m}\\ \end{bmatrix}(m\times p)，
\end{equation}
$$

有 $$\mathrm{d}\boldsymbol{f} = \frac{\partial \boldsymbol{f} }{\partial \boldsymbol{x} }^T \mathrm{d}\boldsymbol{x}$$；再定义矩阵的（按列优先）向量化

$$
\begin{equation}
    \mathrm{vec}(\boldsymbol{X}) = [\boldsymbol{X}_{11}, \ldots, \boldsymbol{X}_{m1}, \boldsymbol{X}_{12}, \ldots, \boldsymbol{X}_{m2}, \ldots, \boldsymbol{X}_{1n}, \ldots, \boldsymbol{X}_{mn}]^T(mn\times1),
\end{equation}
$$

并定义矩阵 $$\boldsymbol{F}$$ 对矩阵 $$\boldsymbol{X}$$ 的导数：

$$
\begin{equation}
    \frac{\partial \boldsymbol{F}}{\partial \boldsymbol{X}} = \frac{\partial \mathrm{vec}(\boldsymbol{F})}{\partial \mathrm{vec}(\boldsymbol{X})}(mn\times pq).
\end{equation}
$$

导数与微分有联系 $$\mathrm{vec}(\mathrm{d}\boldsymbol{F}) = \frac{\partial \boldsymbol{F}}{\partial \boldsymbol{X}}^T \mathrm{vec}(\mathrm{d}\boldsymbol{X})$$。几点说明如下：

- 按此定义，标量 $$f$$ 对矩阵 $$\boldsymbol{X}(m\times n)$$ 的导数 $$\frac{\partial f}{\partial \boldsymbol{X}}$$ 是 $$mn\times1$$ 向量，与上篇的定义不兼容，不过二者容易相互转换。为避免混淆，我们用 $$\nabla_{\boldsymbol{X}} f$$ 表示上篇定义的 $$m\times n$$ 矩阵，则有 $$\frac{\partial f}{\partial \boldsymbol{X}}=\mathrm{vec}(\nabla_{\boldsymbol{X}} f)$$。虽然本篇的技术可以用于标量对矩阵求导这种特殊情况，但使用上篇中的技术更方便。读者可以通过上篇中的算例试验两种方法的等价转换；
- 标量对矩阵的二阶导数，又称 Hessian 矩阵，定义为 $$\nabla^2_{\boldsymbol{X}} f = \frac{\partial^2 f}{\partial \boldsymbol{X}^2} = \frac{\partial \nabla_{\boldsymbol{X}} f}{\partial \boldsymbol{X}}(mn\times mn)$$，是对称矩阵。对向量 $$\frac{\partial f}{\partial \boldsymbol{X}}$$ 或矩阵 $$\nabla_{\boldsymbol{X}} f$$ 求导都可以得到 Hessian 矩阵，但从矩阵 $$\nabla_{\boldsymbol{X}} f$$ 出发更方便；
- $$\frac{\partial \boldsymbol{F}}{\partial \boldsymbol{X}} = \frac{\partial\mathrm{vec} (\boldsymbol{F})}{\partial \boldsymbol{X}} = \frac{\partial \boldsymbol{F}}{\partial \mathrm{vec}(\boldsymbol{X})} = \frac{\partial\mathrm{vec}(\boldsymbol{F})}{\partial \mathrm{vec}(\boldsymbol{X})}$$，求导时矩阵被向量化，弊端是这在一定程度破坏了矩阵的结构，会导致结果变得形式复杂；好处是多元微积分中关于梯度、Hessian 矩阵的结论可以沿用过来，只需将矩阵向量化。例如优化问题中，牛顿法的更新 $$\Delta \boldsymbol{X}$$，满足 $$\mathrm{vec}(\Delta \boldsymbol{X}) = -(\nabla^2_{\boldsymbol{X}} f)^{-1}\mathrm{vec}(\nabla_{\boldsymbol{X}} f)$$；
- 在资料中，矩阵对矩阵的导数还有其它定义，比如 $$\frac{\partial \boldsymbol{F}}{\partial \boldsymbol{X}} = \left[\frac{\partial \boldsymbol{F}_{kl}}{\partial \boldsymbol{X}}\right](mp\times nq)$$，它能兼容上篇中的标量对矩阵导数的定义，但微分与导数的联系（$$\mathrm{d}\boldsymbol{F}$$ 等于 $$\frac{\partial \boldsymbol{F}}{\partial \boldsymbol{X}}$$ 中每个 $$m\times n$$ 子块分别与 $$\mathrm{d}\boldsymbol{X}$$ 做内积）不够简明，不便于计算和应用。

然后来建立运算法则。仍然要利用导数与微分的联系 $$\mathrm{vec}(\mathrm{d}\boldsymbol{F}) = \frac{\partial \boldsymbol{F}}{\partial \boldsymbol{X}}^T \mathrm{vec}(\mathrm{d}\boldsymbol{X})$$，求微分的方法与上篇相同，而从微分得到导数需要一些向量化的技巧：

- 线性：$$\mathrm{vec}(\boldsymbol{A}+\boldsymbol{B}) = \mathrm{vec}(\boldsymbol{A}) + \mathrm{vec}(\boldsymbol{B})$$；
- 矩阵乘法：$$\mathrm{vec}(\boldsymbol{AXB}) = (\boldsymbol{B}^T \otimes \boldsymbol{A}) \mathrm{vec}(\boldsymbol{X})$$，其中 $$\otimes$$ 表示 Kronecker 积，$$\boldsymbol{A}(m\times n)$$ 与 $$\boldsymbol{B}(p\times q)$$ 的 Kronecker 积是 $$\boldsymbol{A}\otimes \boldsymbol{B} = [\boldsymbol{A}_{ij}\boldsymbol{B}](mp\times nq)$$。此式证明见张贤达《矩阵分析与应用》第 107-108 页；
- 转置：$$\mathrm{vec}(\boldsymbol{A}^T) = \boldsymbol{K}_{mn}\mathrm{vec}(\boldsymbol{A})$$，$$\boldsymbol{A}$$ 是 $$m\times n$$ 矩阵，其中 $$\boldsymbol{K}_{mn}(mn\times mn)$$ 是交换矩阵(commutation matrix)；
- 逐元素乘法：$$\mathrm{vec}(\boldsymbol{A}\odot \boldsymbol{X}) = \mathrm{diag}(\boldsymbol{A})\mathrm{vec}(\boldsymbol{X})$$，其中 $$\mathrm{diag}(\boldsymbol{A})(mn\times mn)$$ 是用 $$A$$ 的元素（按列优先）排成的对角阵。

观察一下可以断言，若矩阵函数 $$\boldsymbol{F}$$ 是矩阵 $$\boldsymbol{X}$$ 经加减乘法、行列式、逆、逐元素函数等运算构成，则使用相应的运算法则对 $$\boldsymbol{F}$$ 求微分，再做向量化并使用技巧将其它项交换至 $$\mathrm{vec}(\mathrm{d}\boldsymbol{X})$$  左侧，即能得到导数。

再谈一谈复合：假设已求得 $$\frac{\partial \boldsymbol{F}}{\partial \boldsymbol{Y}}$$，而 $$\boldsymbol{Y}$$ 是 $$\boldsymbol{X}$$ 的函数，如何求 $$\frac{\partial \boldsymbol{F}}{\partial \boldsymbol{X}}$$ 呢？从导数与微分的联系入手，$$\mathrm{vec}(\mathrm{d}\boldsymbol{F}) = \frac{\partial \boldsymbol{F}}{\partial Y}^T\mathrm{vec}(\mathrm{d}\boldsymbol{Y}) = \frac{\partial \boldsymbol{F}}{\partial \boldsymbol{Y}}^T\frac{\partial \boldsymbol{Y}}{\partial \boldsymbol{X}}^T\mathrm{vec}(\mathrm{d}\boldsymbol{X})$$，可以推出链式法则 $$\frac{\partial \boldsymbol{F}}{\partial \boldsymbol{X}} = \frac{\partial \boldsymbol{Y}}{\partial \boldsymbol{X}}\frac{\partial \boldsymbol{F}}{\partial \boldsymbol{Y}}$$。

和标量对矩阵的导数相比，矩阵对矩阵的导数形式更加复杂，从不同角度出发常会得到形式不同的结果。有一些 Kronecker 积和交换矩阵相关的恒等式，可用来做等价变形：

- $$(\boldsymbol{A}\otimes \boldsymbol{B})^T = \boldsymbol{A}^T \otimes \boldsymbol{B}^T$$；
- $$\mathrm{vec}(\boldsymbol{ab}^T) = \boldsymbol{b}\otimes\boldsymbol{a}$$；
- $$(\boldsymbol{A}\otimes \boldsymbol{B})(\boldsymbol{C}\otimes \boldsymbol{D}) = (\boldsymbol{AC})\otimes (\boldsymbol{BD})$$。可以对 $$\boldsymbol{F} = \boldsymbol{D}^T\boldsymbol{B}^T\boldsymbol{XAC}$$ 求导来证明，一方面，直接求导得到 $$\frac{\partial \boldsymbol{F}}{\partial \boldsymbol{X}} = (\boldsymbol{AC}) \otimes (\boldsymbol{BD})$$；另一方面，引入 $$\boldsymbol{Y} = \boldsymbol{B}^T\boldsymbol{XA}$$，有 $$\frac{\partial \boldsymbol{F}}{\partial \boldsymbol{Y}} = \boldsymbol{C} \otimes \boldsymbol{D}$$，$$\frac{\partial \boldsymbol{Y}}{\partial \boldsymbol{X}} = \boldsymbol{A} \otimes \boldsymbol{B}$$，用链式法则得到 $$\frac{\partial \boldsymbol{F}}{\partial \boldsymbol{X}} = (\boldsymbol{A}\otimes \boldsymbol{B})(\boldsymbol{C} \otimes \boldsymbol{D})$$；
- $$\boldsymbol{K}_{mn} = \boldsymbol{K}_{nm}^T, \boldsymbol{K}_{mn}\boldsymbol{K}_{nm} = \boldsymbol{I}$$；
- $$\boldsymbol{K}_{pm}(\boldsymbol{A}\otimes \boldsymbol{B}) \boldsymbol{K}_{nq} = \boldsymbol{B}\otimes \boldsymbol{A}$$，$$\boldsymbol{A}$$ 是 $$m\times n$$ 矩阵，$$\boldsymbol{B}$$ 是 $$p\times q$$ 矩阵。可以对 $$\boldsymbol{AXB}^T$$ 做向量化来证明，一方面，$$\mathrm{vec}(\boldsymbol{AXB}^T) = (\boldsymbol{B}\otimes \boldsymbol{A})\mathrm{vec}(\boldsymbol{X})$$；另一方面，$$\mathrm{vec}(\boldsymbol{AXB}^T) = \boldsymbol{K}_{pm}\mathrm{vec}(\boldsymbol{BX}^T\boldsymbol{A}^T) = \boldsymbol{K}_{pm}(\boldsymbol{A}\otimes \boldsymbol{B})\mathrm{vec}(\boldsymbol{X}^T) = \boldsymbol{K}_{pm}(\boldsymbol{A}\otimes \boldsymbol{B}) \boldsymbol{K}_{nq}\mathrm{vec}(\boldsymbol{X})$$。

接下来演示一些算例。

**例1**：$$\boldsymbol{F} = \boldsymbol{AX}$$，$$\boldsymbol{X}$$ 是 $$m\times n$$ 矩阵，求 $$\frac{\partial \boldsymbol{F}}{\partial \boldsymbol{X}}$$。

解：先求微分：$$\mathrm{d}\boldsymbol{F}=\boldsymbol{A}\mathrm{d}\boldsymbol{X}$$，再做向量化，使用矩阵乘法的技巧，注意在 $$\mathrm{d}\boldsymbol{X}$$ 右侧添加单位阵：$$\mathrm{vec}(\mathrm{d}\boldsymbol{F}) = \mathrm{vec}(\boldsymbol{A}\mathrm{d}\boldsymbol{X}) = (\boldsymbol{I}_n\otimes \boldsymbol{A})\mathrm{vec}(\mathrm{d}\boldsymbol{X})$$，对照导数与微分的联系得到 $$\frac{\partial \boldsymbol{F}}{\partial \boldsymbol{X}} = \boldsymbol{I}_n\otimes \boldsymbol{A}^T$$。

特例：如果 $$\boldsymbol{X}$$ 退化为向量，$$\boldsymbol{f} = \boldsymbol{A} \boldsymbol{x}$$，则根据向量的导数与微分的关系 $$\mathrm{d}\boldsymbol{f} = \frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}}^T \mathrm{d}\boldsymbol{x}$$，得到  $$\frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}} = \boldsymbol{A}^T$$。

**例2**：$$\boldsymbol{f} = \log\lvert\boldsymbol{X}\rvert$$，$$\boldsymbol{X}$$ 是 $$n\times n$$ 矩阵，求 $$\nabla_{\boldsymbol{X}} \boldsymbol{f}$$ 和 $$\nabla^2_{\boldsymbol{X}} \boldsymbol{f}$$。

解：使用第一部分中的技术可求得 $$\nabla_{\boldsymbol{X}} \boldsymbol{f} = \boldsymbol{X}^{-1T}$$。为求 $$\nabla^2_{\boldsymbol{X}} \boldsymbol{f}$$，先求微分：$$\mathrm{d}\nabla_{\boldsymbol{X}} \boldsymbol{f} = -(\boldsymbol{X}^{-1}\mathrm{d}\boldsymbol{XX}^{-1})^T$$，再做向量化，使用转置和矩阵乘法的技巧
 $$\mathrm{vec}(\mathrm{d}\nabla_{\boldsymbol{X}} \boldsymbol{f})= -\boldsymbol{K}_{nn}\mathrm{vec}(\boldsymbol{X}^{-1}\mathrm{d}\boldsymbol{XX}^{-1}) = -\boldsymbol{K}_{nn}(\boldsymbol{X}^{-1T}\otimes \boldsymbol{X}^{-1})\mathrm{vec}(\mathrm{d}\boldsymbol{X})$$，对照导数与微分的联系，得到 $$\nabla^2_{\boldsymbol{X}} \boldsymbol{f} = -\boldsymbol{K}_{nn}(\boldsymbol{X}^{-1T}\otimes \boldsymbol{X}^{-1})$$，注意它是对称矩阵。在 $$\boldsymbol{X}$$ 是对称矩阵时，可简化为 $$\nabla^2_{\boldsymbol{X}} \boldsymbol{f} = -\boldsymbol{X}^{-1}\otimes \boldsymbol{X}^{-1}$$。

**例3**：$$\boldsymbol{F} = \boldsymbol{A}\exp(\boldsymbol{XB})$$，$$\boldsymbol{A}$$ 是 $$l\times m$$，$$\boldsymbol{X}$$ 是 $$m\times n$$，$$\boldsymbol{B}$$ 是 $$n\times p$$ 矩阵，$$\exp()$$ 为逐元素函数，求 $$\frac{\partial \boldsymbol{F}}{\partial \boldsymbol{X}}$$。

解：先求微分：$$\mathrm{d}\boldsymbol{F} = \boldsymbol{A}(\exp(\boldsymbol{XB})\odot (\mathrm{d}\boldsymbol{XB}))$$，再做向量化，使用矩阵乘法的技巧：$$\mathrm{vec}(\mathrm{d}\boldsymbol{F}) = (\boldsymbol{I}_p\otimes \boldsymbol{A})\mathrm{vec}(\exp(\boldsymbol{XB})\odot (\mathrm{d}\boldsymbol{XB}))$$，再用逐元素乘法的技巧：$$\mathrm{vec}(\mathrm{d}\boldsymbol{F}) = (\boldsymbol{I}_p \otimes \boldsymbol{A}) \mathrm{diag}(\exp(\boldsymbol{XB}))\mathrm{vec}(\mathrm{d}\boldsymbol{XB})$$，再用矩阵乘法的技巧：$$\mathrm{vec}(\mathrm{d}\boldsymbol{F}) = (\boldsymbol{I}_p\otimes \boldsymbol{A})\mathrm{diag}(\exp(\boldsymbol{XB}))(\boldsymbol{B}^T\otimes I_m)\mathrm{vec}(\mathrm{d}\boldsymbol{X})$$，对照导数与微分的联系得到 $$\frac{\partial \boldsymbol{F}}{\partial \boldsymbol{X}} = (\boldsymbol{B}\otimes \boldsymbol{I}_m)\mathrm{diag}(\exp(\boldsymbol{XB}))(\boldsymbol{I}_p\otimes \boldsymbol{A}^T)$$。

**例4【一元 logistic 回归】**：$$l = -y \boldsymbol{x}^T \boldsymbol{w} + \log(1 + \exp(\boldsymbol{x}^T\boldsymbol{w}))$$，求 $$\nabla_{\boldsymbol{w}} l$$ 和 $$\nabla_{\boldsymbol{w}}^2 l$$。其中 $$y$$ 是取值 0 或 1 的标量，$$\boldsymbol{x}$$ 和 $$\boldsymbol{w}$$ 是向量。

解：使用上篇中的技术可求得 $$\nabla_{\boldsymbol{w}} l = \boldsymbol{x}(\sigma(\boldsymbol{x}^T\boldsymbol{w}) - y)$$，其中 $$\sigma(a) = \frac{\exp(a)}{1+\exp(a)}$$ 为 sigmoid 函数。为求 $$\nabla_{\boldsymbol{w}}^2 l$$，先求微分： $$\mathrm{d}\nabla_{\boldsymbol{w}} l = \boldsymbol{x} \sigma'(\boldsymbol{x}^T\boldsymbol{w})\boldsymbol{x}^T \mathrm{d}\boldsymbol{w}$$，其中 $$\sigma'(a) = \frac{\exp(a)}{(1+\exp(a))^2}$$ 为 sigmoid 函数的导数，对照导数与微分的联系，得到 $$\nabla_{\boldsymbol{w}}^2 l = \boldsymbol{x}\sigma'(\boldsymbol{x}^T\boldsymbol{w})\boldsymbol{x}^T$$。

**推广**：样本( $$\boldsymbol{x}_1, y_1), \dots, (\boldsymbol{x}_n,y_n)$$，$$l = \sum_{i=1}^N \left(-y_i \boldsymbol{x}_i^T\boldsymbol{w} + \log(1+\exp(\boldsymbol{x_i}^T\boldsymbol{w}))\right)$$，求 $$\nabla_{\boldsymbol{w}} l$$ 和 $$\nabla_{\boldsymbol{w}}^2 l$$。 有两种方法，方法一：先对每个样本求导，然后相加；方法二：定义矩阵 $$\boldsymbol{X} = \begin{bmatrix}\boldsymbol{x}_1^T \\ \vdots \\ \boldsymbol{x}_n^T \end{bmatrix}$$，向量 $$\boldsymbol{y} = \begin{bmatrix}y_1 \\ \vdots \\ y_n\end{bmatrix}$$，将 $$l$$ 写成矩阵形式 $$$l = -\boldsymbol{y}^T \boldsymbol{X}\boldsymbol{w} + \boldsymbol{1}^T\log(\boldsymbol{1} + \exp(\boldsymbol{X}\boldsymbol{w}))$$，进而可以求得
 $$\nabla_{\boldsymbol{w}} l = \boldsymbol{X}^T(\sigma(\boldsymbol{X}\boldsymbol{w}) - \boldsymbol{y})$$，$$\nabla_{\boldsymbol{w}}^2 l = \boldsymbol{X}^T\text{diag}(\sigma'(\boldsymbol{X}\boldsymbol{w}))\boldsymbol{X}$$。


**例5【多元 logistic 回归】**：$$l = -\boldsymbol{y}^T\log \mathrm{softmax}(\boldsymbol{Wx})=$$$$-\boldsymbol{y}^T\boldsymbol{Wx}+$$\log(\boldsymbol{1}^T\exp(\boldsymbol{Wx}))$$，求 $$\nabla_{\boldsymbol{W}} l$$ 和 $$\nabla_{\boldsymbol{W}}^2 l$$。

**解**：上篇例 3 中已求得 $$\nabla_{\boldsymbol{W}} l = (\mathrm{softmax}(\boldsymbol{Wx})-\boldsymbol{y})\boldsymbol{x}^T$$。为求 $$\nabla_{\boldsymbol{W}}^2 l$$，先求微分：定义 $$\boldsymbol{a} = \boldsymbol{Wx}，\mathrm{d}\mathrm{softmax}(\boldsymbol{a}) = \frac{\exp(\boldsymbol{a})\odot \mathrm{d}\boldsymbol{a}}{\boldsymbol{1}^T\exp(\boldsymbol{a})} - \frac{\exp(\boldsymbol{a}) (\boldsymbol{1}^T(\exp(\boldsymbol{a})\odot \mathrm{d}\boldsymbol{a}))}{(\boldsymbol{1}^T\exp(\boldsymbol{a}))^2}$$，这里需要化简去掉逐元素乘法，第一项中 $$\exp(\boldsymbol{a})\odot \mathrm{d}\boldsymbol{a} = \mathrm{diag}(\exp(\boldsymbol{a})) \mathrm{d}\boldsymbol{a} $$，第二项中 $$\boldsymbol{1}^T(\exp(\boldsymbol{a})\odot \mathrm{d}\boldsymbol{a}) = \exp(\boldsymbol{a})^T\mathrm{d}\boldsymbol{a}$$，故有 $$\mathrm{d}\mathrm{softmax}(\boldsymbol{a}) = \mathrm{softmax}'(\boldsymbol{a})\mathrm{d}\boldsymbol{a}$$，其中
 $$\mathrm{softmax}'(\boldsymbol{a}) = \frac{\mathrm{diag}(\exp(\boldsymbol{a}))}{\boldsymbol{1}^T\exp(\boldsymbol{a})} - \frac{\exp(\boldsymbol{a})\exp(\boldsymbol{a})^T}{(\boldsymbol{1}^T\exp(\boldsymbol{a}))^2} $$，代入有 $$\mathrm{d}\nabla_{\boldsymbol{W}} l = \mathrm{softmax}'(\boldsymbol{a})\mathrm{d}\boldsymbol{a}\boldsymbol{x}^T = \mathrm{softmax}'(\boldsymbol{Wx})\mathrm{d}\boldsymbol{Wx}\boldsymbol{x}^T$$，做向量化并使用矩阵乘法的技巧，得到 $$\nabla_{\boldsymbol{W}}^2 l = (\boldsymbol{x}\boldsymbol{x}^T) \otimes \mathrm{softmax}'(\boldsymbol{Wx})$$。


最后做个总结。我们发展了从整体出发的矩阵求导的技术，导数与微分的联系是计算的枢纽，标量对矩阵的导数与微分的联系是 $$\mathrm{d}f = \mathrm{tr}(\nabla_{\boldsymbol{X}}^T f \mathrm{d}\boldsymbol{X})$$，先对 $$f$$ 求微分，再使用迹技巧可求得导数，特别地，标量对向量的导数与微分的联系是 $$\mathrm{d}f = \nabla_{\boldsymbol{x}}f^T \mathrm{d}\boldsymbol{x}$$；矩阵对矩阵的导数与微分的联系是 $$\mathrm{vec}(\mathrm{d}\boldsymbol{F}) = \frac{\partial \boldsymbol{F}}{\partial \boldsymbol{X}}^T \mathrm{vec}(\mathrm{d}\boldsymbol{X})$$，先对 $$\boldsymbol{F}$$  求微分，再使用向量化的技巧可求得导数，特别地，向量对向量的导数与微分的联系是 $$\mathrm{d}\boldsymbol{f} = \frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}}^T\mathrm{d}\boldsymbol{x}$$。

## 3 反向传播算法的完整向量形式推导

首先定义一些常用的变量：

- $$n^l$$：第 $$l$$ 层的神经元个数；
- $$f_l(\cdot)$$：第 $$l$$ 层的激活函数；
- $$\boldsymbol{W}^l\in \mathbb{R}^{n^l\times n^{l-1}}$$：第 $$l-1$$ 层到第 $$l$$ 层的权重矩阵；
- $$\boldsymbol{b}^l\in \mathbb{R}^{n^l}$$：第 $$l-1$$ 层到第 $$l$$ 层的偏置；
- $$\boldsymbol{z}^l\in \mathbb{R}^{n^l}$$：第 $$l$$ 层的输入；
- $$\boldsymbol{a}^l\in \mathbb{R}^{n^l}$$：第 $$l$$ 层的输出。

在每一层的正向传播过程计算过程如下所示：

$$
\begin{align}
    \boldsymbol{z}^l &= \boldsymbol{W}^l\boldsymbol{a}^{(l-1)} + \boldsymbol{b}^l \\
    \boldsymbol{a}^l &= f_l(\boldsymbol{z}^l)
\end{align}
$$

为进行梯度计算，我们需要先写出目标函数：

$$
\begin{equation}
J(\boldsymbol{W},\boldsymbol{b})=\sum_{i=1}^{N}J(\boldsymbol{W},\boldsymbol{b};\boldsymbol{x}^i,y^i)+\frac{1}{2}\lambda||\boldsymbol{W}||_F^2
\end{equation}
$$

其中 

$$||\boldsymbol{W}||_F^2=\sum_{l=1}^L\sum_{j=1}^{n^{l+1}}\sum_{i=1}^{n^l}W_{ij}^l$$

那么对 $$\boldsymbol{W}$$ 和 $$\boldsymbol{b}$$ 的更新公式理论上可以写成：

$$
\begin{align}
    \boldsymbol{W}^l &= \boldsymbol{W}^l-\alpha\frac{\partial J(\boldsymbol{W},\boldsymbol{b})}{\partial \boldsymbol{W}^l} \\
              &= \boldsymbol{W}^l-\alpha\bigg( \sum_{i=1}^N \frac{\partial J(\boldsymbol{W},\boldsymbol{b};\boldsymbol{x}^i,y^i)}{\partial \boldsymbol{W}^l} + \lambda\boldsymbol{W}^l \bigg) \tag{1}\\
    \boldsymbol{b}^l &= \boldsymbol{b}^l-\alpha\frac{\partial J(\boldsymbol{W},\boldsymbol{b})}{\partial \boldsymbol{b}^l} \\
              &= \boldsymbol{b}^l-\alpha \sum_{i=1}^N \frac{\partial J(\boldsymbol{W},\boldsymbol{b};\boldsymbol{x}^i,y^i)}{\partial \boldsymbol{b}^l} \tag{2}
\end{align}
$$

我们考虑每一个单独样本的梯度，先假设 $$\frac{\partial J(\boldsymbol{W},\boldsymbol{b};\boldsymbol{x},y)}{\partial \boldsymbol{z}^l}=\boldsymbol{\delta}^l$$，根据第一部分矩阵导数与微分的联系，有 $$\mathrm{d}J=\mathrm{tr}\bigg(\boldsymbol{\delta}^{lT}\mathrm{d}\boldsymbol{z}^l\bigg)$$，而 $$\boldsymbol{z}^l = \boldsymbol{W}^l\boldsymbol{a}^{(l-1)} + \boldsymbol{b}^l$$，所以 $$\mathrm{d}\boldsymbol{z}^l = \mathrm{d}\boldsymbol{W}^l\boldsymbol{a}^{(l-1)}$$，因此，

$$
\begin{align}
    \mathrm{d}J &= \mathrm{tr}\bigg( \boldsymbol{\delta}^{lT}\mathrm{d}\boldsymbol{W}^l\boldsymbol{a}^{(l-1)} \bigg)\\
                &= \mathrm{tr}\bigg( \boldsymbol{a}^{(l-1)}\boldsymbol{\delta}^{lT}\mathrm{d}\boldsymbol{W}^l \bigg)
\end{align}
$$

所以最后有

$$
\begin{equation}
    \frac{\partial J(\boldsymbol{W},\boldsymbol{b};\boldsymbol{x},y)}{\partial \boldsymbol{W}^l}=\boldsymbol{\delta}^l\boldsymbol{a}^{(l-1)T}. \tag{3}
\end{equation}
$$

同理有

$$
\begin{equation}
    \frac{\partial J(\boldsymbol{W},\boldsymbol{b};\boldsymbol{x},y)}{\partial \boldsymbol{b}^l}=\boldsymbol{\delta}^l. \tag{4}
\end{equation}
$$

下面求 $$\boldsymbol{\delta}^l$$，假设已知 $$\frac{\partial J(\boldsymbol{W},\boldsymbol{b};\boldsymbol{x},y)}{\partial \boldsymbol{z}^{(l+1)}}=\boldsymbol{\delta}^{(l+1)}$$，则有 $$\mathrm{d}J=\mathrm{tr}\bigg(\boldsymbol{\delta}^{(l+1)T}\mathrm{d}\boldsymbol{z}^{(l+1)}\bigg)$$，而 $$\boldsymbol{z}^{(l+1)} = \boldsymbol{W}^{(l+1)}\boldsymbol{a}^l + \boldsymbol{b}^{(l+1)}$$，所以 $$\mathrm{d}\boldsymbol{z}^{(l+1)} = \boldsymbol{W}^{(l+1)} \mathrm{d}\boldsymbol{a}^l$$，又因为 $$\boldsymbol{a}^l=f_l(\boldsymbol{z}^l)$$ 是逐元素的，所以 $$\mathrm{d}\boldsymbol{a}^l=\mathrm{diag}(f'_l(\boldsymbol{z}^l))\mathrm{d}\boldsymbol{z}^l$$，代入后有

$$
\begin{equation}
    \mathrm{d}J=\mathrm{tr}\bigg( \boldsymbol{\delta}^{(l+1)T}\boldsymbol{W}^{(l+1)}\mathrm{diag}(f'_l(\boldsymbol{z}^l))\mathrm{d}\boldsymbol{z}^l \bigg).
\end{equation}
$$

所以有

$$
\begin{equation}
    \boldsymbol{\delta}^l=\frac{\partial J(\boldsymbol{W},\boldsymbol{b};\boldsymbol{x},y)}{\partial \boldsymbol{z}^l}=\mathrm{diag}(f'_l(\boldsymbol{z}^l))\boldsymbol{W}^{(l+1)T}\boldsymbol{\delta}^{(l+1)}. \tag{5}
\end{equation}
$$

其中当 $$f(\boldsymbol{x})=\mathrm{sigmoid}(\boldsymbol{x})$$ 时，有 $$f'(\boldsymbol{x})=f(\boldsymbol{x})(1-f(\boldsymbol{x}))$$，当 $$f(\boldsymbol{x})=\mathrm{tanh}(\boldsymbol{x})$$ 时，有 $$f'(\boldsymbol{x})=1-f(\boldsymbol{x})^2$$。

最后考虑最后一层的 $$\boldsymbol{\delta}^{(l+1)}$$。当使用交叉熵损失函数时（以下 $$\boldsymbol{a}$$ 和 $$\boldsymbol{z}$$ 都略去上标 $$(l+1)$$）

$$
\begin{equation}
    J(\boldsymbol{W},\boldsymbol{b};\boldsymbol{x},y)=-\boldsymbol{y}^T\ln\boldsymbol{a}-(\boldsymbol{1}-\boldsymbol{y})^T\ln(1-\boldsymbol{a}).
\end{equation}
$$

这时

$$
\begin{align}
    \mathrm{d}J &= -\boldsymbol{y}^T\bigg(\frac{1}{\boldsymbol{a}} \odot \mathrm{d}\boldsymbol{a}\bigg)-(\boldsymbol{1}-\boldsymbol{y})^T\bigg(\frac{-1}{\boldsymbol{1}-\boldsymbol{a}}\odot \mathrm{d}\boldsymbol{a}\bigg) \\
                &= -\bigg(\boldsymbol{y} \odot \frac{1}{\boldsymbol{a}} \bigg)^T \mathrm{d}\boldsymbol{a} + \bigg( (\boldsymbol{1}-\boldsymbol{y})\odot\frac{1}{\boldsymbol{1}-\boldsymbol{a}}\bigg)^T \mathrm{d}\boldsymbol{a} \\
                &= \bigg(-\boldsymbol{y} \odot \frac{1}{\boldsymbol{a}} + (\boldsymbol{1}-\boldsymbol{y})\odot\frac{1}{\boldsymbol{1}-\boldsymbol{a}}\bigg)^T \mathrm{d}\boldsymbol{a}
\end{align}
$$

又因为 $$\mathrm{d}\boldsymbol{a}=\mathrm{diag}(\mathrm{sigmoid}'(\boldsymbol{z}))\mathrm{d}\boldsymbol{z}=\mathrm{diag}(\boldsymbol{a(1-a)})\mathrm{d}\boldsymbol{z}$$，代入上式有

$$
\begin{align}
    \mathrm{d}J &= \bigg(-\boldsymbol{y} \odot \frac{1}{\boldsymbol{a}} + (\boldsymbol{1-y})\odot\frac{1}{\boldsymbol{1-a}}\bigg)^T \mathrm{diag}(\boldsymbol{a(1-a)})\mathrm{d}\boldsymbol{z} \\
                &= \bigg(-\mathrm{diag}(\boldsymbol{a(1-a)}) \boldsymbol{y} \odot \frac{1}{\boldsymbol{a}} + \mathrm{diag}(\boldsymbol{a(1-a)})(\boldsymbol{1-y})\odot\frac{1}{\boldsymbol{1-a}}\bigg)^T \mathrm{d}\boldsymbol{z} \\
                &= (-\boldsymbol{y}+\boldsymbol{y}\odot\boldsymbol{a} + \boldsymbol{a} - \boldsymbol{a}\odot\boldsymbol{y})^T \mathrm{d}\boldsymbol{z} \\
                &= (-\boldsymbol{y}+\boldsymbol{a})^T \mathrm{d}\boldsymbol{z} 
\end{align}
$$

所以有

$$
\begin{equation}
    \boldsymbol{\delta}^{(l+1)}=\frac{\partial J(\boldsymbol{W},\boldsymbol{b};\boldsymbol{x},y)}{\partial \boldsymbol{z}^{(l+1)}}=-\boldsymbol{y}+\boldsymbol{a}^{(l+1)}. \tag{6}
\end{equation}
$$

最后，将式 (6) 和式 (4) 代入式 (2) 则得到偏置项 $$\boldsymbol{b}$$ 的更新公式，将式 (6)，式 (5) 和式 (3) 代入式 (1) 则得到 $$\boldsymbol{W}$$ 的更新公式。
